# 1.1 Mon
* 1、（**值得玩玩**）Github揽获3k+星！清华开源CogAgent：基于多模态大模型的GUI Agent  PaperWeekly  https://mp.weixin.qq.com/s/ZXbEpSCFG90qz0uwHJ6cJQ \
  论文名称：CogAgent: A Visual Language Model for GUI Agents \
  论文链接：https://arxiv.org/pdf/2312.08914.pdf \
  GitHub项目地址（含开源模型、网页版Demo）: https://github.com/THUDM/CogVLM \
  CogAgent 是一个通用的视觉理解大模型，具备视觉问答、视觉定位（Grounding）、GUI Agent 等多种能力，在 9 个经典的图像理解榜单上（含 VQAv2，STVQA, DocVQA，TextVQA，MM-VET，POPE 等）取得了通用能力第一的成绩，并在涵盖电脑、手机的GUI Agent数据集上（含 Mind2Web，AITW 等），大幅超过基于 LLM 的 Agent，取得第一。为了更好地促进多模态大模型、Agent 社区的发展，我们已将 CogAgent-18B 开源至 GitHub 仓库（可商用），并提供了网页版 Demo。欢迎大家体验、使用与反馈！ \
* 2、（**务必看看**）基础模型+机器人：现在已经走到哪一步了  机器之心  https://mp.weixin.qq.com/s/n774PtQZn3XzJ7cb7WAV9Q \
  论文名称：Toward General-Purpose Robots via Foundation Models:A Survey and Meta-Analysis \
  论文地址：https://arxiv.org/pdf/2312.08782.pdf \
* 3、（**值得看看**）你没有看过的全新版本，Transformer数学原理揭秘  机器之心  https://mp.weixin.qq.com/s/i0vNZoqWejmFFPUMR4qo5w \
  揭秘 Transformer 的数学原理！  AINLPer  https://mp.weixin.qq.com/s/qVZqwpD7ngdSX-UzIj68hw \
  论文名称：A MATHEMATICAL PERSPECTIVE ON TRANSFORMERS \
  论文地址：https://arxiv.org/pdf/2312.10794.pdf
* 4、Mamba可以替代Transformer，但它们也能组合起来使用  机器之心  https://mp.weixin.qq.com/s/X4NCoSObAZTjyl0GpYUtpQ \
  论文名称：Block-State Transformers \
  论文地址：https://arxiv.org/pdf/2306.09539.pdf
* 5、通往具身通用智能：如何让机器从自然模态中学习到世界模型？  专知  https://mp.weixin.qq.com/s/9Lk77mgpvtHmqs3hd6iKqQ 

# 1.2 Tue
* 6、Hyena成下一代Transformer？StripedHyena-7B开源：最高128k输入，训练速度提升50%  新智元  https://mp.weixin.qq.com/s/dHDtfOkX1RVj6-bZVVUKlg

# 1.3 Wed
* 7、面向超长上下文，大语言模型如何优化架构，这篇综述一网打尽了  机器之心  https://mp.weixin.qq.com/s/VrV3E_SKTbpjJBfFyirvhA \
  论文链接：https://arxiv.org/pdf/2311.12351.pdf
* 8、(**牛逼，值得看看**)维基百科+大模型打败幻觉！斯坦福WikiChat性能碾压GPT-4，准确率高达97.3%   新智元  https://mp.weixin.qq.com/s/5fDBx9eLS9Q7nw3ZZiccKw \
  碾压GPT-4！斯坦福 | 发布WikiChat聊天机器人，准确率达97.3%  AINLPer  https://mp.weixin.qq.com/s/zX5bo1gvCI_nFKnjkg1Dwg \
  论文名称：WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia \
  论文地址：https://aclanthology.org/2023.findings-emnlp.157.pdf \
  项目代码：https://github.com/stanford-oval/WikiChat \
  Demo地址：https://wikichat.genie.stanford.edu/

# 1.4 Thu
* 9、斯坦福炒虾机器人爆火全网！华人团队成本22万元，能做满汉全席还会洗碗  新智元  https://mp.weixin.qq.com/s/i9VQJXxQI9uXpsrtyAlTOQ
* 10、首创pix2emb范式！NUS清华联合发布NExT-Chat：对话/检测/分割全能多模态大模型  新智元  https://mp.weixin.qq.com/s/BgDRskXV6HFBWwP9_M31Ew \
  多模态对话模型Demo：https://next-chatv.github.io/ \
  论文：https://arxiv.org/pdf/2311.04498.pdf \
  代码：https://github.com/NExT-ChatV/NExT-Chat \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/b71b9937-f67f-43c7-b3e5-fe81fc23ef38)
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/5d9fc943-005e-4c34-a9f6-e0424e4d3c3a)
* 11、GPT-4V开源平替！清华浙大领衔，LLaVA、CogAgent等开源视觉模型大爆发  新智元  https://mp.weixin.qq.com/s/LDVgCTvUcfnMX4Dl78gDHg
* 12、吴恩达最新推出基于大模型的《AI高级检索》课程，限时免费白嫖！  夕小瑶科技说  https://mp.weixin.qq.com/s/8GZPnm8tq2Dqgvj1iAfNAw \
  课程名是 《Advanced Retrieval for AI with Chroma》，即 《使用 Chroma 进行 AI 高级检索》 ，这里面Chroma 是一个 AI 原生开源嵌入数据库。
* 13、(**值得看看，什么叫LLM知识编辑**)如何编辑大模型中的知识？浙大等最新《大型语言模型知识编辑》全面综述  专知  https://mp.weixin.qq.com/s/vJwoQPVqNDWqfe8vpI7StA \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/d4b278d4-4da4-49dd-8523-ddd30b56871f)
  
# 1.5 Fri
* 14、谷歌DeepMind机器人成果三连发！两大能力全提升，数据收集系统可同时管理20个机器人  量子位  https://mp.weixin.qq.com/s/j7tviAf2DNbLxdIwC5rzWg \
  谷歌 DeepMind 机器人成果三连发，具身智能指日可待？  Founder Park  https://mp.weixin.qq.com/s/GF173SOD3HrO1Kl3Lxp6zQ \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/59ed061c-609d-4648-9695-f6f6a532e441)
  参考链接：https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/

# 1.6 Sat
* 15、《大型语言模型（LLMs）: 训练到推理》全面概述技术细节  专知 https://mp.weixin.qq.com/s/Pw_b0ndP1tFXuYc39pveOw \
  论文名称：understanding LLMs- A Comprehensive Overview from Training to Inference
* 16、模型A：幸亏有你，我才不得0分，模型B：俺也一样  机器学习研究组订阅  https://mp.weixin.qq.com/s/Q4z_dLoCeSn-d62So6TiVw \
  在大模型（LLM）爆发的当下，我们能不能像拼积木一样，把不同的模型搭建起来，而不会影响原来模型的功能，还能起到 1+1>2 的效果 \
  论文名称：LLM Augmented LLMs- Expanding Capabilities Through Composition \
  论文地址：https://arxiv.org/pdf/2401.02412.pdf \
  该研究是这样实现的，他们提出了一种新颖的 CALM（组合到增强语言模型 Composition to Augment Language Models）框架来解决模型组合设置。CALM 不是增强和 anchor LM 的浅层组合，而是在增强和 anchor 模型的中间层表示上引入了少量的可训练参数。

# 1.7 Sun
* 17、(**重要，值得看看**)万字长文再论大语言模型的位置编码及其外推性  PaperWeekly  https://mp.weixin.qq.com/s/xpvO169i8IhhgvNkcrtHow \

# 1.8 Mon
* 18、(**重要，值得看看**)以LLAMA为例，快速入门LLM的推理过程  吃果冻不吐果冻皮  https://mp.weixin.qq.com/s/5lbrqbqiHPZIARsVW6l6tA \
* 19、中科院最新工作：基于自步课程学习实现多模态大模型CLIP在多模态视觉语言理解与定位任务上的迁移研究
  论文CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding，其工作内容是基于自步课程学习实现多模态大模型CLIP在多模态视觉语言理解与定位任务上的迁移研究。\
  Arxiv：https://arxiv.org/abs/2305.08685 \
  代码：https://github.com/linhuixiao/CLIP-VG（已开源）
* 20、轻量级模型，重量级性能，TinyLlama、LiteLlama小模型火起来了  机器之心  https://mp.weixin.qq.com/s/qVFa2z9JJ6t6kTJlTubHfQ \
  论文名称：TinyLlama- An Open-Source Small Language Model \
  论文地址：https://arxiv.org/pdf/2401.02385.pdf \
  项目地址：https://github.com/jzhang38/TinyLlama/blob/main/README_zh-CN.md \
  LiteLlama:
  项目地址：https://huggingface.co/ahxt/LiteLlama-460M-1T
* 21、400万token上下文、推理再加速46%！最新开源方案升级MIT成果，推理成本再降低  量子位  https://mp.weixin.qq.com/s/fiYSESKcOgZIDe8dpLdAdQ \
  大模型无限流式输入推理飙升46%！国产开源加速「全家桶」，打破多轮对话长度限制  新智元  https://mp.weixin.qq.com/s/7aBzsiUjX-WHRzpY4IAU5A \
  开源地址：https://github.com/hpcaitech/SwiftInfer \
  Colossal-AI开源地址：https://github.com/hpcaitech/ColossalAI \
  参考链接：https://hpc-ai.com/blog/Colossal-AI-SwiftInfer \
  StreamingLLM可以在不牺牲生成效果、推理速度的前提下，实现多轮对话共400万个token，22.2倍推理速度提升。该项目在上线不到3个月时间内，GitHub项目标星达到5.7k star。不过，StreamingLLM使用原生PyTorch实现，对于多轮对话推理场景落地应用的低成本、低延迟、高吞吐等需求仍有优化空间。Colossal-AI团队开源了SwiftInfer，基于TensorRT的StreamingLLM，可以进一步提升大模型推理性能46%，有效解决如上问题
* 22、(**值得看看**)图解大模型计算加速系列：Flash Attention V1，从硬件到计算逻辑  吃果冻不吐果冻皮  https://mp.weixin.qq.com/s/J2i2MDv4us_GMwCyku0tnw
* 23、机器人又拿下一种家务：10小时学会煮咖啡，仅需观看人类演示视频  量子位  https://mp.weixin.qq.com/s/UnrKueHigPhIAthoLG2hGA \
  首个无师自通、泛化使用各种家具家电的具身三维图文大模型系统  机器之心  https://mp.weixin.qq.com/s/9Zs8bPhcqm4tZV4uITIhAA \
  SAGE: Bridging Semantic and Actionable Parts for GEneralizable Articulated-Object Manipulation under Language Instructions \
  论文链接：https://arxiv.org/abs/2312.01307 \
  项目主页：https://geometry.stanford.edu/projects/sage/ \
  代码：https://github.com/geng-haoran/SAGE

# 1.9 Tue
* 24、2024属于小模型时代？TinyLlama 等小模型爆火：参数轻量级，性能重量级！  AINLPer  https://mp.weixin.qq.com/s/P6Znjdi01KbvVYNH9EEsUw \
  轻量级模型，重量级性能，TinyLlama、LiteLlama小模型火起来了  机器学习研究组订阅  https://mp.weixin.qq.com/s/OOt0TrWb0DUcitb48jSoMg \
  该研究表示仅需 16 块 A100-40G 的 GPU，便可在 90 天内完成 TinyLlama 的训练。 \
  论文地址：https://arxiv.org/pdf/2401.02385.pdf \
  项目地址：https://github.com/jzhang38/TinyLlama/blob/main/README_zh-CN.md
* 25、奋战一年，LangChain首个稳定版本终于发布，LangGraph把智能体构建为图  机器之心  https://mp.weixin.qq.com/s/EOYTMs_ucGdr5P7_lABl_A \
  参考链接：https://blog.langchain.dev/langchain-v0-1-0/

# 1.10 Wed
* 26、多模态MLLM都是怎么实现的（1） 关于NLP那些你不知道的事儿  https://mp.weixin.qq.com/s/KbcYiVz43ia5umeCrOuTmA 
* 27、一条磁力链爆全网，Mixtral 8x7B论文来了！碾压Llama 2 70B，每token仅需激活13B参数  新智元  https://mp.weixin.qq.com/s/N2zCp2MpzJJaVEghcl_jKQ \
  Mixtral of Experts \
  论文地址：https://arxiv.org/abs/2401.04088 \
  https://github.com/mistralai/mistral-src \
  https://mistral.ai/news/mixtral-of-experts/
* 28、多模态智能体AI开启新浪潮！李飞飞等14位斯坦福微软大牛等撰写 80页《AGENT AI: 综述多模态交互的前沿展望》  专知  https://mp.weixin.qq.com/s/ckMMHdmuiWvW7n7cOVsh8g

# 1.11 Thu
* 29、西工大等最新《大型语言模型机器人技术》综述，详述多模态 GPT-4V 机器人技术  专知  https://mp.weixin.qq.com/s/g9dUd9nRZrVlu33rytl50w \

# 1.12 Fri
# 1.13 Sat
# 1.14 Sun

# 1.15 Mon
* 30、卡内基梅隆(CMU) | 发布万字综述，克服大模型(LLM)部署障碍，全面理解LLM当前状态！  AINLPer  https://mp.weixin.qq.com/s/ztnusTsEVog5Ej6c-U6-Eg \
  论文链接：https://arxiv.org/abs/2312.15234

# 1.16 Tue
* 31、国产GPTs来了，基于智谱第4代大模型！模型性能均达GPT-4九成以上  量子位  https://mp.weixin.qq.com/s/GYvFWj84kqWNLKHPgHIPbw \
  智谱AI
* 32、开源模型新纪录：超越Mixtral 8x7B Instruct的模型来了  机器之心  https://mp.weixin.qq.com/s/SH1AZ1wa0VylykaaSnxbCA \
  
# 1.17 Wed

# 1.18 Thu
* 33、新一代注意力机制Lightning Attention-2：无限序列长度、恒定算力开销、更高建模精度  机器之心  https://mp.weixin.qq.com/s/PN6e4iWVrg92HIrPxGQA7A \
  2024年 | 新一代线性注意力机制：Lightning Attention-2，大模型长序列问题或将解决！  AINLPer  https://mp.weixin.qq.com/s/oiPJilTa62O7atUx7IdcLQ \
  论文：Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models \
  论文地址：https://arxiv.org/pdf/2401.04658.pdf \
  开源地址：https://github.com/OpenNLPLab/lightning-attention
* 34、马毅LeCun谢赛宁曝出多模态LLM重大缺陷！开创性研究显著增强视觉理解能力  新智元  https://mp.weixin.qq.com/s/46DFuabZ6FJ4OHhn1pcKyw \
  来自纽约大学和UC伯克利的研究团队成功捕捉到了多模态大模型在视觉理解方面存在的重大缺陷。针对这个问题，他们进一步提出了一个将DINOv2特征与CLIP特征结合的方法，有效地提升了多模态大模型的视觉功能。 \
  论文:Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs \
  论文地址：https://arxiv.org/abs/2401.06209 \
  开源项目：https://github.com/tsb0601/MMVP

# 1.19 Fri
* 35、(**值得看看**)视觉Mamba来了：速度提升2.8倍，内存能省87%  机器之心  https://mp.weixin.qq.com/s/Cc9D5L8MPwvTti2OAM4w5w \
  论文地址：https://arxiv.org/pdf/2401.09417.pdf \
  项目地址：https://github.com/hustvl/Vim \
  论文标题：Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model \
  Vim 比 DeiT 快 2.8 倍，并节省 86.8% 的 GPU 内存。结果表明，Vim 能够克服对高分辨率图像执行 Transformer 式理解时的计算和内存限制，并且具有成为视觉基础模型的下一代骨干的巨大潜力。 \
  Mamba 的提出带动了研究者对状态空间模型（state space model，SSM）兴趣的增加，不同于 Transformer 中自注意力机制的计算量会随着上下文长度的增加呈平方级增长，由于 SSM 擅长捕捉远程依赖关系，因而开始受到大家追捧。

# 1.20 Sat
* 36、(**值得看看**)大模型自我奖励：Meta让Llama2自己给自己微调，性能超越了GPT-4  机器之心  https://mp.weixin.qq.com/s/tBVosNn07shQZxfvtSlaOw \
  一夜之间，Llama2击败GPT-4 ！| Meta提出自奖励语言模型，实现Llama2超进化！  AINLPer  https://mp.weixin.qq.com/s/0Hd3VsmPVMXWATytZQuOiQ \
  论文标题：Self-Rewarding Language Models \
  论文链接：https://arxiv.org/abs/2401.10020 \
  模型生成训练数据，并评估这些数据的质量，然后用这些数据来自己训练自己。

# 1.21 Sun
* 37、【伯克利博士论文】通过生成式模型实现视觉与语言理解，109页pdf  专知  https://mp.weixin.qq.com/s/Pc-XJTlL1tvTZrVBfPbaCQ 
* 38、视觉Mamba模型的Swin时刻，中国科学院、华为等推出VMamba  机器之心  https://mp.weixin.qq.com/s/1h4z2Kb0B5GTPnSUUU8c8g \
  论文标题：VMamba: Visual State Space Model \
  论文地址: https://arxiv.org/abs/2401.10166 \
  代码地址: https://github.com/MzeroMiko/VMamba \
  CNN 和视觉 Transformer（ViT）是当前最主流的两类基础视觉模型。尽管 CNN 具有线性复杂度，ViT 具有更为强大的数据拟合能力，然而代价是计算复杂较高。研究者认为 ViT 之所以拟合能力强，是因为其具有全局感受野和动态权重。受 Mamba 模型的启发，研究者设计出一种在线性复杂度下同时具有这两种优秀性质的模型，即 Visual State Space Model（VMamba）。大量的实验证明，VMamba 在各种视觉任务中表现卓越。如下图所示，VMamba-S 在 ImageNet-1K 上达到 83.5% 的正确率，比 Vim-S 高 3.2%，比 Swin-S 高 0.5%。
* 39、(**值得玩玩**)Agent触摸汉堡辨冷热，首次拥有类人感官！UCLA等发布3D多模态交互具身智能大模型  新智元  https://mp.weixin.qq.com/s/37_SuI4O2CwTlSVDki5CjA \
  论文：MultiPLY：A Multisensory Object-Centric Embodied Large Language Model in 3D World \
  论文地址：https://arxiv.org/abs/2401.08577 \
  参考资料：https://vis-www.cs.umass.edu/multiply/ \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/cecdbafd-10b6-4fee-af39-69f23aa2df51)

# 1.22 Mon

# 1.23 Tue
* 40、字节发布机器人领域首个开源视觉-语言操作大模型，激发开源VLMs更大潜能  PaperWeekly  https://mp.weixin.qq.com/s/JgnZobJ-_HEBkYIn_m_Nyg
* 41、(**可以看看**)LLM会写代码≠推理+规划！AAAI主席揭秘：代码数据质量太高｜LeCun力赞  新智元  https://mp.weixin.qq.com/s/nQqzJP2dpBsD1Dl-Lb3oLA \
  论文：Can LLMs Really Reason & Plan \
  视频链接：https://www.youtube.com/watch?v=uTXXYi75QCU \
  PPT链接：https://www.dropbox.com/scl/fi/g3qm2zevcfkp73wik2bz2/SCAI-AI-Day-talk-Final-as-given.pdf
* 42、Yann LeCun：生成模型不适合处理视频，AI得在抽象空间中进行预测  机器之心  https://mp.weixin.qq.com/s/sAWFkcTFfZVJ_oLKditqVA \
  适合用来处理视频的模型并不是我们现在大范围应用的生成模型。而且新的模型应该学会在抽象的表征空间中预测，而不是在像素空间中。\
  (好像又在讲世界模型) \
  原视频地址：https://www.weforum.org/events/world-economic-forum-annual-meeting-2024/sessions/the-expanding-universe-of-generative-models/
* 43、MoE与Mamba强强联合，将状态空间模型扩展到数百亿参数  机器之心  https://mp.weixin.qq.com/s/eXe3KjcHVJAk9t3VfZ9z0Q \
  碾压 Transformer，Mamba联合MoE，成本减少2.2倍，SSM或将扩至百亿参数！  AINLPer  https://mp.weixin.qq.com/s/xzW9EtHRRhcXRH5ghf8FZQ \
  论文：MoE-Mamba：Efficient Selective State Space Models with Mixture of Experts \
  论文地址：https://arxiv.org/pdf/2401.04081.pdf

# 1.24 Wed
* 44、大模型+自动驾驶？华为等最新《为自动驾驶打造视觉基础模型》综述，涵盖250篇文献详述其挑战、方法论和机遇  专知  https://mp.weixin.qq.com/s/Aa64VL3nO9S2Ot-7oEebog
* 45、买个机器人端茶倒水有希望了？Meta、纽约大学造了一个OK-Robot  机器之心 https://mp.weixin.qq.com/s/rRkKySaIbIsRIQk-kErU0w \
  论文标题：OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics \
  论文链接：https://arxiv.org/pdf/2401.12202.pdf

# 1.25 Thu
* 46、拳打Gen-2脚踢Pika，谷歌爆肝7个月祭出AI视频大模型！首提时空架构，时长史诗级延长  机器学习研究组订阅  https://mp.weixin.qq.com/s/zi_3yDff7sbPfzbhxjxDtw \
  论文：Lumiere: A Space-Time Diffusion Model for Video Generation \
  论文地址：https://arxiv.org/abs/2401.12945
* 47、「think step by step」还不够，让模型「think more steps」更有用  机器学习研究组订阅  https://mp.weixin.qq.com/s/HUtcytlMIzBl-hnrSp6wPw \
  本文对思维链的推理步长进行了控制变量实验，发现推理步长和答案的准确性是线性相关的，这种影响机制甚至超越了问题本身所产生的差异。\
  论文标题：The Impact of Reasoning Step Length on Large Language Models \
  论文链接：https://arxiv.org/pdf/2401.04925.pdf
* 48、(**可以看看**)聊聊LLM Agents的现状，问题与未来  关于NLP那些你不知道的事  https://mp.weixin.qq.com/s/kZ8he03yXTfEW2KUiA4zvw 
* 49、大模型如何可解释？帝国理工最新《大型语言模型的解释性》最新综述  专知  https://mp.weixin.qq.com/s/kW5qpJQQ74wo8LfiiGgeCA \
  From Understanding to Utilization: A Survey on Explainability for Large Language Models \
  https://arxiv.org/pdf/2401.12874.pdf

# 1.26 Fri
* 50、《多模态大型语言模型》最新进展，详述26种现有MM-LLMs  专知  https://mp.weixin.qq.com/s/PnzYvlfwS0m97ocD4zjD1w \
  MM-LLMs: Recent Advances in MultiModal Large Language Models

# 1.27 Sat
* 51、何恺明谢赛宁团队步步解构扩散模型，最后竟成经典去噪自编码器  机器之心  https://mp.weixin.qq.com/s/tP_evn-Rn3sy7_R3pQeghQ \
  论文标题：Deconstructing Denoising Diffusion Models for Self-Supervised Learning \
  论文地址：https://arxiv.org/pdf/2401.14404.pdf
* 52、Llama-2+Mistral+MPT=? 融合多个异构大模型显奇效  机器之心  https://mp.weixin.qq.com/s/7JJBB_lNHKVoeaMtvCEyfQ \
  论文标题：Knowledge Fusion of Large Language Models \
  论文地址：https://arxiv.org/abs/2401.10491 \
  论文仓库：https://github.com/fanqiwan/FuseLLM
* 53、(**非常值得看看**)CMU华人18万打造高能机器人，完爆斯坦福炒虾机器人！全自主操作，1小时学会开12种门  新智元  https://mp.weixin.qq.com/s/m4Fydw5W0wqFryvoNxkmvg \
  Adaptive Mobile Manipulation for Articulated Objects In the Open World \
  论文地址：https://arxiv.org/abs/2401.14403
* 54、文本直接在3D场景中生成对象，谷歌推出InseRF模型  AIGC开放社区  https://mp.weixin.qq.com/s/uPid0qkT9VKiPnL_vVqvgg \
  https://mp.weixin.qq.com/s/uPid0qkT9VKiPnL_vVqvgg
* 55、模型压缩：CNN和Transformer通用，修剪后精度几乎无损，速度提升40%  计算机视觉研究院  https://mp.weixin.qq.com/s/E3mRAKn_Nq3_-lnDRhjXyw \
  论文地址：https://arxiv.org/pdf/2401.06426.pdf

# 1.28 Sun
* 56、(**值得看看**)从20亿数据中学习物理世界，基于Transformer的通用世界模型成功挑战视频生成  量子位  https://mp.weixin.qq.com/s/UYfzovdDQ_K9j4Zlyz1qDw \
  清华和极佳科技  WorldDreamer \
  论文地址：https://arxiv.org/abs/2401.09985 \
  项目主页：https://world-dreamer.github.io/
* 57、161页《大模型推理》最新综述，涵盖650多篇大模型论文  专知  https://mp.weixin.qq.com/s/HK29hRpMiblDntA6MB72Pw 
* 58、OpenAI、斯坦福大学提出Meta-Prompting，有效提升语言模型的性能  夕小瑶科技说  https://mp.weixin.qq.com/s/ton58dImkg2VBiH57rlyUg \
  论文题目:Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding \
  论文链接:https://arxiv.org/abs/2401.12954 \
  元提示（meta-prompting）的核心思想在于使用一个模型来协调和执行多个独立的提问，然后综合它们的回复以生成最终的答案。
* 59、Meta发布自我奖励机制，Llama在3轮训练后超越GPT-4  夕小瑶科技说  https://mp.weixin.qq.com/s/Uo75sGR6s2-pZ9m5GLQObg \
  GPT-4准确率最高飙升64%！斯坦福OpenAI重磅研究：全新Meta-Prompting方法让LLM当老板  新智元  https://mp.weixin.qq.com/s/y-bMFqpu7cuxpSAzbyt4cQ \
  论文标题：Self-Rewarding Language Models \
  论文链接：https://arxiv.org/pdf/2401.10020.pdf \
  文中提出了用自我奖励的方法使得模型在训练过程中自行提供奖励，从而提升模型执行指令和自我奖励的能力。
* 60、通用智能人小女孩“通通”（Little Girl）惊喜亮相，为通研院两周岁庆生！  北京通用人工智能研究院  https://mp.weixin.qq.com/s/57RxlGEJQ_Xr6zF56jSm5A

# 1.29 Mon
* 61、(**值得玩玩**)从零手搓MoE大模型，大神级教程来了  量子位  https://mp.weixin.qq.com/s/gDtzzSRunUrKjoIUGSHCvA \
  原文地址：https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch \
  笔记版本（GitHub）：https://github.com/AviSoori1x/makeMoE/tree/main

# 1.30 Tue
* 62、大模型也能切片，微软SliceGPT让LLAMA-2计算效率大增  机器之心  https://mp.weixin.qq.com/s/HlgpNkjZAQm7Q-ffk3Qfmw \
  论文标题：SLICEGPT: COMPRESS LARGE LANGUAGE MODELS BY DELETING ROWS AND COLUMNS \
  论文链接：https://arxiv.org/pdf/2401.15024.pdf \
  SliceGPT 的核心思想是删除权重矩阵中的行和列来降低网络的嵌入维数，同时保持模型性能。(一种模型压缩技术)

# 1.31 Wed
* 63、伯克利开源高质量大型机器人操控基准，面对复杂自主操控任务不再犯难  机器之心  https://mp.weixin.qq.com/s/jF2CmIlt3afb99KpmbjhkQ \
  项目主页：https://functional-manipulation-benchmark.github.io/ \
  论文地址：https://arxiv.org/abs/2401.08553 \
  论文题目：FMB: a Functional Manipulation Benchmark for Generalizable Robotic Learning \
  共同第一作者主页：https://people.eecs.berkeley.edu/~jianlanluo/ \
  https://charlesxu0124.github.io/
* 64、将多模态大模型稀疏化，3B模型MoE-LLaVA媲美LLaVA-1.5-7B  机器之心  https://mp.weixin.qq.com/s/ICylR6n2LhqQRS0CAHFI1A \
  论文地址：https://arxiv.org/abs/2401.15947 \
  项目地址：https://github.com/PKU-YuanGroup/MoE-LLaVA \
  Demo地址：https://huggingface.co/spaces/LanguageBind/MoE-LLaVA \
  论文题目：MoE-LLaVA: Mixture of Experts for Large Vision-Language Models
