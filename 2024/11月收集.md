# 11.1 Fri
* 1.强化学习之父Richard Sutton给出一个简单思路，大幅增强所有RL算法  机器之心 
 https://mp.weixin.qq.com/s/lwoq764gVSFjsEhzPS3ChQ \
  Reward Centering(奖励聚中) \
  从奖励中减去实际观察到的奖励的平均值。这样做会让修改后的奖励看起来以均值为中心 \
  总之，实验表明，奖励聚中可以提高 Q 学习算法的表格、线性和非线性变体在多种问题上的性能。当折现因子接近 1 时，学习率的提升会更大。此外，该算法对问题奖励变化的稳健性也有所提升。
* 2.AI自己「长出」了类似大脑的「脑叶」？新研究揭示LLM特征的惊人几何结构  机器之心  https://mp.weixin.qq.com/s/GCoz5eEp1vnQfcTEs3UQqA \
  The Geometry of Concepts: Sparse Autoencoder Feature Structure 
* 3.机器人迈向ChatGPT时刻！清华团队首次发现具身智能Scaling Laws  机器之心  https://mp.weixin.qq.com/s/hJjE_C3KMn7gKjIvfXMhGg \
  Data Scaling Laws in Imitation Learning for Robotic Manipulation \
  https://data-scaling-laws.github.io/
* 4.NeurIPS 2024 | 机器人操纵世界模型来了，成功率超过谷歌RT-1 26.6%  机器之心  https://mp.weixin.qq.com/s/PErDL3i8JTpElmfxuwup0Q \
  来自中山大学和华为诺亚等单位的研究团队提出了一种全新的原语驱动的路径点感知世界模型，借助 VLMs 作为机器人的大脑，理解任务之间的动作关联性，并通过 “世界模型” 获取对未来动作的表征，从而更好地帮助机器人学习和决策 \
  PIVOT-R: Primitive-Driven Waypoint-Aware World Modelfor Robotic Manipulation \
  https://abliao.github.io/PIVOT-R/ \
  ???世界模型怎么用的
* 5.打破RLHF瓶颈，克服奖励欺骗！Meta发布全新后训练方式CGPO，编程水平直升5%  新智元  https://mp.weixin.qq.com/s/OL4atcyG0xY1f9ElCQ_KRA 
* 6.清华团队革新MoE架构！像搭积木一样构建大模型，提出新型类脑稀疏模块化架构  量子位  https://mp.weixin.qq.com/s/9Yr5FFoBLcGQizTAbxCZVw \
  Confgurable Foundation Models: Building LLMs from a Modular Perspective
* 7.无需游戏引擎，大模型实时生成“我的世界”，每秒20帧零延迟可交互，已开源  量子位  https://mp.weixin.qq.com/s/ISQj7nZJgMOT6_g7GFFQ-A \
  oasis https://oasis-model.github.io/
* 8.冯·诺依曼的遗产：寻找人工生命的理论根源  集智俱乐部  https://mp.weixin.qq.com/s/3fUQ6zdgaaeTfgGgk1YGug 

# 11.2 Sat
* 9.微软开源视觉GUI智能体：增强GPT-4V能力，超3800颗星  AIGC开放社区  https://mp.weixin.qq.com/s/w2M_EcdotEdQDrDe2gGeqw \
  OmniParser: Screen Parsing tool for Pure Vision BasedGUl Agent \
  https://github.com/microsoft/OmniParser
* 10.(**nerf综述，值得看看**)机器人中的神经场：综述  专知  https://mp.weixin.qq.com/s/vU3rdDIfwulYBfUTOojz3w \
  Neural Fields in Robotics: A Survey \
  关键词—神经辐射场（Neural Radiance Field, NeRF）、神经场（Neural Fields）、符号距离场（Signed Distance Fields）、3D高斯分布（3D Gaussian Splatting）、占用网络（Occupancy Networks）、计算机视觉、新视角合成（Novel View Synthesis）、神经渲染（Neural Rendering）、体渲染（Volume Rendering）、姿态估计、机器人、操控、导航、自动驾驶
* 11.(**看不懂**)3个思想实验撕裂时空！实验证实：人类居住时空并非物理实体，而是近似  新智元  https://mp.weixin.qq.com/s/hiytAFR9AUHJzufKtW0qZA \
  The Thought Experiments That Fray the Fabric of Space-Time
* 12.阿里千问提出Self-Lengthen，大模型实现自迭代扩展输出长度 
 PaperWeekly  https://mp.weixin.qq.com/s/1m1hUkhs3altxjYP6IxUVw \
  Language Models Can Self-Lengthen to Generate Long Texts \
  https://github.com/QwenLM/Self-Lengthen
* 13.3B模型打通机器人任督二脉！冲咖啡叠衣服都能干，7种形态适配，OpenAI也投了  量子位  https://mp.weixin.qq.com/s/AHll8FDtbb8qZv-SlUOfGQ \
  技术报告：https://www.physicalintelligence.company/download/pi0.pdf \
  π0 3B
* 14.StaR ｜ 用少量推理数据让模型学会通用推理能力，显著提升模型复杂推理  NLP PaperWeekly  https://mp.weixin.qq.com/s/BmCIDx5wPQPqvz6DljZMlA \
  STaR: Bootstrapping Reasoning With Reasoning

# 11.3 Sun
* 15.人工智能综述：物理学与人工智能的跨界新范式（全文版本）  图灵人工智能  https://mp.weixin.qq.com/s/XmxI382KHearcuSZKJwdMw
* 16.RAG新突破：块状注意力机制实现超低延迟检索增强  机器之心  https://mp.weixin.qq.com/s/yv2iIpaJTi4g4nhZG1WLZw \
  Block-Attention for Efficient RAG \
  论文《Block-Attention for Efficient RAG》为检索增强 (RAG) 场景实现了一种块状注意力机制，Block-Attention，通过分块独立编码检索到的文档，使得模型无需重复编码计算已经在其他 query 中已经见过的文档，从而实现线上推理效率的有效提升
* 17.LLM 比之前预想的更像人类，竟也能「三省吾身」  机器之心  https://mp.weixin.qq.com/s/Ri-Wdl_Xk5OxWF5IIJmrxg \
  Looking Inward: Language Models Can Learn About Themselves by Introspection \
  结论：\
  1.LLM 可以获得无法从其训练数据中推断出的知识。\
  2.这种对关于自身的某些事实的「特权访问」与人类内省的某些方面有关联。
* 18.o1满血版泄露！奥数题图片推理手拿把掐，奥特曼上线剧透o2  量子位  https://mp.weixin.qq.com/s/hzXIVc0wYJV3mOaAE8H-tA
* 19.微软清华改进Transformer：用降噪耳机原理升级注意力，一作在线答疑  量子位  https://mp.weixin.qq.com/s/btZB8tDNkvZux-XNivMObQ \
  具体到在语言模型中，如果句子很长，只有少数token会真正影响当前token的含义。而注意力机制允许每两个词之间产生交互，其中就包含大量噪声了。团队提出的方法是在注意力层中增加一个Softmax，然后两个Softmax做减法。这一减，噪音信息就被大幅抵消，让注意力更集中在相关内容上。\
  DIFFERENTIAL TRANSFORMER
* 20.(**有趣，非常值得看看**)世界首个1000亿AI智能体文明诞生！北大校友打造真实版「西部世界」，技术细节全公开  新智元  https://mp.weixin.qq.com/s/cstOIX-K_H1FgR9M_Nzj0w \
  1000个智能体打造《我的世界》，北大校友35页技术报告揭秘  量子位  https://mp.weixin.qq.com/s/fad7tjxNlwfVn0HNCQTcGQ \
  Project Sid: Many-agent simulations toward Al civilization \
  https://github.com/altera-al/project-sid \
  PIANO（并行信息聚合神经协调），是一个能让AI智能体实现多方互动的架构，同时在多个输出流中保持连贯性 \
  这些智能体，构筑了一个「文明」。纳税、贸易、政府、国家、宗教....，一切人类世界所有的日常，AI智能体也有。

# 11.4 Mon
* 21.大型模型中的参数高效微调：方法论综述  专知  https://mp.weixin.qq.com/s/p8HmBwJGv2E908Fu5Oc1Iw
* 22.慢思考准确率反降30%！普林斯顿揭示思维链某些任务上失效的秘密  量子位  https://mp.weixin.qq.com/s/sFaPB8CxtYj0HIi_5bA74Q \
  研究实锤：别让大模型「想」太多，OpenAI o1准确率竟下降36.3% 
  夕小瑶科技说  https://mp.weixin.qq.com/s/UPO-QfBAMafrq_Ewxeye8Q \
  MIND YOUR STEP(BY STEP):CHAIN-OF-THOUGHTCAN REDUCE PERFORMANCEONTASKS WHERETHINKING MAKES HUMANS WORSE

# 11.5 Tue
* 23.Llama版o1来了，来自上海AI Lab，强化学习代码已开源，基于AlphaGo Zero范式  量子位  https://mp.weixin.qq.com/s/2vRvD4x6WWihmnbrWG33SQ \
  https://github.com/SimpleBerry/LLaMA-O1 \
  上海AI Lab LLaMA版o1 \
  上交大团队 O1-Journey 
* 24.(**非常值得看看**)LLM超越人类时该如何对齐？谷歌用新RLHF框架解决了这个问题 
 AINLPer  https://mp.weixin.qq.com/s/IGqWUiYzE4j8h6FTMAERww \
  随着 LLM 能力越来越强，它们将能解决越来越复杂和越来越多的难题，而这些难题所需的训练数据已经超出了人类的能力 \
  因此，我们就需要为 LLM 构建一种能使其实现自我提升的基本机制，让模型可以持续地自我生成和自我求解更困难的问题 \
  语言模型能否自我创建可学习的新任务，从而实现自我改进以更好地泛化用于人类偏好对齐？\
  为了提升语言模型的对齐能力，人们已经提出了许多偏好优化算法，但它们都默认使用固定的提示词训练分布。这种固定的训练范式**缺乏可扩展性**，并不可避免地导致泛化问题和效率问题。\
  基于这些考虑，谷歌 DeepMind 和芝加哥大学一个研究团队开发了一种可扩展的开放式 RLHF 框架 eva，即 Evolving Alignment via Asymmetric Self-Play，也就是「通过非对称自博弈实现的演进式对齐」。\
  Evolving Alignment via Asymmetric Self-Play

# 11.6 Wed
* 25.OpenAI重拾规则系统，用「AI版机器人定律」守护大模型安全 
 机器之心  https://mp.weixin.qq.com/s/amO3828miwEnr8aQBLpW9A \
  Rule Based Rewards for Language Model Safety \
  https://github.com/openai/safety-rbr-code-and-data
* 26.沿着人类思路探索：达摩院提出创意链，使大模型生成人类水平的科研Idea  PaperWeekly  https://mp.weixin.qq.com/s/7yV6PABIr6QAzlbUUY9dAA \
  Chain of Ideas: Revolutionizing Research in Novel Idea Development with LLM Agents
* 27.「黑神话」级3A大作AI实时游戏生成！港科大、中科大等祭出最强扩散Transformer，火爆国外  新智元  https://mp.weixin.qq.com/s/b1H_8JXBpsiXb-3BmrhrFA \
  GameGen-X: Open-world Video Game Generation
  https://gamegen-x.github.io/
* 28.不靠更复杂的策略，仅凭和大模型训练对齐，零样本零经验单LLM调用，成为网络任务智能体新SOTA  机器之心  https://mp.weixin.qq.com/s/UvNCUVBbH9TqfbEdoB7mTA \
  AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents

# 11.7 Thur
* 29.玩转「智能体魔方」！清华推出AgentSquare模块化搜索框架，开启AI智能体高速进化时代  新智元  https://mp.weixin.qq.com/s/NL7AnKb1WBM1_CywNM7uyQ \
  清华大学团队提出了AgentSquare模块化智能体设计框架，通过标准化的模块接口抽象，让AI智能体可以通过模块演化和重组高速进化，实现针对不同任务场景的自适应演进，赋能超越人类设计的智能体系统在多种评测数据集上广泛自我涌现 \
  AgentSquare: Automatic LLM Agent Search In Modular Design Space

# 11.8 Fri
* 30.(**非常值得看看，用视频生成模型做世界模型还缺什么？**)LeCun赞转！类Sora模型能否理解物理规律？字节豆包大模型团队系统性研究揭秘  机器之心  https://mp.weixin.qq.com/s/mwm6UgJByVOdnZHIgcMngA \
  Keras 之父 François Chollet 则认为，Sora 这样的视频生成模型确实嵌入了「物理模型」，但问题是：这个物理模型是否准确？它能否泛化到新的情况，即那些不仅仅是训练数据插值的情形？这些问题至关重要，决定了生成图像的应用范围 —— 是仅限于媒体生产，还是可以用作现实世界的可靠模拟。最后他指出，不能简单地通过拟合大量数据来期望得到一个能够泛化到现实世界所有可能情况的模型 \
  字节豆包大模型团队通过大规模实验发现 —— 即便依照 Scaling Law 扩大模型参数与训练数据量，模型依然无法抽象出一般物理规则，甚至连牛顿第一定律、抛物线运动都无法领会 \
  视频生成模型并没有在学习、理解物理规律 \
  How Far is Video Generation from World Model: A Physical Law Perspective \
  研究中也有一个好消息：如果训练视频中所有概念和物体都是模型已熟悉的，此时加大训练视频的复杂度，比如组合增加物体间的物理交互，通过加大训练数据，模型对物理规律的遵循将越来越好。这一结果可为视频生成模型继续提升表现提供启发
* 31.智能体首次达到Kaggle Grandmaster水平，华为用结构化推理补齐思维链短板  机器之心  https://mp.weixin.qq.com/s/w3nIhxeQMuTdqFDuqXsAYA \
  Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level

# 11.9 Sat
* 32.MetaGPT开源自动生成智能体工作流，4.55%成本超GPT-4o  机器之心  https://mp.weixin.qq.com/s/5YpPFYIpuCkSf0sJp0_RnQ \
  MetaGPT 开源了 AFLOW，它使用 MCTS 进行 Agentic Workflow 的自动搜索，可以完全自动地构建与优化 Agentic Workflow 问题，让我们不再需要手写代码、调试提示词 \
  AFlow: Automating Agentic Workflow Generation \
  https://github.com/geekan/MetaGPT/tree/main/examples/aflow

# 11.10 Sun
* 33.NeurIPS 2024 (Oral) | 如何量化与提升思维链的推理能力边界？  机器之心  https://mp.weixin.qq.com/s/BwuGacSHKY4RTdvYNMa66Q \
  Unlocking the Capabilities of Thought:A Reasoning Boundary Framework to Quantify andOptimize Chain-of-Thought \
  https://github.com/LightChen233/reasoning-boundary
* 34.高能干货分享，有关提示词工程的一切都在这份教程里  机器之心  https://mp.weixin.qq.com/s/RaIzHtRIShIcpXydRE6kQg \
  https://github.com/NirDiamant/Prompt_Engineering
* 35.谷歌苹果曝出LLM惊人内幕，自主识别错误却装糊涂！AI幻觉背后藏着更大秘密  新智元  https://mp.weixin.qq.com/s/u_h6qwmHdXI74_9feKNeEw \
  LLMS KNOW MORE THAN THEY SHOW: ON THE INTRINSIC REPRESENTATION OF LLM HALLUCINATIONS
* 36.(**值得看看**)空间智能版ImageNet来了！李飞飞吴佳俊团队出品  量子位  https://mp.weixin.qq.com/s/_yi69foQdQzhOazgUqiiZA \
  HourVideo:1-Hour Video-Language Understanding \
  团队提出了一个新的任务对应套件，包含总结、感知（回忆、跟踪）、视觉推理（空间、时间、预测、因果、反事实）和导航（房间到房间、对象检索）任务，共18个子任务

# 11.11 Mon
* 37.LoRA、完全微调到底有何不同？MIT 21页论文讲明白了  机器之心  https://mp.weixin.qq.com/s/Xxh-MrSfkRlRz7reE-fkzA \
  LORA VS FULL FINE-TUNING: AN ILLUSION OF EQUIVALENCE 
* 38.GitHub超火开发者路线图库有AI学习路线了！star数近30万  机器之心  https://mp.weixin.qq.com/s/P9hWHGsiWcfEfMq54E02xg \
  https://github.com/kamranahmedse/developer-roadmap \
  https://roadmap.sh
* 39.专家模型不要专家并行！微软开源MoE新路径  新智元  https://mp.weixin.qq.com/s/pt-AlH_z4e3PNiKC9Iyz7A \
  GRIN:GRadient-INformed MoE

# 11.12 Tue
* 40.最强开源CodeLLM模型深夜来袭！320亿参数，Qwen2.5-Coder新模型超越GPT-4o  量子位  https://mp.weixin.qq.com/s/K55WD7466VMTGiYJVsMZUA
* 41.开源版SearchGPT来了，两张3090就可复现，超越Perplexity付费版  量子位  https://mp.weixin.qq.com/s/xa0oS-LPnZAQqEyUozf_eg \
  港中文MMLab、上海AI Lab、腾讯团队简易实现了Vision Search Assistant，模型设计简单，只要两张RTX3090就可复现 \
  Vision Search Assistant: Empower Vision-Language Models as MultimodalSearch Engines \
  https://cnzzx.github.io/VSA/
* 42.(**值得了解**)o1不是唯一路径！MIT新研究：在测试时训练，模型推理能力最高升至5.8倍 
 量子位  https://mp.weixin.qq.com/s/sRv-BswlDOn-CFtR-rRqDQ \
 连OpenAI都推不动Scaling Law了？MIT把「测试时训练」系统研究了一遍，发现还有路  机器之心  https://mp.weixin.qq.com/s/tfrG21mfteVAkjqYx5mDsQ \
  Scaling Law还有救？MIT破解LLM进化新路！测试时训练让性能暴涨6倍，准确率媲美人类  新智元  https://mp.weixin.qq.com/s/dhAJVUpx0Il90w2y8cprTQ \
  开源：Test-Time Training 巨大提升抽象推理  CreateAMind  https://mp.weixin.qq.com/s/DfTiYmZpBUONL61WoPf-XA \
  MIT的新研究发现，在测试时对大模型进行训练，可以让推理水平大幅提升 \
  TTT: test-time training \
  https://ekinakyurek.github.io/papers/ttt.pdf \
  不同于传统的先训练后测试模式，测试时训练（Test-Time Training，TTT）在部署阶段面对新的测试样本时，不直接用训练好的模型去推理。在推理之前，测试样本自身携带的信息，会通过快速的训练过程被用于调整模型参数
* 43.Ilya认错，Scaling Law崩了？自曝SSI秘密技术路线取代OpenAI  新智元  https://mp.weixin.qq.com/s/EbO4lhGH1kwQstDSBoTHww \
  Ilya承认Scaling Law到头了，Meta和谷歌寻找替代o1的方法  夕小瑶科技说  https://mp.weixin.qq.com/s/i0dTLDrEEznh4gn8cSsgEw 
* 44.深度揭秘CoT！普林斯顿耶鲁发布最新报告：大模型既有记忆推理、也有概率推理  新智元  https://mp.weixin.qq.com/s/PtuCL1WBRvrFr7nkJaKgrQ \
  Deciphering the Factors Infuencing the Effcacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning

# 11.13 Wed
* 45.突破次元壁！新加坡国立发布GenXD：拿捏真实感3D、4D动态场景  新智元  https://mp.weixin.qq.com/s/30rBNR1Io9w6nVuOxR2uLw \
  GENXD:GENERATING ANY 3D AND 4D SCENES \
  https://gen-x-d.github.io/ \
  https://github.com/HeliosZhao/GenXD
* 46.Nature：AI也许可以拥有常识，但不是现在  新智元  https://mp.weixin.qq.com/s/S8YkHt6q4KCvSuNSfTdCUw 
* 47.UIUC | 提出“提取-精炼-检索-读取”框架：ERRR，提升RAG系统性能 
  AINLPer  https://mp.weixin.qq.com/s/yjwuJNtftiHYqtsT66mp8w \
  Query Optimization for Parametric Knowledge Refinement inRetrieval-Augmented Large Language Models

# 11.14 Thur
* 48.(**有趣，值得了解**)Token化一切，甚至网络！北大&谷歌&马普所提出TokenFormer，Transformer从来没有这么灵活过！  机器之心  https://mp.weixin.qq.com/s/1wOGd1h5y5DjpAt1-n5WDA \
  TokenFormer 不仅像原始 Transformer 一样 Token 化了 input data，并且 Token 化了网络参数，将 attention 机制拓展到 Token 和 parameters 的交互中，最大化了 Transformer 的灵活性，真正得到了一个 Fully attention-based 的网络结构。\
  这种方式打破了原有人们区别看待 data 和 model 的观念，即所有的计算都归纳为不同类型的 Token（e.g., data, param token）通过灵活的 attention 来交互。得益于这一灵活的性质，TokenFormer 允许 incremental scaling model size，基于训好的模型上增量的拓展新的更大的模型，大大节省了计算的开销 \
  TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters \
  https://github.com/Haiyang-W/TokenFormer
* 49.李飞飞吴佳俊团队新作：推出具身智能决策能力评价基准，o1-preview登顶 | NeurIPS  量子位  https://mp.weixin.qq.com/s/ixiGVdBZKm4-wnmH3KoHRg \
  大模型的具身智能决策能力，终于有系统的通用评估基准了。李飞飞吴佳俊团队新提出的评估框架，对具身智能决策的四项关键子能力来了个全面检查 \
  该框架名为Embodied Agent Interface（简称EAI），提供了连接不同模块和基准环境的标准接口 \
  为了更深入理解大模型的行为模式和优劣势分布，EAI提出了四个关键能力模块:目标解释、子目标分解、动作序列规划、转换建模 \
  https://embodied-agent-interface.github.io/

# 11.15 Fri
* 50.**视觉自回归模型综述**  专知  https://mp.weixin.qq.com/s/O9gFoCRdHa7YOPgjPwNCqQ \
  Autoregressive Models in Vision: A Survey \
  https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey
* 51.(**有趣**)AI在《我的世界》PK盖楼，新旧Claude差距过于明显，网友：审美也是智力的一种  量子位  https://mp.weixin.qq.com/s/_ZkhXxvEpe3ZOWZq4wC3cQ \
  视频教程：https://x.com/mckaywrigley/status/1849613686098506064 \
  开源代码：\
  https://github.com/kolbytn/mindcraft \
  https://github.com/mc-bench/orchestrator
* 52.Make U-Nets Great Again！北大&华为提出扩散架构U-DiT，六分之一算力即可超越DiT  机器之心  https://mp.weixin.qq.com/s/IhlH4CyvM6hFke-8flnscA \
  U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers
* 53.
* 54.
* 55.

# 11.16 Sat
# 11.17 Sun

# 11.18 Mon
# 11.19 Tue
# 11.20 Wed
# 11.21 Thur
# 11.22 Fri
# 11.23 Sat
# 11.24 Sun

# 11.25 Mon
# 11.26 Tue
# 11.27 Wed
# 11.28 Thur
# 11.29 Fri
# 11.30 Sat
