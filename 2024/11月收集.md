# 11.1 Fri
* 1.强化学习之父Richard Sutton给出一个简单思路，大幅增强所有RL算法  机器之心 
 https://mp.weixin.qq.com/s/lwoq764gVSFjsEhzPS3ChQ \
  Reward Centering(奖励聚中) \
  从奖励中减去实际观察到的奖励的平均值。这样做会让修改后的奖励看起来以均值为中心 \
  总之，实验表明，奖励聚中可以提高 Q 学习算法的表格、线性和非线性变体在多种问题上的性能。当折现因子接近 1 时，学习率的提升会更大。此外，该算法对问题奖励变化的稳健性也有所提升。
* 2.AI自己「长出」了类似大脑的「脑叶」？新研究揭示LLM特征的惊人几何结构  机器之心  https://mp.weixin.qq.com/s/GCoz5eEp1vnQfcTEs3UQqA \
  The Geometry of Concepts: Sparse Autoencoder Feature Structure 
* 3.机器人迈向ChatGPT时刻！清华团队首次发现具身智能Scaling Laws  机器之心  https://mp.weixin.qq.com/s/hJjE_C3KMn7gKjIvfXMhGg \
  Data Scaling Laws in Imitation Learning for Robotic Manipulation \
  https://data-scaling-laws.github.io/
* 4.NeurIPS 2024 | 机器人操纵世界模型来了，成功率超过谷歌RT-1 26.6%  机器之心  https://mp.weixin.qq.com/s/PErDL3i8JTpElmfxuwup0Q \
  来自中山大学和华为诺亚等单位的研究团队提出了一种全新的原语驱动的路径点感知世界模型，借助 VLMs 作为机器人的大脑，理解任务之间的动作关联性，并通过 “世界模型” 获取对未来动作的表征，从而更好地帮助机器人学习和决策 \
  PIVOT-R: Primitive-Driven Waypoint-Aware World Modelfor Robotic Manipulation \
  https://abliao.github.io/PIVOT-R/ \
  ???世界模型怎么用的
* 5.打破RLHF瓶颈，克服奖励欺骗！Meta发布全新后训练方式CGPO，编程水平直升5%  新智元  https://mp.weixin.qq.com/s/OL4atcyG0xY1f9ElCQ_KRA 
* 6.清华团队革新MoE架构！像搭积木一样构建大模型，提出新型类脑稀疏模块化架构  量子位  https://mp.weixin.qq.com/s/9Yr5FFoBLcGQizTAbxCZVw \
  Confgurable Foundation Models: Building LLMs from a Modular Perspective
* 7.无需游戏引擎，大模型实时生成“我的世界”，每秒20帧零延迟可交互，已开源  量子位  https://mp.weixin.qq.com/s/ISQj7nZJgMOT6_g7GFFQ-A \
  oasis https://oasis-model.github.io/
* 8.冯·诺依曼的遗产：寻找人工生命的理论根源  集智俱乐部  https://mp.weixin.qq.com/s/3fUQ6zdgaaeTfgGgk1YGug 

# 11.2 Sat
* 9.微软开源视觉GUI智能体：增强GPT-4V能力，超3800颗星  AIGC开放社区  https://mp.weixin.qq.com/s/w2M_EcdotEdQDrDe2gGeqw \
  OmniParser: Screen Parsing tool for Pure Vision BasedGUl Agent \
  https://github.com/microsoft/OmniParser
* 10.(**nerf综述，值得看看**)机器人中的神经场：综述  专知  https://mp.weixin.qq.com/s/vU3rdDIfwulYBfUTOojz3w \
  Neural Fields in Robotics: A Survey \
  关键词—神经辐射场（Neural Radiance Field, NeRF）、神经场（Neural Fields）、符号距离场（Signed Distance Fields）、3D高斯分布（3D Gaussian Splatting）、占用网络（Occupancy Networks）、计算机视觉、新视角合成（Novel View Synthesis）、神经渲染（Neural Rendering）、体渲染（Volume Rendering）、姿态估计、机器人、操控、导航、自动驾驶
* 11.(**看不懂**)3个思想实验撕裂时空！实验证实：人类居住时空并非物理实体，而是近似  新智元  https://mp.weixin.qq.com/s/hiytAFR9AUHJzufKtW0qZA \
  The Thought Experiments That Fray the Fabric of Space-Time
* 12.阿里千问提出Self-Lengthen，大模型实现自迭代扩展输出长度 
 PaperWeekly  https://mp.weixin.qq.com/s/1m1hUkhs3altxjYP6IxUVw \
  Language Models Can Self-Lengthen to Generate Long Texts \
  https://github.com/QwenLM/Self-Lengthen
* 13.3B模型打通机器人任督二脉！冲咖啡叠衣服都能干，7种形态适配，OpenAI也投了  量子位  https://mp.weixin.qq.com/s/AHll8FDtbb8qZv-SlUOfGQ \
  技术报告：https://www.physicalintelligence.company/download/pi0.pdf \
  π0 3B
* 14.StaR ｜ 用少量推理数据让模型学会通用推理能力，显著提升模型复杂推理  NLP PaperWeekly  https://mp.weixin.qq.com/s/BmCIDx5wPQPqvz6DljZMlA \
  STaR: Bootstrapping Reasoning With Reasoning

# 11.3 Sun
* 15.人工智能综述：物理学与人工智能的跨界新范式（全文版本）  图灵人工智能  https://mp.weixin.qq.com/s/XmxI382KHearcuSZKJwdMw
* 16.RAG新突破：块状注意力机制实现超低延迟检索增强  机器之心  https://mp.weixin.qq.com/s/yv2iIpaJTi4g4nhZG1WLZw \
  Block-Attention for Efficient RAG \
  论文《Block-Attention for Efficient RAG》为检索增强 (RAG) 场景实现了一种块状注意力机制，Block-Attention，通过分块独立编码检索到的文档，使得模型无需重复编码计算已经在其他 query 中已经见过的文档，从而实现线上推理效率的有效提升
* 17.LLM 比之前预想的更像人类，竟也能「三省吾身」  机器之心  https://mp.weixin.qq.com/s/Ri-Wdl_Xk5OxWF5IIJmrxg \
  Looking Inward: Language Models Can Learn About Themselves by Introspection \
  结论：\
  1.LLM 可以获得无法从其训练数据中推断出的知识。\
  2.这种对关于自身的某些事实的「特权访问」与人类内省的某些方面有关联。
* 18.o1满血版泄露！奥数题图片推理手拿把掐，奥特曼上线剧透o2  量子位  https://mp.weixin.qq.com/s/hzXIVc0wYJV3mOaAE8H-tA
* 19.微软清华改进Transformer：用降噪耳机原理升级注意力，一作在线答疑  量子位  https://mp.weixin.qq.com/s/btZB8tDNkvZux-XNivMObQ \
  具体到在语言模型中，如果句子很长，只有少数token会真正影响当前token的含义。而注意力机制允许每两个词之间产生交互，其中就包含大量噪声了。团队提出的方法是在注意力层中增加一个Softmax，然后两个Softmax做减法。这一减，噪音信息就被大幅抵消，让注意力更集中在相关内容上。\
  DIFFERENTIAL TRANSFORMER
* 20.(**有趣，非常值得看看**)世界首个1000亿AI智能体文明诞生！北大校友打造真实版「西部世界」，技术细节全公开  新智元  https://mp.weixin.qq.com/s/cstOIX-K_H1FgR9M_Nzj0w \
  1000个智能体打造《我的世界》，北大校友35页技术报告揭秘  量子位  https://mp.weixin.qq.com/s/fad7tjxNlwfVn0HNCQTcGQ \
  Project Sid: Many-agent simulations toward Al civilization \
  https://github.com/altera-al/project-sid \
  PIANO（并行信息聚合神经协调），是一个能让AI智能体实现多方互动的架构，同时在多个输出流中保持连贯性 \
  这些智能体，构筑了一个「文明」。纳税、贸易、政府、国家、宗教....，一切人类世界所有的日常，AI智能体也有。

# 11.4 Mon
* 21.大型模型中的参数高效微调：方法论综述  专知  https://mp.weixin.qq.com/s/p8HmBwJGv2E908Fu5Oc1Iw
* 22.慢思考准确率反降30%！普林斯顿揭示思维链某些任务上失效的秘密  量子位  https://mp.weixin.qq.com/s/sFaPB8CxtYj0HIi_5bA74Q \
  研究实锤：别让大模型「想」太多，OpenAI o1准确率竟下降36.3% 
  夕小瑶科技说  https://mp.weixin.qq.com/s/UPO-QfBAMafrq_Ewxeye8Q \
  MIND YOUR STEP(BY STEP):CHAIN-OF-THOUGHTCAN REDUCE PERFORMANCEONTASKS WHERETHINKING MAKES HUMANS WORSE

# 11.5 Tue
* 23.Llama版o1来了，来自上海AI Lab，强化学习代码已开源，基于AlphaGo Zero范式  量子位  https://mp.weixin.qq.com/s/2vRvD4x6WWihmnbrWG33SQ \
  https://github.com/SimpleBerry/LLaMA-O1 \
  上海AI Lab LLaMA版o1 \
  上交大团队 O1-Journey 
* 24.(**非常值得看看**)LLM超越人类时该如何对齐？谷歌用新RLHF框架解决了这个问题 
 AINLPer  https://mp.weixin.qq.com/s/IGqWUiYzE4j8h6FTMAERww \
  随着 LLM 能力越来越强，它们将能解决越来越复杂和越来越多的难题，而这些难题所需的训练数据已经超出了人类的能力 \
  因此，我们就需要为 LLM 构建一种能使其实现自我提升的基本机制，让模型可以持续地自我生成和自我求解更困难的问题 \
  语言模型能否自我创建可学习的新任务，从而实现自我改进以更好地泛化用于人类偏好对齐？\
  为了提升语言模型的对齐能力，人们已经提出了许多偏好优化算法，但它们都默认使用固定的提示词训练分布。这种固定的训练范式**缺乏可扩展性**，并不可避免地导致泛化问题和效率问题。\
  基于这些考虑，谷歌 DeepMind 和芝加哥大学一个研究团队开发了一种可扩展的开放式 RLHF 框架 eva，即 Evolving Alignment via Asymmetric Self-Play，也就是「通过非对称自博弈实现的演进式对齐」。\
  Evolving Alignment via Asymmetric Self-Play

# 11.6 Wed
* 25.OpenAI重拾规则系统，用「AI版机器人定律」守护大模型安全 
 机器之心  https://mp.weixin.qq.com/s/amO3828miwEnr8aQBLpW9A \
  Rule Based Rewards for Language Model Safety \
  https://github.com/openai/safety-rbr-code-and-data
* 26.沿着人类思路探索：达摩院提出创意链，使大模型生成人类水平的科研Idea  PaperWeekly  https://mp.weixin.qq.com/s/7yV6PABIr6QAzlbUUY9dAA \
  Chain of Ideas: Revolutionizing Research in Novel Idea Development with LLM Agents
* 27.「黑神话」级3A大作AI实时游戏生成！港科大、中科大等祭出最强扩散Transformer，火爆国外  新智元  https://mp.weixin.qq.com/s/b1H_8JXBpsiXb-3BmrhrFA \
  GameGen-X: Open-world Video Game Generation
  https://gamegen-x.github.io/
* 28.不靠更复杂的策略，仅凭和大模型训练对齐，零样本零经验单LLM调用，成为网络任务智能体新SOTA  机器之心  https://mp.weixin.qq.com/s/UvNCUVBbH9TqfbEdoB7mTA \
  AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents

# 11.7 Thur
* 29.玩转「智能体魔方」！清华推出AgentSquare模块化搜索框架，开启AI智能体高速进化时代  新智元  https://mp.weixin.qq.com/s/NL7AnKb1WBM1_CywNM7uyQ \
  清华大学团队提出了AgentSquare模块化智能体设计框架，通过标准化的模块接口抽象，让AI智能体可以通过模块演化和重组高速进化，实现针对不同任务场景的自适应演进，赋能超越人类设计的智能体系统在多种评测数据集上广泛自我涌现 \
  AgentSquare: Automatic LLM Agent Search In Modular Design Space

# 11.8 Fri
* 30.(**非常值得看看，用视频生成模型做世界模型还缺什么？**)LeCun赞转！类Sora模型能否理解物理规律？字节豆包大模型团队系统性研究揭秘  机器之心  https://mp.weixin.qq.com/s/mwm6UgJByVOdnZHIgcMngA \
  Keras 之父 François Chollet 则认为，Sora 这样的视频生成模型确实嵌入了「物理模型」，但问题是：这个物理模型是否准确？它能否泛化到新的情况，即那些不仅仅是训练数据插值的情形？这些问题至关重要，决定了生成图像的应用范围 —— 是仅限于媒体生产，还是可以用作现实世界的可靠模拟。最后他指出，不能简单地通过拟合大量数据来期望得到一个能够泛化到现实世界所有可能情况的模型 \
  字节豆包大模型团队通过大规模实验发现 —— 即便依照 Scaling Law 扩大模型参数与训练数据量，模型依然无法抽象出一般物理规则，甚至连牛顿第一定律、抛物线运动都无法领会 \
  视频生成模型并没有在学习、理解物理规律 \
  How Far is Video Generation from World Model: A Physical Law Perspective \
  研究中也有一个好消息：如果训练视频中所有概念和物体都是模型已熟悉的，此时加大训练视频的复杂度，比如组合增加物体间的物理交互，通过加大训练数据，模型对物理规律的遵循将越来越好。这一结果可为视频生成模型继续提升表现提供启发
* 31.智能体首次达到Kaggle Grandmaster水平，华为用结构化推理补齐思维链短板  机器之心  https://mp.weixin.qq.com/s/w3nIhxeQMuTdqFDuqXsAYA \
  Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level

# 11.9 Sat
* 32.MetaGPT开源自动生成智能体工作流，4.55%成本超GPT-4o  机器之心  https://mp.weixin.qq.com/s/5YpPFYIpuCkSf0sJp0_RnQ \
  MetaGPT 开源了 AFLOW，它使用 MCTS 进行 Agentic Workflow 的自动搜索，可以完全自动地构建与优化 Agentic Workflow 问题，让我们不再需要手写代码、调试提示词 \
  AFlow: Automating Agentic Workflow Generation \
  https://github.com/geekan/MetaGPT/tree/main/examples/aflow

# 11.10 Sun
* 33.NeurIPS 2024 (Oral) | 如何量化与提升思维链的推理能力边界？  机器之心  https://mp.weixin.qq.com/s/BwuGacSHKY4RTdvYNMa66Q \
  Unlocking the Capabilities of Thought:A Reasoning Boundary Framework to Quantify andOptimize Chain-of-Thought \
  https://github.com/LightChen233/reasoning-boundary
* 34.高能干货分享，有关提示词工程的一切都在这份教程里  机器之心  https://mp.weixin.qq.com/s/RaIzHtRIShIcpXydRE6kQg \
  https://github.com/NirDiamant/Prompt_Engineering
* 35.谷歌苹果曝出LLM惊人内幕，自主识别错误却装糊涂！AI幻觉背后藏着更大秘密  新智元  https://mp.weixin.qq.com/s/u_h6qwmHdXI74_9feKNeEw \
  LLMS KNOW MORE THAN THEY SHOW: ON THE INTRINSIC REPRESENTATION OF LLM HALLUCINATIONS
* 36.(**值得看看**)空间智能版ImageNet来了！李飞飞吴佳俊团队出品  量子位  https://mp.weixin.qq.com/s/_yi69foQdQzhOazgUqiiZA \
  HourVideo:1-Hour Video-Language Understanding \
  团队提出了一个新的任务对应套件，包含总结、感知（回忆、跟踪）、视觉推理（空间、时间、预测、因果、反事实）和导航（房间到房间、对象检索）任务，共18个子任务

# 11.11 Mon
* 37.LoRA、完全微调到底有何不同？MIT 21页论文讲明白了  机器之心  https://mp.weixin.qq.com/s/Xxh-MrSfkRlRz7reE-fkzA \
  LORA VS FULL FINE-TUNING: AN ILLUSION OF EQUIVALENCE 
* 38.GitHub超火开发者路线图库有AI学习路线了！star数近30万  机器之心  https://mp.weixin.qq.com/s/P9hWHGsiWcfEfMq54E02xg \
  https://github.com/kamranahmedse/developer-roadmap \
  https://roadmap.sh
* 39.专家模型不要专家并行！微软开源MoE新路径  新智元  https://mp.weixin.qq.com/s/pt-AlH_z4e3PNiKC9Iyz7A \
  GRIN:GRadient-INformed MoE

# 11.12 Tue
* 40.最强开源CodeLLM模型深夜来袭！320亿参数，Qwen2.5-Coder新模型超越GPT-4o  量子位  https://mp.weixin.qq.com/s/K55WD7466VMTGiYJVsMZUA
* 41.开源版SearchGPT来了，两张3090就可复现，超越Perplexity付费版  量子位  https://mp.weixin.qq.com/s/xa0oS-LPnZAQqEyUozf_eg \
  港中文MMLab、上海AI Lab、腾讯团队简易实现了Vision Search Assistant，模型设计简单，只要两张RTX3090就可复现 \
  Vision Search Assistant: Empower Vision-Language Models as MultimodalSearch Engines \
  https://cnzzx.github.io/VSA/
* 42.(**值得了解**)o1不是唯一路径！MIT新研究：在测试时训练，模型推理能力最高升至5.8倍 
 量子位  https://mp.weixin.qq.com/s/sRv-BswlDOn-CFtR-rRqDQ \
 连OpenAI都推不动Scaling Law了？MIT把「测试时训练」系统研究了一遍，发现还有路  机器之心  https://mp.weixin.qq.com/s/tfrG21mfteVAkjqYx5mDsQ \
  Scaling Law还有救？MIT破解LLM进化新路！测试时训练让性能暴涨6倍，准确率媲美人类  新智元  https://mp.weixin.qq.com/s/dhAJVUpx0Il90w2y8cprTQ \
  开源：Test-Time Training 巨大提升抽象推理  CreateAMind  https://mp.weixin.qq.com/s/DfTiYmZpBUONL61WoPf-XA \
  MIT的新研究发现，在测试时对大模型进行训练，可以让推理水平大幅提升 \
  TTT: test-time training \
  https://ekinakyurek.github.io/papers/ttt.pdf \
  不同于传统的先训练后测试模式，测试时训练（Test-Time Training，TTT）在部署阶段面对新的测试样本时，不直接用训练好的模型去推理。在推理之前，测试样本自身携带的信息，会通过快速的训练过程被用于调整模型参数
* 43.Ilya认错，Scaling Law崩了？自曝SSI秘密技术路线取代OpenAI  新智元  https://mp.weixin.qq.com/s/EbO4lhGH1kwQstDSBoTHww \
  Ilya承认Scaling Law到头了，Meta和谷歌寻找替代o1的方法  夕小瑶科技说  https://mp.weixin.qq.com/s/i0dTLDrEEznh4gn8cSsgEw 
* 44.深度揭秘CoT！普林斯顿耶鲁发布最新报告：大模型既有记忆推理、也有概率推理  新智元  https://mp.weixin.qq.com/s/PtuCL1WBRvrFr7nkJaKgrQ \
  Deciphering the Factors Infuencing the Effcacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning

# 11.13 Wed
* 45.突破次元壁！新加坡国立发布GenXD：拿捏真实感3D、4D动态场景  新智元  https://mp.weixin.qq.com/s/30rBNR1Io9w6nVuOxR2uLw \
  GENXD:GENERATING ANY 3D AND 4D SCENES \
  https://gen-x-d.github.io/ \
  https://github.com/HeliosZhao/GenXD
* 46.Nature：AI也许可以拥有常识，但不是现在  新智元  https://mp.weixin.qq.com/s/S8YkHt6q4KCvSuNSfTdCUw 
* 47.UIUC | 提出“提取-精炼-检索-读取”框架：ERRR，提升RAG系统性能 
  AINLPer  https://mp.weixin.qq.com/s/yjwuJNtftiHYqtsT66mp8w \
  Query Optimization for Parametric Knowledge Refinement inRetrieval-Augmented Large Language Models

# 11.14 Thur
* 48.(**有趣，值得了解**)Token化一切，甚至网络！北大&谷歌&马普所提出TokenFormer，Transformer从来没有这么灵活过！  机器之心  https://mp.weixin.qq.com/s/1wOGd1h5y5DjpAt1-n5WDA \
  TokenFormer 不仅像原始 Transformer 一样 Token 化了 input data，并且 Token 化了网络参数，将 attention 机制拓展到 Token 和 parameters 的交互中，最大化了 Transformer 的灵活性，真正得到了一个 Fully attention-based 的网络结构。\
  这种方式打破了原有人们区别看待 data 和 model 的观念，即所有的计算都归纳为不同类型的 Token（e.g., data, param token）通过灵活的 attention 来交互。得益于这一灵活的性质，TokenFormer 允许 incremental scaling model size，基于训好的模型上增量的拓展新的更大的模型，大大节省了计算的开销 \
  TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters \
  https://github.com/Haiyang-W/TokenFormer
* 49.李飞飞吴佳俊团队新作：推出具身智能决策能力评价基准，o1-preview登顶 | NeurIPS  量子位  https://mp.weixin.qq.com/s/ixiGVdBZKm4-wnmH3KoHRg \
  大模型的具身智能决策能力，终于有系统的通用评估基准了。李飞飞吴佳俊团队新提出的评估框架，对具身智能决策的四项关键子能力来了个全面检查 \
  该框架名为Embodied Agent Interface（简称EAI），提供了连接不同模块和基准环境的标准接口 \
  为了更深入理解大模型的行为模式和优劣势分布，EAI提出了四个关键能力模块:目标解释、子目标分解、动作序列规划、转换建模 \
  https://embodied-agent-interface.github.io/

# 11.15 Fri
* 50.**视觉自回归模型综述**  专知  https://mp.weixin.qq.com/s/O9gFoCRdHa7YOPgjPwNCqQ \
  Autoregressive Models in Vision: A Survey \
  https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey
* 51.(**有趣**)AI在《我的世界》PK盖楼，新旧Claude差距过于明显，网友：审美也是智力的一种  量子位  https://mp.weixin.qq.com/s/_ZkhXxvEpe3ZOWZq4wC3cQ \
  视频教程：https://x.com/mckaywrigley/status/1849613686098506064 \
  开源代码：\
  https://github.com/kolbytn/mindcraft \
  https://github.com/mc-bench/orchestrator
* 52.Make U-Nets Great Again！北大&华为提出扩散架构U-DiT，六分之一算力即可超越DiT  机器之心  https://mp.weixin.qq.com/s/IhlH4CyvM6hFke-8flnscA \
  U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers

# 11.16 Sat
* 53.传说中Ilya Sutskever精选论文清单：AI领域40大论文完整版「破解」完成  机器之心  https://mp.weixin.qq.com/s/7Bj_K1Vjp2FtfklfJsAMbQ 
* 54.(**非常值得了解**)LeCun 的世界模型初步实现！基于预训练视觉特征，看一眼任务就能零样本规划  机器之心  https://mp.weixin.qq.com/s/2wYS7bC0RsKqvqvkp9SbPA \
  DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning \
  https://dino-wm.github.io/
* 55.首个自主机器学习AI工程师，刚问世就秒了OpenAI o1，Kaggle大师拿到饱 
 机器之心  https://mp.weixin.qq.com/s/h1firXC07eOvcTyuzh2Tqg \
 NEO 可以自动化整个机器学习的工作流程，为开发人员节省数千小时的繁重工作。它是一个多智能体（AI Agent）系统，可用并行的方式解决单一问题
* 56.Nature:「人类亲吻难题」彻底难倒LLM，所有大模型全部失败！LLM根本不会推理，只是工具  新智元  https://mp.weixin.qq.com/s/c4-zh0-YZOjcq2qVFAgxBA \
  Testing Al on language comprehension tasks revealsinsensitivity to underlying meaning
* 57.NeurIPS 2024 | 大模型知识表示的“知识回路”假说，深入理解大模型的知识机理  PaperWeekly  https://mp.weixin.qq.com/s/YuRuvCgm2WbJA7AUGqjr3g \
  Knowledge Circuits in Pretrained Transformers \
  知识图谱通过符号表示明确刻画实体和概念间的关系，而语言模型则依赖神经网络和注意力机制隐式关联知识元素。以多跳推理为例，知识图谱提供可解释、可控的显式路径，而语言模型在隐式参数空间中依赖注意力等权重找到答案，导致解释性和可靠性不足 \
  为深入理解大模型的知识机理，本文被提出大模型知识表示的“知识回路”（Knowledge Circuits）假说，认为大模型知识处理过程中的实体、概念和关系是通过参数激活逐步形成闭合回路，以助于发展更可靠、可控、安全的大模型知识学习架构和方法

# 11.17 Sun
* 58.Scaling Law或将终结？哈佛MIT预警：低精度量化已无路可走，重磅研究掀翻AI圈  新智元  https://mp.weixin.qq.com/s/js_L3X0B-CSRorVFWNxNjg \
  对于推理过程来说，训练数据越多，量化带来的性能损失越大 \
  Scaling Laws for Precision
* 59.(**LLM教程**)14天速成LLM高手！大佬开源学习笔记，GitHub狂揽700星  新智元  https://mp.weixin.qq.com/s/aDkH9E5b0yNd1J_Kthkh5Q \
  https://github.com/hesamsheikh/ml-retreat \
  学习路线中的主要知识点包括token嵌入、位置嵌入、自注意力、Transformer、对Q、K、V的直观理解、因果和多头注意力、温度、top-k、top-p、分类和指令微调、旋转位置编码（RoPE）、KV缓存、无限注意力（长上下文窗口）、专家混合（MoE）、分组查询注意力（grouped query attention）、llama-2架构及相关技术等
* 60.(**值得看看**)从未见过现实世界数据，MIT在虚拟环境中训练出机器狗，照样能跑酷  机器之心  https://mp.weixin.qq.com/s/0iFs8N9OkLdAArTL1-AS8Q \
  MIT 的这个团队希望用生成模型来作为机器人学习的新数据源，用工程手段来取代传统的数据收集，实现一条通过由生成模型加持的物理仿真来训练机器人视觉的技术路线 \
  Learning Visual Parkour from Generated Images \
  https://lucidsim.github.io/

# 11.18 Mon
* 61.Karpathy后悔了：2015年就看到了语言模型的潜力，却搞了多年强化学习  机器之心  https://mp.weixin.qq.com/s/fF5a-ydlTVPzfFlSePAczw \
  Karpathy 还提到：「Yann LeCun 当时就不太看好强化学习，他一遍又一遍地谈论『蛋糕』，而强化学习（RL）只是蛋糕顶部最后一颗樱桃，表征学习是蛋糕主体，监督学习是锦上添花。至少在今天看来，他在概念上是完全正确的（预训练 = 蛋糕主体，监督微调（SFT）= 糖衣，RLHF = 樱桃，即基本的 ChatGPT 训练 pipeline）。这很有趣，因为今天他仍然不太看好 LLM。」
* 62.NeurIPS 2024 | 自我纠错如何使OpenAI o1推理能力大大加强？北大、MIT团队给出理论解释  机器之心  https://mp.weixin.qq.com/s/W-YOehfVSRlYIBlg8itnBg \
  传统的大语言模型，因为在输出答案的时候是逐个Token输出，当输出长度较长时，中间某些Token出错是必然发生。但即使LLM后来知道前面输出的Token错了，它也得用更多错误来“圆谎”，因为没有机制让它去修正前面的错误 \
  Reflection 70B的关键技术也包括错误识别和错误纠正。他们用到了一种名为 Reflection-Tuning（反思微调） 的技术，使得模型能够在最终确定回复之前，先检测自身推理的错误并纠正 \
  A Theoretical Understanding of Self-Correction through In-context Alignment \
  https://github.com/yifeiwang77/Self-Correction
* 63.(**值得研究**)扩散模型版CS: GO！世界模型+强化学习：2小时训练登顶Atari 100K  新智元  https://mp.weixin.qq.com/s/_y25jjsP2jjJcN7N6MtiRw \
  DIAMOND是一种新型的强化学习智能体，在一个由扩散模型构建的虚拟世界中进行训练，能够以更高效率学习和掌握各种任务。在Atari 100k基准测试中，DIAMOND的平均得分超越了人类玩家，证明了其在模拟复杂环境中处理细节和进行决策的能力

# 11.19 Tue
* 64.超GPT-4o，1240亿参数！最强开源多模态模型 Pixtral Large！  AIGC开放社区  https://mp.weixin.qq.com/s/7RiAbsNh2ZH7vpYMQMVqMg 
* 65.(**值得看看**)北大等发布多模态版o1！首个慢思考VLM将开源，视觉推理超越闭源模型  量子位  https://mp.weixin.qq.com/s/GicLwdIgkFUpm7tCFkDbyg \
  LLaVA-o1: Let Vision Language Models Reason Step.
by-Step \
  https://github.com/PKU-YuanGroup/LLaVA-o1
* 66.LLM为何频频翻车算术题？最新研究追踪单个神经元，「大脑短路」才是根源  新智元  https://mp.weixin.qq.com/s/DUbb3X8t5u2_GYoecqUkCA \
  ARITHMETIC WITHOUT ALGORITHMS: LANGUAGEMODELS SOLVE MATH WITH A BAG OF HEURISTICS \
  在大模型回答正确的情况下，更多比例的正确神经元被激活了，而回答错误的案例中，应当被激活的神经元激活概率反而较小。这意味着大模型在特定算术题上失败的主要原因是对能得出正确答案的神经元缺少泛化能力，而不是算术神经元的数量不足。
* 67.Qwen2.5更新百万超长上下文，推理速度4.3倍加速，网友：RAG要过时了  量子位  https://mp.weixin.qq.com/s/wDqId7s4mEuG1ft-qy4qsQ
* 68.逼真到离谱！1000个人类「克隆」进西部世界，AI相似度85%细节太炸裂  新智元  https://mp.weixin.qq.com/s/FKLCKHhP7xgYArLISDdiaw \
  Generative Agent Simulations of 1,000 People

# 11.20 Wed

# 11.21 Thur
* 69.强化学习之父Richard Sutton：AGI研究的下一个范式  图灵人工智能  https://mp.weixin.qq.com/s/crIKr2m1tmfeiup9MzjVlg
* 70.推理性能直逼o1，DeepSeek再次出手，重点：即将开源  机器之心  https://mp.weixin.qq.com/s/xB6ji7-gbJrnFlzqhVdjOg 
* 71.神级项目训练GPT-2仅需5分钟，Andrej Karpathy都点赞  机器之心  https://mp.weixin.qq.com/s/rKGXjIu_k9N58x92RGaT0Q \
  llm.c 旨在大幅简化大模型的训练，ta 使用纯 C 语言 / CUDA，不需要 245MB 的 PyTorch 或 107MB 的 cPython。不过即使是这样的优化，复现 GPT-2 级别的模型也需要在 8 块 H100 上花费 45 分钟进行训练。\
  https://github.com/KellerJordan/modded-nanogpt/tree/master
* 72.诺奖得主哈萨比斯新作登Nature，AlphaQubit解码出更可靠量子计算机  机器之心  https://mp.weixin.qq.com/s/1iACBxiRz7SUSgMgtmjnXA 
* 73.宝可梦GO「偷家」李飞飞空间智能？全球最强3D地图诞生，150万亿参数解锁现实边界  新智元  https://mp.weixin.qq.com/s/l24hohc-gzsS_WQFOKNxow \
  Matching 2D lmages in 3D: Metrie Relative Pose from Metric Correspondences 
* 74.(**值得看看**)EMNLP 2024最佳论文：从反向传播矩阵来理解Transformer的运作机制 
 PaperWeekly  https://mp.weixin.qq.com/s/AaCaiT4MZK9lWZ0xXHTqhg \
 Backward Lens: Projecting Language Model Gradients into the Vocabulary Space 
* 75.(**值得看看**)AI版《黑客帝国》：无限生成逼真视频，3A画质，还能实时交互  量子位  https://mp.weixin.qq.com/s/9FzRjmeszpVRvdCagQB6oA \
  The Matrix: Infnite-Horizon World Generation with Real-Time Moving Control \
  https://thematrix1999.github.io/article/the_matrix.pdf \
  https://thematrix1999.github.io/

# 11.22 Fri
* 76.预测编码的脑启发计算智能  CreateAMind  https://mp.weixin.qq.com/s/6P3rn1zcDPFACywJ9InimQ \
 Brain-inspired computational intelligence via predictive coding \
 看不懂
* 77.上交大o1复现新突破：蒸馏超越原版，警示AI研发"捷径陷阱"  机器之心  https://mp.weixin.qq.com/s/bJc_hSrXsUgrzAfSxAoYoA \
  技术文档：https://github.com/GAIR-NLP/O1-Journey/blob/main/resource/report-part2.pdf \
  相关资源将近日公开：https://github.com/GAIR-NLP/O1-Journey
* 78.(**值得看看**)大模型不会推理，为什么也能有思路？有人把原理搞明白了  机器之心  https://mp.weixin.qq.com/s/2_ccqg23n05iGK3zUH5KMg \
  大语言模型的「推理」能力应该不是推理: \
  Language is primarily a tool for communication rather than thought \
  Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models \
  详细探讨了大语言模型（LLM）在执行推理任务时采用的泛化策略类型 \
  一个普遍的猜测是：大模型的推理难道是在从参数知识中检索答案？该研究给出了反对这一观点的证据。作者认为，是预训练中的**程序性知识**在推动大模型进行推理。\
  LLM进行推理时不是在死记硬背
* 79.如今的智能体，已经像人一样「浏览」视频了，国内就有  机器之心  https://mp.weixin.qq.com/s/Nbt4gpDPfE5tXs0CYvVj5g \
  https://github.com/om-ai-lab/OmAgent \
  OmAgent 是一个开源的智能体框架，支持简单快速地面向设备进行智能体系统的开发，为智能手机、智能可穿戴设备、智能摄像头乃至机器人等各类硬件设备赋能。OmAgent 为各种类型的设备创建了一个抽象概念，并大大简化了将这些设备与最先进的多模态基础模型和智能体算法相结合的过程，使每个人都能基于设备建立最有趣的 AI 应用
* 80.续命Scaling Law？世界模型GPT-4o让智能体超级规划，OSU华人一作  新智元  https://mp.weixin.qq.com/s/S-Y3SYZ9nIoj_RTjb2tMMQ \
  Is Your LLM Secretly a World Model ofthe Internet? MODEL-BASED PLANNING FOR WEB AGENTS
  WebDreamer的核心是「做梦」的概念：在承诺采取任何行动之前，智能体使用LLM去想象预测每个可能步骤的结果，并以自然语言描述状态将如何变化 \
  用GPT-4o作为网络交互的世界模型
  
# 11.23 Sat
* 81.陶哲轩宣布“等式理论计划”成功，人类AI协作，57天完成2200万+数学关系证明  量子位  https://mp.weixin.qq.com/s/UAaW6YYANyHIYQd18DsJhA
* 82.Claude自动玩崩铁清日常，NUS新论文完整测评AI电脑操控：GUI智能体的黎明  量子位  https://mp.weixin.qq.com/s/XCJZ7uZouSkA_ljLYfDcdA \
  The Dawn of GUI Agent: A PreliminaryCase Study with Claude 3.5 Computer Use
* 83.GWT意识模型的工程实现，从访问意识到灵活行为  CreateAMind  https://mp.weixin.qq.com/s/vm5-9_CAiqBRwE8lcdlp8g \
  Flexible serial processing in networks of spiking neurons
* 84.这才是真・开源模型！公开「后训练」一切，性能超越Llama 3.1 Instruct  机器之心  https://mp.weixin.qq.com/s/sTtBkVkqy0CQtpzcR6SN-A \
  Tülu 3 来自艾伦人工智能研究所（Ai2），目前包含 8B 和 70B 两个版本（未来还会有 405B 版本），并且其性能超过了 Llama 3.1 Instruct 的相应版本 \
  T'Lu 3: Pushing Frontiers in Open Language Model Post-Training \
  技术报告：https://allenai.org/papers/tulu-3-report.pdf \
  数据集：https://huggingface.co/collections/allenai/tulu-3-datasets-673b8df14442393f7213f372 \
  GitHub：https://github.com/allenai/open-instruct \
  模型：https://huggingface.co/allenai
* 85.阿里国际版o1来了，Marco-o1：聚焦开放式问题推理  机器之心  https://mp.weixin.qq.com/s/k1gwBWNYIn_tfviWxbj8fw \
  Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions \
  https://github.com/AIDC-AI/Marco-o1
* 86.英伟达开源福利：视频生成、机器人都能用的SOTA tokenizer  机器之心  https://mp.weixin.qq.com/s/eqaxr6_2j_1J1h3CQfjBZg \
  https://research.nvidia.com/labs/dir/cosmos-tokenizer/ \
  https://huggingface.co/collections/nvidia/cosmos-tokenizer-672b93023add81b66a8ff8e6
* 87.NeurIPS 2024 Oral | 还原所见！揭秘从脑信号重建高保真流畅视频  机器之心  https://mp.weixin.qq.com/s/Hpz98TdMFNBPcCjMDCkF1A \
  NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction \
  https://github.com/gongzix/NeuroClips
* 89.视觉语言模型能否实现鲁棒的数学推理？UC伯克利发布测评基准DynaMath 
 PaperWeekly  https://mp.weixin.qq.com/s/yxDFuMg6OZHGD7IdSJCxbA \
  DYNAMATH: A DYNAMIC VISUAL BENCHMARK FOR EVALUATING MATHEMATICAL REASONING ROBUSTNESS OF VISION LANGUAGE MODELS \
  https://dynamath.github.io
* 90.大模型在连续学习中真的遗忘了吗？重新审视基于预训练语言模型的增量学习
  PaperWeekly  https://mp.weixin.qq.com/s/G5oD4UM21PFMNlGz2iwUVw \
  Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models 

# 11.24 Sun
* 91.RTX 4090可跑、完全开源，最快视频生成模型问世，实测一言难尽  机器之心  https://mp.weixin.qq.com/s/zoU3O8cqS4x7VZc-CKeehA \
  https://github.com/Lightricks/LTX-Video
* 92.(**小模型综述**)研究大模型门槛太高？不妨看看小模型SLM，知识点都在这  机器之心  https://mp.weixin.qq.com/s/sg7HveGDjMEj-ZcHS3YizA \
  A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness \
  https://github.com/FairyFali/SLMs-Survey
* 93.(**有趣，值得看看**)智能体零样本解决未见过人类设计环境！全靠这个开放式物理RL环境空间  机器之心  https://mp.weixin.qq.com/s/ZWlOFO535hSggCvp6Wu8jQ \
  Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks \
  https://kinetix-env.github.io/ \
  Kinetix 涵盖的范围足够广，可以表征机器人任务（如抓取和移动）、经典的 RL 环境（如 Cartpole、Acrobot 和 Lunar）、电子游戏（Pinball）和其他很多任务 \
  为了后端运行 Kinetix，研究者开发了一种硬件加速物理引擎 Jax2D，它能够高效地模拟训练智能体所需的数十亿次环境交互。他们表示，通过从可表征的 2D 物理问题空间中随机采样 Kinetix 环境，可以几乎无限地生成有意义的多样化训练任务
* 94.扩散模型=进化算法！生物学大佬用数学揭示本质  新智元  https://mp.weixin.qq.com/s/XQSmeL9bbvRWpTMuaeRF4A \
  DIFFUSION MODELS ARE EVOLUTIONARY ALGORITHMS \
  https://gonzoml.substack.com/p/diffusion-models-are-evolutionary
* 95.(**可以看看**)EMNLP 2024最佳论文！通过耦合理解与生成，实现用户互动反馈的持续学习 
 PaperWeekly  https://mp.weixin.qq.com/s/GDwxsqmDZJhZ_VuKVM8q-w \
 CoGen: Learning from Feedback with Coupled Comprehension and Generation

# 11.25 Mon
* 96.自我反思助力VLM推理！南大清华提出VLM自训练框架，支持Inference Scaling  PaperWeekly  https://mp.weixin.qq.com/s/SIkxnR_isceGHoraleyB2Q \
  Vision-Language Models Can Self-Improve Reasoning via Reflection
* 97.OpenAI怒斥Scaling撞墙论！o1已产生推理直觉潜力巨大  新智元  https://mp.weixin.qq.com/s/zy0l-dFXsXOLV5s_XNE6yw 
* 98.世界模型挑战赛，单项奖金10000美元！英伟达全新分词器助力下一帧预测 
 新智元  https://mp.weixin.qq.com/s/zVekQiEuPJzcLl7HXisjfg \
 人形机器人公司1X公布了世界模型挑战赛的二阶段：Sampling。一同登场的还有合作伙伴英伟达新发布的Cosmos视频分词器，超高质量和压缩率助力构建虚拟世界
* 99.
* 100.

# 11.26 Tue
# 11.27 Wed
# 11.28 Thur
# 11.29 Fri
# 11.30 Sat
