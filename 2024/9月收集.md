# 9.1 Sun
* 1.(**有趣，值得玩玩**)用Mac训练个机器人叠衣服，HuggingFace开源全套教程，开源AI机器人革命要来了？  机器学习研究组订阅  https://mp.weixin.qq.com/s/zDRlYf6MPjr7Jd8_vKaPrA \
  教程链接：https://github.com/huggingface/lerobot/blob/main/examples/7_get_started_with_real_robot.md

# 9.2 Mon
* 2.(**LTM值得观望**)RAG不存在了？世界首个1亿token神级上下文模型诞生，前OpenAI大佬加盟获4.65亿融资！  机器学习研究组订阅  https://mp.weixin.qq.com/s/7QAR-rFaMT8-TKrHMse-Cw \
  Magic开发了一个专门针对代码的语言模型——LTM-2-mini
* 3.用最直观的动画，讲解LLM如何存储事实，3Blue1Brown的这个视频又火了  机器之心  https://mp.weixin.qq.com/s/PSMfQLBBQZyG2GwgzatqvA \
  视频地址：https://www.youtube.com/watch?v=9-Jl0dxWQs8 \
  来自3Blue1Brown 的《深度学习》课程
* 4.Claude认出自画像，惊现自我意识！工程师多轮测试，实锤AI已过图灵测试？  新智元  https://mp.weixin.qq.com/s/0yIiWZK-ZiaOYYDSDRC5Tg 
* 5.【伯克利博士论文】《通向开放世界机器人的基础模型路径》，237页pdf  专知  https://mp.weixin.qq.com/s/O_YH3mPCbkBsSg-nwUpR4w 
* 6.实现机器人领域的ChatGPT时刻，需要大模型+强化学习丨明星教授Sergey特邀报告  智源社区  https://mp.weixin.qq.com/s/hTExLgMh_W1KneAs0uIZAw 
* 7.(**AI意识，值得看看**)Anil Seth：机器的思考是否触及灵魂？  集智俱乐部  https://mp.weixin.qq.com/s/FYx3wnJSRhGjwGIz70OJbw \
  Conscious artificial intelligence and biological naturalism

# 9.3 Tue
* 8.专用于理解游戏场景的开源大模型-VideoGameBunny  AIGC开放社区  https://mp.weixin.qq.com/s/gS_8LZShO2eaSbJdvgV0HA \
  https://huggingface.co/VideoGameBunny/VideoGameBunny-V1/tree/main
* 9.(**值得看看**)《视觉Transformers自监督学习机制综述》  专知  https://mp.weixin.qq.com/s/7zQpX8gwe8Y4xQYrOfJ-KQ \
  A Survey of the Self Supervised LearningMechanisms for Vision Transformers
* 10.(**值得看看**)李飞飞团队提出ReKep，让机器人具备空间智能，还能整合GPT-4o  机器之心  https://mp.weixin.qq.com/s/AdyOPA6RhFIu5sjra5cW2Q \
  ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation \
  https://rekep-robot.github.io \
  https://github.com/huangwl18/ReKep
* 11.34页，超200篇文献，浙江大学最新综述，揭秘大语言模型中知识的利用机制  夕小瑶科技说  https://mp.weixin.qq.com/s/KoUoPN6Kw_GBek8bwy7OkQ \
  Knowledge Mechanisms in Large Language Models: A Survey and Perspective
* 12.上交大推出“可进化游戏引擎”！大模型加持代码自动成长，虚拟世界演化无需预设  量子位  https://mp.weixin.qq.com/s/fJeDHi65kFSzywyEjsHz0w \
  大模型与传统游戏引擎结合，能够被被特定的条件触发，自动地成长出新的代码。 \
  EVOLVING VIRTUAL WORLD WITH DELTA-ENGINE

# 9.4 Wed
* 13.将MoE塞到LoRA：一篇文章的诞生  PaperWeekly  https://mp.weixin.qq.com/s/CF4Qkc-9M87U9F6dp4W2LA \
  MoSLoRA \
  Mixture-of-Subspaces in Low-Rank Adaptation \
  https://github.com/wutaiqiang/MoSLoRA
* 14.1000个智能体创建首个「AI文明」！北大校友放弃MIT教职打造「西部世界」  新智元  https://mp.weixin.qq.com/s/S4r-eNa3YWUeSfev7GwR8w \
  全球首个「智能体文明」诞生！一千个智能体在「我的世界」自由发展  机器之心  https://mp.weixin.qq.com/s/-sdrR0n2ldSO6us_g5HTTA 

# 9.5 Thur
* 15.3天把Llama训成Mamba，性能不降，推理更快！  新智元  https://mp.weixin.qq.com/s/P_081wed8rp61_eSk_1AxQ \
  The Mamba in the Llama:Distilling and Accelerating Hybrid Models \
  https://github.com/jxiw/MambaInLlama
* 16.第一个100%开源的MoE大模型，7B的参数，1B的推理成本  机器之心  https://mp.weixin.qq.com/s/FvsYm5HxH4f9Km4Aqrso_Q \
  OLMoE: Open Mixture-of-Experts Language Models
* 17.小模型杀疯了！仅4B参数性能超GPT-3.5！无限长文本性能超Kimi  PaperWeekly  https://mp.weixin.qq.com/s/qJM9OTDHS3pJB9ozFuRP1g \
  MiniCPM 3.0  以 4B 参数，带来超越 GPT-3.5 的性能  量化后仅 2GB 内存

# 9.6 Fri
* 18.当心AI给你“洗脑”！MIT最新研究：大模型成功给人类植入错误记忆，马库斯：太可怕  量子位  https://mp.weixin.qq.com/s/vC-yLJVrQhDSuP5YTEwc6w

# 9.7 Sat
* 19.GPT-4o不会数r，被外国小哥原地逼疯！ 谷歌论文揭秘Transformer「数不到n」  新智元  https://mp.weixin.qq.com/s/eaWghjrj8f5DkL3dQr8nhw \
  谷歌最近的论文也揭示了本质原因：LLM没有足够空间，来存储计数向量。 ??? 没搞明白 \
  When Can Transformers Count to n?

# 9.8 Sun
* 20.大模型边推理边纠错，有可能做到吗？这是ICML爆火的演讲  机器之心  https://mp.weixin.qq.com/s/NOVFYmXiHUJ7x1SU7yH0CA \
  来自 Meta FAIR、CMU 和 MBZUAI 的叶添、徐子诚、李远志、朱泽园团队在最新的 arXiv 论文《语言模型物理学 Part 2.2：如何从错误中学习》中，通过可控实验，探索了让模型「边推理边纠错」的可能性。

# 9.9 Mon
* 21.视觉模型底座超越OpenAI，格灵深瞳开启多模态落地的Scaling Law  量子位  https://mp.weixin.qq.com/s/K2GbXh6GlV06-6Q4DYbPFg \
  Unicom v2

# 9.10 Tue
* 22.300篇文献！大模型走向物理世界：TeleAI发布大模型驱动的具身智能综述   PaperWeekly  https://mp.weixin.qq.com/s/GlchtrUBWjN788-PfOz1Eg \
  "大模型驱动的具身智能:发展与挑战"
* 23.(**可以看看**)如何统一视觉理解与生成？清华MIT等《VILA-U：一个融合视觉理解与生成的统一基础模型》  专知  https://mp.weixin.qq.com/s/SPbAg1uXWaJZ3qAtRR0sRg \
  VILA-U: a Unifed Foundation Model IntegratingVisual Understanding and Generation
* 24.(**值得看看**)清华、北大等发布Self-Play强化学习最新综述  机器之心  https://mp.weixin.qq.com/s/oMY0O0OIVYJc04zkoMzgcQ \
  A Survey on Self-play Methods in Reinforcement Learning \
  自博弈（self-play）通过与自身副本或过去版本进行交互，从而实现更加稳定的策略学习过程。自博弈在围棋、国际象棋、扑克以及游戏等领域都取得了一系列的成功应用。在这些场景中，通过自博弈训练得到了超越人类专家的策略。尽管自博弈应用广泛，但它也伴随着一些局限性，例如可能收敛到次优策略以及显著的计算资源需求等。
* 25.LIama 3+Mamba强强联手！蒸馏到线性RNN，推理速度提升1.6倍  量子位  https://mp.weixin.qq.com/s/2oyeCdlqKaeQATje--U8qg \
  The Mamba in the Llama: Distilling and Accelerating Hybrid Models

# 9.11 Wed
* 26.(**值得看看**)【ECCV2024】开放世界动态提示与持续视觉表征学习  专知  https://mp.weixin.qq.com/s/yUiU0CB1xdHWVADLfCRsug \
  Open-World Dynamic Prompt and Continual Visual Representation Learning \
  开放世界本质上是动态的，其特征是不断演变的概念和分布。在这种动态开放世界环境中，持续学习 (CL) 面临着如何有效泛化到未见测试类别的重大挑战。
* 27.召唤100多位学者打分，斯坦福新研究：「AI科学家」创新确实强  机器之心  https://mp.weixin.qq.com/s/XHrQ2espDn9SdB9LlcOBvw \
  Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers 
* 28.(**LLM对齐综述，值得了解**)LLM对齐综述｜迈向可扩展的大模型自动对齐，中科院软件所&阿里千问发布   夕小瑶科技说  https://mp.weixin.qq.com/s/TkljI1or0q0Rx389AviUEw \
  Towards Scalable Automated Alignment of LLMs: A Survey \
  https://github.com/cascip/awesome-auto-alignment \
  本文主要探讨了以下4种代表性的自动对齐信号构建机制：归纳偏置、行为模仿、模型反馈、环境反馈

# 9.12 Thur
* 29.(**值得看看**)大模型新课来了！宾夕法尼亚大学最新《大模型》课程  专知  https://mp.weixin.qq.com/s/De0EcdJFHBlTCYb0mfQhDg \
  https://llm-class.github.io/ \
  涉及主题: LLMs 的发展历史 \
           Transformer 架构 \
           模型训练技术提示工程（Prompt Engineering） \
           伦理考量和安全措施高级集成技术（例如，RAG、智能体、神经符号学习）
* 30.​Mistral首个多模态模型Pixtral 12B来了！还是直接放出24GB磁力链接  机器之心  https://mp.weixin.qq.com/s/c_fmpkuimmEtY6pD9f_bZQ \
  magnet:?xt=urn:btih:7278e625de2b1da598b23954c13933047126238a&dn=pixtral-12b-240910 \
  https://huggingface.co/mistral-community/pixtral-12b-240910
* 31.(**值得看看**)北大提出首个通用指令导航大模型系统 | CoRL 24  量子位  https://mp.weixin.qq.com/s/T145ZQlDBTWyX621mNVYGQ \
  InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment
* 32.检索总结能力超博士后，首个大模型科研智能体PaperQA2开源了  机器之心  https://mp.weixin.qq.com/s/gzGQ53ifHOn9uUPb212QMg \
  LANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SOEENIEC KNOWLEDGE
* 33.“闭门造车”之多模态思路浅谈：位置编码  PaperWeekly  https://mp.weixin.qq.com/s/wi_PCPmT3t5LWTJcPedNEQ
* 34.ACL 2024杰出论文：GPT-4V暴露致命缺陷？JHU等发布首个多模态ToM测试集 
  PaperWeekly  https://mp.weixin.qq.com/s/FJatGkN4kgifl4rSHS8U-A \
  心智能力（Theory of Mind，ToM） \
  MMToM-QA: Multimodal Theory of Mind Question Answering

# 9.13 Fri
* 35.刚刚，OpenAI震撼发布o1大模型！强化学习突破LLM推理极限  机器之心  https://mp.weixin.qq.com/s/sGcx90Q_uI8se-DKosj9dw \
  在技术博客《Learning to Reason with LLMs》中，OpenAI 对 o1 系列语言模型做了详细的技术介绍。\
  OpenAI o1 是经过强化学习训练来执行复杂推理任务的新型语言模型。特点就是，o1 在回答之前会思考 —— 它可以在响应用户之前产生一个很长的内部思维链。
* 36.面向软件工程的AI智能体最新进展，复旦、南洋理工、UIUC联合发布全面综述 
 机器之心  https://mp.weixin.qq.com/s/FxKYq6KO7rLZeT3yNBIsdA \
  Large Language Model-Based Agents forSoftware Engineering: A Survey
* 37.OpenAI o1模型问世，五级AGI再突破！推理极限超博士，清北复旦华人立功  新智元  https://mp.weixin.qq.com/s/xbSEvWCkanMWTGeIv5m--w \
  Introducing OpenAlo1-preview

# 9.14 Sat
* 38.OpenAI o1惊现自我意识？陶哲轩实测大受震撼，门萨智商100夺模型榜首  新智元  https://mp.weixin.qq.com/s/ZODF593CcNmb0_4092nOIw 
* 39.李飞飞携24人最强天团打造「大世界模型」！Hinton站台力挺，获2.3亿融资 
 新智元  https://mp.weixin.qq.com/s/BmJ8t8tYYKzV_5RHEQRo8A \
  World Labs的诞生就是为了构建「大世界模型」（LWM），感知、生成3D世界，并与之进行交互。

# 9.15 Sun
* 40.(**o1采用的技术非常值得深入研究**)北大对齐团队独家解读：OpenAI o1开启「后训练」时代强化学习新范式  机器之心  https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ \
  OpenAI o1 运用的技术关键还是在于强化学习的搜索与学习机制，基于 LLM 已有的推理能力，迭代式的 Bootstrap 模型产生合理推理过程（Rationales) 的能力，并将 Rationales 融入到训练过程内，让模型学会进行推理，而后再运用足够强大的计算量实现 Post-Training 阶段的 Scaling。类似于 STaR 的扩展版本。\
  技术要点有三：\
  1.后训练扩展律 Post-Training Scaling Laws 已经出现，并且 Post-Training Scaling Laws 为上述技术路径的成功提供了有力支持。\
  2.模型学习的是产生合理推理的过程，MCTS 在其中的作用是诱导合理推理过程的产生或构建相应的偏序对形成细粒度奖励信号，而非直接搜索过程和最终答案。\
  3.模型的 BootStrap 有助于构建新的高质量数据，并且新的 Rationales 数据促进了模型进一步提升能力。

# 9.16 Mon
* 41.(**有趣，值得了解**)1000个Agent圈地模拟人类社会，北大校友创业AI版「我的世界」  量子位  https://mp.weixin.qq.com/s/32dj-xI0S-EG8M4Fes1JEQ \
  Sid项目
* 42.比LoRA更高效！上交大&哈佛推出新微调框架，瞄准特定任务方向  量子位  https://mp.weixin.qq.com/s/4S8ORbMpwcB_iXDbxgrHEw \
  UNLEASHING THE POWER OF TASK-SPECIFIC DIRECTIONS IN PARAMETER EFFICIENT FINE-TUNING \
  LoRA-Dash
* 43.o1基石论文火爆传阅，Ilya仍是关键先生！核心项目清北校友闪光  量子位  https://mp.weixin.qq.com/s/woElE_YfQni7bwe4UCCK4g \
  Let's Verify Step by Step \
  o1则代表了“从记忆答案到记忆推理的范式转变”。

# 9.17 Tue
* 44.(**值得看看**)o1方法性能无上限！姚班马腾宇等数学证明：推理token够多，就能解决任意问题  量子位  https://mp.weixin.qq.com/s/4sK6NzCiYEjIamC7CHN3GA \
  Chain of Thought Empowers Transformers to Solve InherentlySerial Problems \
  只要思维链足够长，Transformer就可以解决任何问题 \
  CoT让Transformer运行更高效
* 45.(**值得看看**)o1突发内幕曝光？谷歌8月论文已揭示原理，大模型光有软件不存在护城河  量子位  https://mp.weixin.qq.com/s/lNjkRtH08Q93yo5DAHXwAg \
  Scaling LLM Test-Time Compute Optimally canbe More Effective than Scaling Model Parameters \
  增加测试时（test-time）计算比扩展模型参数更有效，这几乎就是o1的原理啊。
* 46.昂贵LLM的救星？Nature新研究提出新型忆阻器，比Haswell CPU高效460倍  机器之心  https://mp.weixin.qq.com/s/ybvuY4Bvc7ZiZkXxLqXMwA \
  Linear symmetric self-selecting 14-bit kinetic molecular memristors
* 47.COLM 24 | 从正确中学习？大模型的自我纠正新视角  机器之心  https://mp.weixin.qq.com/s/F8KpJuiDE9DfSVb1ciLUSQ \
  Learning From Correctness Without Prompting MakesLLM Effcient Reasoner \
  LeCo 从正确而不是错误中进行学习
* 48.OpenAI o1式思维链，开源模型也可以有，成功案例来了  机器之心  https://mp.weixin.qq.com/s/W28qb8ZaJkcyDP69eGw8MA \
  Llamaberry 能教会 AI 透彻地思考，就像是一位人类专家攻克难题时那样。具体来说，Llamaberry 是一个多轮思维链推理系统的实现，其基于运行在 Groq 上的 Llama 3.1 70B 模型。\
  https://huggingface.co/spaces/martinbowling/Llamaberry \
  另一个号称实现了类 o1 推理链的项目：g1 \
  https://github.com/bklieger-groq/g1
* 49.(**非常重要，复现o1必看**)OpenAI o1要跟，怎么跟？这个GitHub项目把解读、博客、相关论文一网打尽  机器之心  https://mp.weixin.qq.com/s/sPYeM5LbfAwyHUxbQ78Vsg \
  https://github.com/hijkzzz/Awesome-LLM-Strawberry 汇总了最近的高质量技术解读博客以及「可能」与 o1 技术路线相关的论文 

# 9.18 Wed
* 50.马斯克的脑机接口能让盲人看见世界？专家：为时尚早  机器之心  https://mp.weixin.qq.com/s/paagKAsm1Z3pVMA6fDmhlQ \
  刺激单个神经元并不会产生清晰的视觉感知，因为神经元表示的是一个感受野，而不是单个像素点
* 51.OpenAI重金押注，机器人NEO世界模型登场！机器人迎来ChatGPT时刻？  新智元  https://mp.weixin.qq.com/s/cOCm8yTqiKP_4iXv64YBhQ 

# 9.19 Thur
* 52.Qwen2.5登上全球开源王座！72B模型击败LIama3 405B，轻松胜过GPT-4o-mini  量子位  https://mp.weixin.qq.com/s/NRS5qAEKGHmtWpkeuYEYrg 
* 53.新SOTA来了：国产9B模型多项得分超4o-mini，中国出海电商已经用上了  
量子位  https://mp.weixin.qq.com/s/VLvF4J-8CsBN6Wam5UlVXQ \
  阿里国际AI团队开源多模态大模型Ovis1.6，在多模态权威综合评测基准OpenCompass上，Ovis1.6-Gemma2-9B版本综合得分超越Qwen2VL-7B、InternVL2-26B和MiniCPM-V-2.6等主流开源模型，在300亿以下参数开源模型中位居第一。
* 54.奥特曼：o1仅仅是“推理模型的GPT-2”；黄仁勋：我给你加速50倍  量子位  https://mp.weixin.qq.com/s/Beo79adl5WEqOLfCUJTtNQ \
  英伟达最新Blackwell架构GPU将推理性能提高了50倍，意味着能把o1模型的响应时间从几分钟缩短到几秒。
* 55.开源版《Her》来了，技术报告已公开！大神Karpathy：它很有个性  量子位  https://mp.weixin.qq.com/s/A4W7dA0vVq75xu3RSMJ9rQ \
  Moshi 端到端实时音频模型 \
  Kyutai将Moshi的代码、技术报告来了个大公开 \
  技术报告：https://kyutai.org/Moshi.pdf \
  GitHub官方仓库：https://github.com/kyutai-labs/moshi \
  HuggingFace模型库：https://huggingface.co/collections/kyutai/moshi-v01-release-66eaeaf3302bef6bd9ad7acd
* 56.248篇文献！复旦、南洋理工联合发布综述：面向软件工程的AI智能体最新进展   PaperWeekly  https://mp.weixin.qq.com/s/GzO2FspaN0IxHOvsZyfBOw \
  Large Language Model-Based Agents for Software Engineering: A Survey \
  https://github.com/FudanSELab/Agent4SE-Paper-List

# 9.20 Fri
* 57.o1带火的CoT到底行不行？新论文引发了论战  机器之心  https://mp.weixin.qq.com/s/_v15-UYpv300XIlhQ-54Vg \
  To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning \
  CoT 能极大助益 LLM 解决涉及数学和符号推理的任务，至于其它任务，CoT 的效果并不显著甚至可能有损模型性能。\
  这样的结果忽然让 CoT 的处境变得有点尴尬：在 CoT 有用的问题上，我们能使用外部工具做得更好；在另一些问题上，CoT 的能力又有限。\
  因此，该团队认为：「第一，很多广泛使用 CoT 解决的问题其实根本没必要使用 CoT：现在已有更高效方法，能以远远更低的推理成本取得相近的性能。第二，基于提示词的 CoT 不够用了，我们看到人们迫切地需要更复杂精妙的方法，比如基于搜索、交互式智能体或针对 CoT 进行过更好微调的模型的方法。」
* 58.从架构、工艺到能效表现，全面了解LLM硬件加速，这篇综述就够了  机器之心  https://mp.weixin.qq.com/s/9oF98UVCXc92S06sKlj6NA \
  Hardware Acceleration of LLMs: A comprehensive survey and comparison
* 59.让OpenAI o1逆天的慢思考，360两月前就做出来了？周鸿祎CoE媲美CoT，应用太前瞻  新智元  https://mp.weixin.qq.com/s/obk5Af8wzZe3cs8mZQWG3Q 
* 60.(**可以看看**)o1核心作者MIT演讲：激励AI自我学习，比试图教会AI每一项任务更重要  量子位  https://mp.weixin.qq.com/s/oE5m4vCrvbpW_51MRcy7XA \
  OpenAI研究科学家、o1核心贡献者Hyung Won Chung，刚刚就此分享了他在MIT的一次演讲：“Don’t teach. Incentivize（不要教，要激励）” \
  激励AI自我学习比试图教会AI每一项具体任务更重要 \
  通往AGI唯一可行的方法是激励模型，使通用技能出现
* 61.Transformer推理天花板被谷歌打破？DeepMind首席科学家亮出84页PPT，却遭LeCun反对  新智元  https://mp.weixin.qq.com/s/_z3ITDGRWXjbh8aVUBdUsg \
  CoT能让模型推理能力无上限？田渊栋、LeCun下场反对：两层MLP还能模拟全世界呢  机器之心  https://mp.weixin.qq.com/s/wi1jvQg47O078Xk83UpYmA \
  斯坦福等机构学者发文证实：LLM在情感方面表现出的认知和推理比人类还像人类，背后最大贡献者竟然就是CoT \
  Human-like Affective Cognition in Foundation Models \
  Chain of Thought Empowers Transformers to Solve Inherently Serial Problems 谷歌团队已经用数学方法证明，Transformer可以解决任何问题，只要允许它们根据需要生成任意数量的中间推理token \
  CoT为更强大的LLM推理提供了新的思路，CoT或将成为未来LLM发展的重要方向，而且很可能闪烁着AGI的火花 \
  还有网友指出，虽然论文中通过「模拟门电路运算」等实验从理论上进行了证明，但这样的模拟方式可能不能完全反映出大模型在真实环境中的行为。\
  「要达到人类级别的智能，暴力解法可能需要为每个问题生成上亿种解决方案。这就是为什么单靠扩展计算能力行不通。人类解决问题时不会考虑成千上万种可能性，而是凭直觉和推理迅速缩小到几个可行的选项。如果我们想实现 AGI，AI 系统也需要模仿这种高效的方式。」 \
  本质上就是对推理步骤进行暴力搜索，直到找到正确的步骤，与人脑的高效搜索还有差距

# 9.21 Sat
* 62.(**值得看看**)强化学习让大模型自动纠错，数学、编程性能暴涨，DeepMind新作  机器之心  https://mp.weixin.qq.com/s/CqxEoL50_FQTGtLYgh6omw \
  Training Language Models to Self-Correct via Reinforcement Learning \
  SCoRe，Self-Correction via Reinforcement Learning

# 9.22 Sun
* 63.(**值得看看**)类人神经网络再进一步！DeepMind最新50页论文提出AligNet框架：用层次化视觉概念「对齐」人类  新智元  https://mp.weixin.qq.com/s/ws-RwRNUvQNvc_5L0Rbu_w \
  提出了一个可能导致AI模型与人类表现存在差异的原因：人类概念知识是从精细到粗尺度进行分层组织的，而深度学习模型表征无法捕捉到人类感知的多层次概念结构 \
  Aligning Machine and Human VisuaRepresentations across Abstraction Levels
* 64.GPT-4o能玩《黑神话》！精英怪胜率超人类，无强化学习纯大模型方案  量子位  https://mp.weixin.qq.com/s/veHSbBxPIqRexG0OWtg4pw \
  Can VLMs Play Action Role-Playing Games? TakeBlack MMyth Wukong as a Study Case \
  https://varp-agent.github.io/
* 65.AI读脑成真，延迟仅0.25秒！Meta里程碑新研究：MEG实时解码大脑图像，LeCun转赞  脑机接口社区  https://mp.weixin.qq.com/s/3TLgOEWm-BHL4sRoVYzdTA \
  BRAIN DECODING: TOWARD REAL-TIMERECONSTRUCTION OF VISUAL PERCEPTION \
  Meta使用脑磁图（MEG）这种非侵入性神经成像技术，每秒进行数千次大脑活动扫描，并开发了一个AI系统，能够几乎实时地解码大脑中的视觉表征

# 9.23 Mon
* 66.节省99.7%训练成本！斯坦福、伯克利新作揭示多模态大模型的视觉表示定律   PaperWeekly  https://mp.weixin.qq.com/s/2Qn7cHyGp3WTL9h1n30zDQ \
  Law of Vision Representation in MLLMs \
  核心问题：\
  1.视觉特征的质量为什么，从何影响 MLLM 的最终性能？ \
  2.如何评估视觉特征的质量？ \
  3.有了视觉特征的质量，如何预测 MLLM 的最终性能？
* 67.o1谎称自己没有CoT？清华UC伯克利：RLHF让模型学会撒谎摸鱼，伪造证据PUA人类  新智元  https://mp.weixin.qq.com/s/zX-PeM5wLl7sRcTA9cizRw \
  LANGUAGE MODELS LEARN TO MISLEAD HUMANSVIA RLHF \
  清华、UC伯克利等机构研究者发现，RLHF之后，AI模型学会更有效地欺骗人类了！种种证据证明，LLM被RLHF后学会了玩心眼子，伪造自己的工作来「向上管理」，对人类展开了「反PUA」
* 68.李飞飞创业之后首个专访：视觉空间智能与语言一样根本  机器之心  https://mp.weixin.qq.com/s/N5iKQAEHm0V1MQqioQgR5g
* 69.仅用4块GPU、不到3天训练出「开源版GPT-4o」，这是国内团队最新研究  机器之心  https://mp.weixin.qq.com/s/6SEsQqd9265A2CycOul9fA \
  LLaMA-Omni: Seamless Speech Interaction with Large Language Models \
  LLaMA-Omni能够接收语音指令，同步生成文本和语音响应，响应延迟低至 226ms，低于 GPT-4o 的平均音频响应延迟 320ms
* 70.OpenAI开启推理算力新Scaling Law，AI PC和CPU的机会来了  量子位  https://mp.weixin.qq.com/s/KBji7k4uWrnglxHMgd7wyg \
  小模型专门加强推理能力，放弃在参数中存储大量世界知识 \
  若是想让AI同时掌握高阶推理能力和大量知识的任务应该怎么办？于是乎，技术的聚光灯再次对焦到了大模型和RAG的组合。具体而言，向量数据库让大模型能够快速有效地检索和处理大量的向量数据，为大模型提供了更丰富和准确的信息，从而增强了模型的整体性能和应用范围。可以说是让大模型有了“好记忆”，减少出现答非所问的情况。

# 9.24 Tue
* 71.姚期智院士大模型新研究：思维图DoT，用数学理论确保AI逻辑一致性  量子位  https://mp.weixin.qq.com/s/pzn802jOIHeL0L9L_yaytw \
  On the Diagram of Thought \
  提出思维图（Diagram of Thought），让大模型思考更像人类。团队更是为这种推理过程提供了数学基础，通过拓扑斯理论（Topos Theory）正式化（formalize）DoT，确保其逻辑一致性和合理性。

# 9.25 Wed
* 72.(**可以看看**)单卡千图推理！港中深等提出全新混合架构多模态大模型LongLLaVA 
 PaperWeekly  https://mp.weixin.qq.com/s/95MU8ntoQMhTjfvGED9qGA \
  该团队将模型架构调整为 Mamba 和 Transformer 块的混合体，在数据构建中考虑多个图像之间的时间和空间依赖性，并采用渐进式训练策略。提出了首个混合架构多模态大语言模型 LongLLaVA，在效率和性能之间实现了更好的平衡 \
  looongLLaVA: Scaling Multi-modal LMs to1000 Images Effciently via Hybrid Architecture \
  https://github.com/FreedomIntelligence/LongLLaVA

# 9.26 Thur
* 73.斯坦福新作：无指令调优的指令遵循  机器之心  https://mp.weixin.qq.com/s/gBDQHtOgx26sjQ-APWBRqQ \
  Instruction Following without Instruction Tuning \
  ???没太明白研究个啥
* 74.调研219篇文献，全面了解GenAI在自适应系统中的现状与研究路线图  机器之心  https://mp.weixin.qq.com/s/H8CRFb2apFEFeF8gsgX4Vg \
  Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap
* 75.(**值得看看**)张俊林详细拆解o1：OpenAI o1完整训练过程逆向推演  夕小瑶科技说  https://mp.weixin.qq.com/s/2rK59hPV8C_PeDsT8Ii73w 
* 76.Meta首款多模态Llama 3.2开源！1B羊驼宝宝，跑在手机上了  新智元  https://mp.weixin.qq.com/s/3MvuJg-956_mdE3-c-4B4w 

# 9.27 Fri
* 77.(**值得看看**)「群体智能」剑指AGI革命！国产架构挑战Transformer霸权，首款非Attention模型更新  新智元  https://mp.weixin.qq.com/s/79yBB8HSuuQPN9aV-RqP5A \
  与其造神，不如依靠群体的力量：这家公司走出了一条不同于OpenAI的AGI路线  机器之心  https://mp.weixin.qq.com/s/RlW1xoo7o8SHzcIpw1R1Dw \
  RockAI首次提出的MCSD（Multi-Channel Slope and Decay）架构就是跳出Transformer路线的一次绝佳尝试 \
  MCSD ??? \
  MCSD: An EfÃcient Language Model with Diverse Fusion \
  在架构层面，RockAI 用一个名叫 **MCSD**（multi-channel slope and decay）的模块替换了 Transformer 中的 Attention 机制；在算法层面，RockAI 提出了一种**类脑激活机制**。这是一种分区激活的机制，就像人开车和写字会分别激活脑部的视觉区域和阅读区域一样，Yan 1.3 会根据学习的类型和知识范围来自适应调整部分神经元，而不是让全量的参数参与训练。
* 78.完全使用「自生成数据」实现LLM自我纠正，DeepMind新突破SCoRe：纠正性能提升15.9%  新智元  https://mp.weixin.qq.com/s/r9ggNLjai-CPKFDcV87IpQ \
  Training Language Models to Self-Correct viaReinforcement Learning \
  Google DeepMind的研究人员发布了一种多轮在线强化学习（RL）方法 SCoRe，在完全使用自生成数据（entirely self-generated data）的情况下，显着提高了LLM的自我纠正能力

# 9.28 Sat
* 79.(**值得看看**)基于认知理论的 AI 架构探索  OneMoreAI  https://mp.weixin.qq.com/s/M9Fr9KToWlYPHNFmzQkQ9w \
  Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning \
  文中名为 Sibyl 实验项目是笔者构建的一套参考认知理论的 AI 系统，在 GAIA（通用AI助手评测）榜单上取得第一的成绩。Sibyl 这个名字来源于《Psycho-Pass》中由众多人脑组成的多智能体系统
* 80.(**可以看看回放**)整合信息论概览介绍：如何定量刻画意识？｜周日直播·整合信息论读书会  集智俱乐部  https://mp.weixin.qq.com/s/RJIv2ZocahVSpWgksaAj0Q \
  
# 9.29 Sun
* 81.Ilya预言错了！华人Nature一作给RLHF「判死刑」，全球大模型都不可靠  机器学习研究组订阅  https://mp.weixin.qq.com/s/lrZGc_PjCUXdgAYEUiF2wQ \
  Larger and more instructable language models become less reliable \
  来自VRAIN、剑桥等机构研究人员对o1-preview等领先的LLM开启了全方位评测，结果发现：\
  - LLM&人类无法保持一致：人类认为复杂的任务，LLM轻易解决；而对人类小菜一碟的问题，LLM却失败了。\
  - LLM不会「回避」复杂任务，而是强撑面子费力思考半天，最终仍旧答错。\
  - 提示工程，无法挽救LLM的不可靠。

# 9.30 Mon
* 82.(**值得研究与尝试**)端到端优化所有能力，字节跳动提出强化学习LLM Agent框架AGILE  机器之心  https://mp.weixin.qq.com/s/cUXuXhfwP6--18DUJkA2dA \
  来自字节跳动 ByteDance Research 的研究人员提出了基于强化学习（Reinforcement Learning, RL）的 LLM Agent 框架 ——AGILE。该框架下，Agent 能够拥有记忆、工具使用、规划、反思、与外界环境交互、主动求助专家等多种能力，并且通过强化学习实现所有能力的端到端训练。尤其值得注意的是，AGILE 框架允许 Agent 在不自信时主动向人类专家寻求建议。这带来了两大优势：首先，Agent 在处理复杂问题时能够持续保持高准确率；其次，通过向人类学习，增强了其快速适应新任务的泛化能力。\
  AGILE: A Novel Framework ofLLM Agents \
  https://github.com/bytarnish/AGILE \
  采用强化学习训练模型何时向人类求助
* 83.北大陈宝权教授：从图形计算到世界模型  机器之心  https://mp.weixin.qq.com/s/COf48GmYBYaRWipGCQSBBA 
