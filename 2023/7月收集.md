# 7.1 周六
* 1、斯坦福大学吴佳俊：通过自然监督编码理解视觉世界 https://mp.weixin.qq.com/s/R5vyph7RBZOaPW0Tu1BK0A \
我们利用自然界中存在的丰富的结构、符号和程序，是为了在视觉世界中更好地感知，更好地理解。因此，有很多丰富的视觉效果，或者场景，你会意识到这不仅仅是像素，尽管这些模型总是以像素为基础，但它们实际上是比像素更丰富的结构。我们是否有可能利用像素之外的某种结构信息，用于智能场景理解和编辑。

# 7.2 周日
* 2、赋予LLM视觉理解能力，360人工智能研究院开源中文多模态对话模型SEEChat https://mp.weixin.qq.com/s/6cPjK5LlP6WFP_XQQ5RRsQ 
* 3、大模型与端到端会成为城市自动驾驶新范式吗？ https://mp.weixin.qq.com/s/_Z5ZJV896rPMARkHSflOVA
* 4、首个见证AI重大突破的领域：数学 https://mp.weixin.qq.com/s/jg-Wg9dAW89ALgbT1WeTjw \
  项目地址：https://leandojo.org/ \
  加州理工、英伟达、MIT等机构的学者，构建了一个基于开源LLM的定理证明器。 \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/9874c22c-f70e-4494-a7be-f08df767f69c)

# 7.4 周二
* 5、理解指向，说出坐标！开源模型“Shikra”开启多模态大模型“参考对话”新模式！ https://mp.weixin.qq.com/s/wIkhAcHgqeQ3LA12J6oBnA \
在人类的日常交流中，经常会关注场景中不同的区域或物体，人们可以通过说话并指向这些区域来进行高效的信息交换。这种交互模式被称为参考对话（Referential Dialogue）。 \
如果 MLLM 擅长这项技能，它将带来许多令人兴奋的应用。例如，将其应用到 Apple Vision Pro 等混合现实 (XR) 眼镜中，用户可以使用视线注视指示任何内容与 AI 对话。同时 AI 也可以通过高亮等形式来指向某些区域，实现与用户的高效交流。
本文提出的 Shikra 模型，就赋予了 MLLM 这样的参考对话能力，既可以理解位置输入，也可以产生位置输出。 \
论文地址：http://arxiv.org/abs/2306.15195 \
代码地址：https://github.com/shikras/shikra \
![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/99972fa3-d5b0-4877-a886-f87253027564)

# 7.5 周三
* 6、大模型自主智能体爆火，OpenAI也在暗中观察、发力，这是内部人的分析博客 https://mp.weixin.qq.com/s/_IkpIIqeKbAj-shvnCV8rg \
  **！！！自主智能体，值得看看** \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/2b847109-b322-46ec-8559-2954541ac657)

# 7.6 周四
* 7、AI智能体卷爆大模型！AutoGPT等4大Agent打擂，「西部世界」谁将成为软件2.0？ https://mp.weixin.qq.com/s/b04F8oQfRaY2z-FjzA4pMw \
  项目1：斯坦福、谷歌「西部世界」  项目2：Camel   项目3：BabyAGI  项目4：AutoGPT \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/63783013-56d1-46ce-a50c-2d64f03429dd)
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/595507ed-f3bf-43a7-b1e5-a53f4a26e1dc)
* 8、OpenAI预言：超级智能10年内降临！正集结算力拯救人类，4年彻底攻克对齐 https://mp.weixin.qq.com/s/X2HfzAxaoSg-28k7BRLvqA
* 9、破解大模型「涌现」之谜：新奇性搜索是AI腾飞的踏脚石 https://mp.weixin.qq.com/s/xPgifx46aZijR_7k-ARE_Q
* 10、Nature：大模型越大越好吗【好文译递】第 8 期 https://mp.weixin.qq.com/s/D1F0fIBP8jDGBD9YF-XmVA 《 In AI, is bigger always better?》
* 11、首个全量化Vision Transformer的方法FQ-ViT，AI大模型落地不远了！  https://mp.weixin.qq.com/s/bipXFZG4lczlqcsMvBMeBA \
* 12、【2023新书】决策智能手册：在复杂世界中基于证据做出决策的实用步骤, 376页pdf https://mp.weixin.qq.com/s/V2dbSmOlv3xXq-thMZHDEw \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/a245f9a2-89e9-493b-895f-02f1de8912af)
* 13、大型视觉模型和视觉提示工程 https://mp.weixin.qq.com/s/wFQ3eTXw7SyhBZe80E6gsQ ？？？什么是视觉提示工程
* 14、太牛了！微软最新研究：LONGNET，Transformer序列长度可支持 10亿+ Token https://mp.weixin.qq.com/s/zJE-1xtD49vUiUOqlv3Hpw \
  微软研究提出了一种Transformer变体：LONGNET，该架构将序列标记长度扩展到了10亿+，且并不会影响较短序列的性能。LONGNET的核心是扩展注意力，将计算复杂度从二次降低到线性。LONGNET可以用作分布式训练器，「跨多个GPU」设备并行训练序列。

# 7.9 周日
* 15、大模型时代，解析周志华教授的「学件」思想：小模型也可做大事 https://mp.weixin.qq.com/s/mYWSDAXzlx-ZW9fE5eVilQ

# 7.10 周一
* 16、李飞飞「具身智能」新成果！机器人接入大模型直接听懂人话，0预训练就能完成复杂指令 https://mp.weixin.qq.com/s/XleXS_5shzZNiOSxUFZfgQ \
  大模型接入机器人，把复杂指令转化成具体行动规划，无需额外数据和训练。  **！！！非常重要** \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/2f4526f3-3616-4f4b-b194-11e9d94ccd15)
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/b0c67d0d-1f5f-422e-8daa-de480614e393)

# 7.12 周三
* 17、YoloV8与ChatGPT互通，这功能是真的强大！ https://mp.weixin.qq.com/s/ODIFRyvfbZOiEORLdWGc_A
* 18、百川开源最强中英文百亿参数模型！超越LLaMA，中国开源大模型开启商用新纪元 https://mp.weixin.qq.com/s/tVc2zvW3JHJbxln-tCuxIQ
* 19、NLP算法培养计划：跟着大佬一起学，不上岸全额退！ https://mp.weixin.qq.com/s/MN_2st7eU0GEQxUuu8uTWg

# 7.13 周四
* 20、语言大模型的进化轨迹 https://mp.weixin.qq.com/s/NyCawOqtA5mH4Ht5nUFOwg
* 21、Transformer作者：指令型智能体的构建之法 https://mp.weixin.qq.com/s/Vt_TGHKaifKVfDPfmKaqKw

# 7.14 周五
* 22、清华大学&腾讯提出DreamDiffusion：你大脑中的画面，可以高清还原了！ https://mp.weixin.qq.com/s/tC2f2FxPMjZCfVH5k1TOYA \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/c3ec229a-1a03-422a-895b-08e277b3fe52) \
  论文地址：https://arxiv.org/pdf/2306.16934 \
  项目地址：https://github.com/bbaaii/DreamDiffusion

# 7.18 周二
* 21、Transformer后继有模！MSRA提出全新大模型基础架构：推理速度8倍提升，内存占用减少70% https://mp.weixin.qq.com/s/zJsb2vdpEwqXEUgqHmpInA \
  Retentive Network（RetNet）：大模型领域Transformer的继任者。RetNet实现了良好的扩展结果、并行训练、低成本部署和高效推理。这些特性使这一基础架构，成为大语言模型中Transformer的有力继承者。 \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/e48e7352-9344-4e6b-8ae0-8a4d67c40b71)

# 7.19 周三
* 22、Google DeepMind掌舵人Demis Hassabis专访：合并后「超级单元」内幕，以及如何开展下一代模型研究  https://mp.weixin.qq.com/s/xnaGDDArWE4gcfxl6GphqA \
  DeepMind 与 Google Brain合并为超级单元（super unit）
* 21、更强的Llama 2开源，可直接商用：一夜之间，大模型格局变了 https://mp.weixin.qq.com/s/klFWFXCbjGaWZ7HO1KFZag \
  今日，Meta 终于发布了大家期待已久的免费可商用版本 Llama 2。 \
  大模型社区再掀波澜，Meta重磅开源LLAMA-2，性能升级可商用 https://mp.weixin.qq.com/s/Eqh-ED4BgiR4BBQQbwXAmA \
  一夜之间，大模型格局变了！ datawhale https://mp.weixin.qq.com/s/cMO_9t5FUGAkQd6V3bJjBw
* 22、AI学语言与人脑极为相似！新研究证明：语言并非人类与生特有的能力，机器也能学丨Nature子刊  https://mp.weixin.qq.com/s/SG0jzmBcu9Ff8NcGmdw1gQ
* 23、类脑智能软硬件协同创新，自动化所团队研发脉冲神经网络加速器“智脉·萤火” https://mp.weixin.qq.com/s/sCELNbxCbbiO2Asgur5Kuw \
  一、创新地利用了FPGA器件中的专用运算模块DSP48E2实现脉冲神经网络的高效运算 \
  二、设计了一个内存系统实现高效的突触权重和膜电压内存访问 \
  三、运算吞吐率达到了5.53TOP/s，在多个数据集和脉冲神经网络上取得了明显的加速效果

# 7.20 周四
* 24、也谈凌晨刷屏的Llama2开源可商用模型：从其数据构造、模型架构和评估方式等方面的一些总结与发现 老刘说nlp https://mp.weixin.qq.com/s/qTJp_hBDiIvJ-ymxLr-Pmg
* 25、轻量级MobileSAM：比FastSAM快4倍，处理一张图像仅需10ms（附源代码） https://mp.weixin.qq.com/s/-rvDQdCIlPULATjY9yOfsA
* 26、iPhone、Mac上都能跑，刷屏的Llama 2究竟性能如何？ 机器之心 https://mp.weixin.qq.com/s/q4xVrfAsCzfdeRoquCV5cg
* 27、再看Llama2的实际体验与民间评测：从现有公开在线测试地址到几个测试例子看初步效果分析 https://mp.weixin.qq.com/s/KIg9sgKC4pHXheTqtK3DFA
  
# 7.21 周五
* 28、听伯克利博士给你讲解Llama 2的技术细节 https://mp.weixin.qq.com/s/9FBMPuoWxLLqrzeVZ8CkfA \
  强推！伯克利AI博士详解Llama 2的技术细节 https://mp.weixin.qq.com/s/Mee7sMq_bxLpIOOr91li9A
* 29、清华开了家员工都是GPT的公司，代码、文档一条龙服务 https://mp.weixin.qq.com/s/hV9cFI9JOzW2dDauL1jHHA
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/30ba15b1-5c87-4b32-b7c8-b4ab4dbc2cf8)
  论文地址 ：https://arxiv.org/abs/2307.07924 \
  **ChatDev重点研究一下**
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/d86dff5a-25ce-4392-9737-ab305bd92620)
* 30、ChatGPT推出自定义指令：说一次就记住，每次对话都能遵守 https://mp.weixin.qq.com/s/X3Pvmxb2BQPtquwqg29QVg
* 31、语言模型做先验，统一强化学习智能体，DeepMind选择走这条通用AI之路 机器之心 https://mp.weixin.qq.com/s/_UlyX_iP0d9fw8L1OPDGSQ \
  一直以来，DeepMind 引领了强化学习（RL）智能体的发展，从最早的 AlphaGo、AlphaZero 到后来的多模态、多任务、多具身 AI 智能体 Gato，智能体的训练方法和能力都在不断演进。\
  近日，谷歌 DeepMind 在一篇新论文《Towards A Unified Agent with Foundation Models》中，探讨了利用基础模型打造统一的智能体。\
  强化学习与大语言模型、视觉语言模型等基础模型的进一步融合 **重点研读该论文**
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/ee51ca99-e044-4f22-a0a8-4de61550a251)
  论文地址：https://arxiv.org/pdf/2307.09668.pdf \
  DeepMind首提「统一智能体」！大模型做推理，赋能终身学习，AI王者加冕 https://mp.weixin.qq.com/s/WbGSo0Xys4Zy17yrkhZEvg

# 7.22 周六
* 32、入门指引：知识图谱应用及基于大模型进行信息抽取的开源实践代表案例推介 https://mp.weixin.qq.com/s/o2OY1I2gVOi2lYGtUVQ4YQ

# 7.23 周日
* 33、12种模态，一个学习框架，Meta-Transformer实现骨干网络大一统 https://mp.weixin.qq.com/s/k7OdIrqf4_3SkfFjMpNieQ \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/767c07c1-b801-46b5-bfd8-d7a0c5531e6d)
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/2d811f22-8352-454b-a425-f3f074ae4a8f)
  网站地址：https://kxgong.github.io/meta_transformer/ \
  代码地址：https://github.com/invictus717/MetaTransformer
* 34、我为什么放弃了 LangChain？ https://mp.weixin.qq.com/s/Iwe6M391b2BBWae-HmOIJQ
  「LangChain 的流行已经扭曲了围绕其本身的人工智能创业生态系统，这就是为什么我不得不坦诚自己对它的疑虑。」\
  langchain论文 论文链接：https://arxiv.org/pdf/2210.03629.pdf \
  该文作者指出了langchain的缺点，开发并开源了 simpleaichat：一个用于轻松连接聊天应用程序的 Python 程序包，它强调代码的最小复杂度，并将向量存储等高级功能与对话逻辑解耦。\
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/560d5d6e-4787-4522-8732-f7edf9f6d4bb)
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/df0c5311-f1ef-4251-9da8-770337fd7890)
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/640efa61-e1a0-4759-a538-9aa57550ca48)

# 7.25 周二
* 35、人人玩转Llama 2！Meta正式官宣免费用，微调羊驼指南大全集 https://mp.weixin.qq.com/s/eKAjuVSi00p33plIb0hsag
* 36、【发布】代码模型 CodeGeeX2-6B 开源，最低6GB显存，性能优于StarCoder https://mp.weixin.qq.com/s/qw31ThM4AjG6RrjNwsfZwg

# 7.26 周三
* 37、Attention机制竟有bug，Softmax是罪魁祸首，影响所有Transformer https://mp.weixin.qq.com/s/VYi8CqPogxm8CZgRdG6loA \
  「我发现注意力公式里有个 bug，八年了都没有人发现。所有 Transformer 模型包括 GPT、LLaMA 都受到了影响。」\
  有人甚至猜测「这就是微软 RetNet 比 transformer 性能更优的原因？」
* 38、Llama-2首个全方位评测，国内外开源模型大比拼 https://mp.weixin.qq.com/s/66CUqQt8G-N5b6ZUoJMZ5w
* 39、刚刚！OpenAI、谷歌、微软等 宣布成立前沿模型论坛，旨在推动模型的安全发展！ https://mp.weixin.qq.com/s/VhsgU88_-ldoCELjYhT4Kg
* 40、如何基于Llama 2搭建自己的大模型？8月26日，4位技术大牛手把手教你 https://mp.weixin.qq.com/s/LHu-rknMIxfN1A6U-FcheA
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/7acbca62-5e4c-4396-a4d2-a1955975f45e)
* 41(**非常重要**)、像GPT-4一样能看懂图文，李飞飞等人的具身AI给机器人造了个多模态对话框 https://mp.weixin.qq.com/s/D2XZd84q5tLTigq8Ld0nbQ \
  VIMA 是一个带有机械臂的 LLM ，它接受多模态 Prompt ：文本、图像、视频或它们的混合。\
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/d2f8a6bc-dfea-4362-9d83-f0262c13c14c)
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/f0d4b411-eb95-4e12-85c3-11c1554ae9e7)
  论文地址：https://arxiv.org/pdf/2210.03094.pdf \
  论文主页：https://vimalabs.github.io/ \
  Github 地址：https://github.com/vimalabs/VIMA

# 7.26 周三
* 42、拓展技术边界，掌握AI大语言模型微调（LLaMA） 【赠算力】  老刘说nlp  https://mp.weixin.qq.com/s/7qAoGlYSE6vSMAyaf9hgPw
* 43、大模型=缸中之脑？通院朱松纯团队剖析AGI关键缺失  机器之心  https://mp.weixin.qq.com/s/OUku0k2lRBix5ltREBkT8Q
  报告进一步提出通用人工智能（AGI）应具备的四个特征：能够执行无限任务，自主生成新任务，由价值系统驱动，以及拥有反映真实世界的世界模型。\
  研究人员在技术报告中指出，“知行合一”（认识和行动的内在统一）是大模型目前所欠缺的机制，也是迈向通用人工智能的必经之路。研究人员认为，概念的学习依赖于与真实世界的交互，且知识的获取并不完全依赖于被动输入，在新环境中获取知识的关键途径更应该是主动探索和试错而非被动接受。\
  论文链接：https://arxiv.org/abs/2307.03762 \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/70e073e6-9b46-4658-a38f-52d165b63ae9)

# 7.28 周五
* 44、「真实网络世界」测试场上线：基于GPT-4的智能体也只能跑10.59%成功率 机器之心 https://mp.weixin.qq.com/s/DdUXXwrBQsd8evU7icQ3VA
  随着生成式 AI 的发展，利用大语言模型构建 AI 智能体逐渐走红。比如斯坦福、谷歌联合构建了一个具有 25 个 AI 智能体的「虚拟小镇」，「小镇居民」的行为比人类角色扮演的更加真实，甚至举办了一场情人节派对。\
  又比如商汤、清华等机构提出的通才 AI 智能体 **Ghost in the Minecraft (GITM)**，在《我的世界》中比以往所有智能体都有更优秀的表现…… \
  这些 AI 智能体的先后涌现，甚至让人认为是未来通用人工智能（AGI）的雏形。 \
  然而，有些智能体主要是在简化的合成环境中创建和测试的，这极大地限制了它们在现实场景中的应用。强如 ChatGPT，也只能通过插件的方式与互联网进行有限的互动。\
  本文，来自卡耐基梅隆大学（CMU）等机构的研究者引入了一个逼真且可复现的网络环境 **WebArena**，旨在促进研究者开发能够执行各种任务的自主智能体。\
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/a9149d16-744d-4192-9d82-40d7d0e17a68)
  论文地址：https://arxiv.org/pdf/2307.13854.pdf \
  论文主页：https://webarena.dev/#try-it-yourself \
  项目地址：https://github.com/web-arena-x/webarena
  WebArena 网络环境包含四个网络应用程序：在线购物、论坛讨论、协作开发以及业务内容管理。 \
  为了模拟人类解决问题的方式，WebArena 还嵌入了实用工具和知识资源：实用工具如地图、计算器和草稿本；知识资源如文档、知识库、开发工具使用手册等。\
  除了 WebArena 之外，他们还开源了一个包含 812 个任务的网络任务基准。
* 45、放弃Softmax，首个线性注意力Transformer大模型：1750亿参数，速度、精度更优 机器之心 https://mp.weixin.qq.com/s/QUxEQtKiYmPePoFLcSrrRw \
  针对transformer中softmax的问题，放弃了softmax \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/b76867e0-64e9-4475-9d33-2860af0e6c3d)
  论文：https://arxiv.org/abs/2307.14995 \
  模型：https://github.com/OpenNLPLab/TransnormerLLM

# 7.29 周六
* 46、（**非常重要**）RT-2  Robotic Transformer —— 机器人的 transformer 模型
  机器人ChatGPT来了：大模型进现实世界，DeepMind重量级突破 机器之心 https://mp.weixin.qq.com/s/xeMbt9mknZVaL3XguQxemg \
  谷歌AGI机器人大招！54人天团憋7个月，强泛化强推理，DeepMind和谷歌大脑合并后新成果 量子位 https://mp.weixin.qq.com/s/4PdT32s-E6QOKkOgOm30cA \
  谷歌发布RT-2，实体机器人版ChatGPT来了！ https://mp.weixin.qq.com/s/XJhw1jYk31YE62dqMTfpag \
  谷歌打造「终结者」！ChatGPT版最强机器人AGI，动嘴操控007 https://mp.weixin.qq.com/s/3P1NHBznIAsW7yD_dvp0Vw \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/8e77d9dd-9013-4841-a1b9-b0a0e8bb7067)
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/b3a08f43-bda3-4a48-96cc-efdd8521584b)
  项目地址：https://robotics-transformer2.github.io/
* 47、轨迹预测的视觉方法有哪些？最新综述！ StrongerTang https://mp.weixin.qq.com/s/GYqfDIuv7fH-8kPB90afEg \
  最近一个综述论文 “Trajectory-Prediction With Vision: A Survey ”，来自现代和安波福的公司Motional；不过它参考了牛津大学的综述文章“Vision-based Intention and Trajectory Prediction in Autonomous Vehicles: A Survey ”。

# 7.30 周日
* 48、加州伯克利&Google | 揭秘Transformer大模型上下文学习能力：线性模型学习 AINLPer https://mp.weixin.qq.com/s/Ik0r9ek_QM9k8mc1ogQHog \
  论文地址：https://arxiv.org/pdf/2306.09927.pdf \
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/df5a5def-50a5-4a35-a672-2195954c35fe)
* 49、Science封面|麻省理工论文：生成式AI对生产力的影响 AIGC开放社区 https://mp.weixin.qq.com/s/vpTTuoSWoeQMIUBAzvW7hg
* 50、（**值得研究**）北大用ChatGPT组建了个开发团队：大模型分饰多角色，协同完成软件开发任务 AIGC开放社区 https://mp.weixin.qq.com/s/MqB2Uc2nZ0w8hESi_PRrUg
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/f9c13867-c379-42d3-b034-51c479fea456)
  论文链接：https://arxiv.org/pdf/2304.07590.pdf





