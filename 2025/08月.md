# 8.1 Fri
* 1.为IIT提供结构，为GWT提供内部逻辑，为预测处理提供区分动态的意识范畴UTD  CreateAMind  https://mp.weixin.qq.com/s/QkvvcluM89ZOw9Hy9GeHHQ \
  From Differentiation to Cognition:UTD as a Model of Recursive Awareness
* 2.图灵奖得主加持，蒙特卡洛树搜索×扩散模型杀回规划赛道｜ICML 2025 Spotlight  量子位  https://mp.weixin.qq.com/s/hmNdvQ3MljFl-NiAia8bOQ \
  Monte Carlo Tree Diffusion for System 2 Planning \
  突破了扩散模型在长程任务推理阶段缺乏可扩展性的瓶颈

# 8.2 
* 3.多模态后训练反常识：长思维链SFT和RL的协同困境  机器之心  https://mp.weixin.qq.com/s/NVG3hjr1xIuLKRSmqckrWg \
  在多模态视觉语言模型（VLM）中，SFT+RL组合难以实现协同增益，甚至有时会互相拖后腿 \
  The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs

# 8.3 Sun
* 4.大模型训练的时间都花在哪了？  关于NLP那些你不知道的事  https://mp.weixin.qq.com/s/bl2_kCxShpghaf9BAu9xbg 
* 5.DeepMind哈萨比斯：AI能建模所有进化而来的事物  量子位  https://mp.weixin.qq.com/s/4vuQrBPHZvjFHiKZhSHFnw 
* 6.图灵奖得主Sutton再突破：强化学习在控制问题上媲美深度强化学习？  机器之心  https://mp.weixin.qq.com/s/I8IE8Ck-k5OoAy7SLqi9-Q \
  Swift-Sarsa: Fast and Robust Linear Control \
  近些天，Sutton 再发新论文，在强化学习领域再次发力，将他在 2024 年的时序差分学习新算法 SwiftTD 拓展到控制领域，在与一些更强大的预处理算法结合使用时，能够展现出与深度强化学习算法相当的性能表现

# 8.4 Mon
* 7.为什么GRPO重要性权重设计错误？详解Qwen3新强化学习算法GSPO  关于NLP那些你不知道的事  https://mp.weixin.qq.com/s/2Ta46U-OpNJPzKORu2wpPw \
  GRPO 对于 next-token 的重要性权重，容易引入高方差的噪声
* 8.连续思考机器 Continuous Thought Machines  CreateAMind  https://mp.weixin.qq.com/s/hwnmiEuOKbmIjNJ3p_LFoQ 
* 9.高质量「上下文工程」资源整理（含速览和精读）  夕小瑶科技说  https://mp.weixin.qq.com/s/bkrTmiWmI6Hxeo6F5IliIA
* 10.Meta华人新秀毕树超，重磅爆料下一代LLM路线！RL+预训练直通AGI  新智元  https://mp.weixin.qq.com/s/EqgKkEB0z90WRnx-sO57OQ
* 11.LLM抢人血案：强化学习天才被挖空，一朝沦为「无人区」！  新智元  https://mp.weixin.qq.com/s/_hUl6kkea6VlS04K-WHmsg \
  PufferLib
* 12.3D-R1：让AI理解3D世界的下一步  机器之心  https://mp.weixin.qq.com/s/TgFY_hZcA7tKX163kztHXg \
  3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding 

# 8.5 Tue

# 8.6 Wed
* 13.震撼，世界模型第一次超真实地模拟了真实世界：谷歌Genie 3昨晚抢了OpenAI风头  机器之心  https://mp.weixin.qq.com/s/iI0-UDW70nOqyRb95WuZNw \
  谷歌Genie3全网玩疯！画质飞跃720P，网友造出西幻RPG游戏  量子位  https://mp.weixin.qq.com/s/KYLIU6MQC_MIOwloahP3Iw \
  仍然存在局限
* 14.Attention Sink现象揭秘：Transformer为何偏爱首Token？  PaperWeekly  https://mp.weixin.qq.com/s/R62I_KVA3FHnk2xAsUlEtg 

# 8.7 Thur
* 15.人大高瓴-华为诺亚：大语言模型智能体记忆机制的系列研究  机器之心  https://mp.weixin.qq.com/s/n_oc7X1cZ1vGwPWhXpLUjA \
  A Survey on the Memory Mechanism of Large Language Model based Agents
* 16.(**值得看看**)强化学习的10层境界：从巴甫洛夫的狗到贝叶斯大脑  关于NLP那些你不知道的事  https://mp.weixin.qq.com/s/mWAAdUQ7LmBFGNtQ3MU8Zg 
* 17.(**从经验中学习的RL框架**)强化学习+MCP=王炸？开源框架教AI在MCP中玩转工具解决任务，实测效果超越GPT！  量子位  https://mp.weixin.qq.com/s/YaP6aTuKvONTpnLp_VEUHA \
  专注于LLM+RL的科技公司OpenPipe提出全新开源强化学习框架——MCP·RL \
  只需一个MCP Server的地址，agent就能自动发现工具、生成任务，通过强化学习在闭环反馈中摸索出最优调用策略 \
  MCP·RL是科技公司OpenPipe基于强化学习的智能体训练系统(Agent Reinforcement Trainer，ART)的最新项目。ART是一个开源强化学习框架，其核心思想是让LLM从经验中学习，从而提高agent的可靠性，ART可以将GRPO集成到任何Python应用中。 \
  https://github.com/OpenPipe/ART?tab=readme-ov-file#-notebooks
* 18.字节&MAP重塑大模型推理算法优化重点，强化学习重在高效探索助力LLM提升上限  量子位  https://mp.weixin.qq.com/s/8XfVIRaLfgViBCPNVew0jA \
  一个普遍存在的现象是：在训练过程中，模型的熵值迅速下降，推理路径趋于固化，导致“利用（exploitation）”远超“探索（exploration）”，严重失衡。这种过早收敛不仅削弱了模型的多样性生成能力，也限制了其性能上限的进一步突破。 \
  First Return, Entropy-Eliciting Explore（FR3E） \
  受openai经典论文 First Return, Then Explore 的启发

# 8.8 Fri
* 19.追问专访·Donald Hoffman教授 | 我们能否构建出意识的数学模型？  追问  https://mp.weixin.qq.com/s/GDmf0sdFZRQznu5OMu3V8w \
  意识领军人物科赫：一个浪漫还原论者的自白  追问  https://mp.weixin.qq.com/s/Ry_igAgItWIy5eUYGEgc4w \
  《意识探索》
* 20.DeepSeek的GRPO会导致模型崩溃？看下Qwen3新范式GSPO  机器之心  https://mp.weixin.qq.com/s/YSlp-SXzi7bSW2Y-shJ8ww \
* 21.硬核拆解大模型，从 DeepSeek-V3 到 Kimi K2 ，一文看懂 LLM 主流架构  机器之心  https://mp.weixin.qq.com/s/_as8aCv325cAeJ6kMv9_aA

# 8.9 Sat
* 22.ICML 2025 | 千倍泛化不涨显存！蚂蚁推出新注意力机制，实现16M上下文精准检索  PaperWeekly  https://mp.weixin.qq.com/s/jdU8o07RanOrlcvDXz9VAA \
  提出一种基于因果检索的注意力机制 GCA (Grouped Cross Attention)，完全端到端地学习如何从上文检索并挑选最相关片段，从而实现超长序列高性能处理与泛化能力。人类记忆的另一个特性是大部分时候记忆处于沉睡状态，相关记忆片段只会在激活时进入意识。类似地，GCA 通过将上文信息卸载到 CPU / 磁盘，只在需要的时候动态加载需要的片段到 GPU 的方式，大幅降低了长文本处理的显存开销 \
  Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling \
  https://github.com/ant-research/long-context-modeling
* 23.3万字长文！深度解析大语言模型LLM原理  Datawhale  https://mp.weixin.qq.com/s/7aYSBY0UxyEHKcqzC1fqtg

# 8.10 Sun
* 24.面向视觉语言模型的持续学习：遗忘之外的综述与分类体系  专知  https://mp.weixin.qq.com/s/_-Vsu56jEy-b_Hvi1EbyOw \
  VLMs 如何在随时间获取新知识的同时，不遗忘其原有能力？ \
  Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting \
  https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models

# 8.11 Mon
* 25.最大熵逆强化学习：理论基础、数学推导与工程实现  图灵人工智能  https://mp.weixin.qq.com/s/14Jfr4uu4poUFm1J1VebJw \
  重点讨论逆强化学习（Inverse Reinforcement Learning, IRL），这是模仿学习的重要分支，其核心目标是基于演示数据学习能够最大化期望奖励的最优策略
* 26.超越样本级RL！人大×快手提出ARPO：熵驱动Agent探索，多轮推理性能飙升  PaperWeekly  https://mp.weixin.qq.com/s/25P7v93nU81HFJoy9T678Q \
  Agentic Reinforced Policy Optimization（ARPO）
* 27.4D空间智能：AI如何一步步「看懂」时空结构？一篇综述解析通往四维世界的五大层次  机器之心  https://mp.weixin.qq.com/s/b2B7fli3qmASykTlYV9eBA \
  Reconstructing 4D Spatial Intelligence: A Survey \
  Project Page：https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence \
  第一层（Level 1）：底层三维属性的重建（如深度、位姿、点云图等） \
  第二层（Level 2）：三维场景组成要素的重建（如物体、人体、建筑、场景等）  \
  第三层（Level 3）：完整的 4D 动态场景的重建 \
  第四层（Level 4）：包含场景内部组成部分之间交互关系的重建 \
  第五层（Level 5）：引入物理规律以及相关约束条件的重建
* 28.为了解释意识，哲学家们重新拾起泛灵论  神经现实  https://mp.weixin.qq.com/s/xBFPPzM_FhX2zntEMzUZqg
* 29.一文全解析：AI 智能体 8 种常见的记忆（Memory）策略与技术实现  Datawhale  https://mp.weixin.qq.com/s/nojzkY-dEnMZ99T17M9wZA 

# 8.12 Tue
* 30.英伟达为机器人推出懂推理的“大脑”！升级版Cosmos世界模型来了  量子位  https://mp.weixin.qq.com/s/r0-Lae5_KfR3-OzvtzXGAw \
  Cosmos主要被用来生成符合现实世界物理规律的合成数据，自发布以来，已被Figure、Agility Robotics、通用汽车等一众知名机器人和自动驾驶公司采用 \
  https://github.com/nvidia-cosmos
* 31.刚刚，商汤内部两万字复盘曝光：多模态通往AGI核心路线首次公开  新智元  https://mp.weixin.qq.com/s/72Spa0wCbRZ7X72-o9bOvQ \
  《迈向多模态通用智能：商汤的思考》
* 32.李飞飞押注的「世界模型」，中国自研Matrix-3D已抢先实现了？   新智元  https://mp.weixin.qq.com/s/hrmjLcaM7j71dARkyiWcYg \
  技术报告：https://github.com/SkyworkAI/Matrix-3D/blob/main/asset/report.pdf \
  项目主页：https://matrix-3d.github.io/ \
  Github：https://github.com/SkyworkAI/Matrix-3D \
  Hugging Face：https://huggingface.co/Skywork/Matrix-3D
* 33.Lumina-mGPT 2.0：自回归模型华丽复兴，媲美顶尖扩散模型  机器之心  https://mp.weixin.qq.com/s/3zcFinrOaK7ZDAojVws5QA \
  一款独立的、仅使用解码器的自回归模型，统一了包括文生图、图像对生成、主体驱动生成、多轮图像编辑、可控生成和密集预测在内的广泛任务 \
  GitHub 地址：Alpha-VLLM/Lumina-mGPT-2.0  

# 8.13 Wed
* 34.《自然》期刊：大脑看世界的方式，竟与语言模型惊人一致  波动智能  https://mp.weixin.qq.com/s/th_URh4ORqYgPh1eCEXeLQ \
  这项研究之所以令人震撼，不仅因为它揭示了语言模型与人脑视觉表征之间的高度一致性，更因为它触及了一个深层次的问题：我们的大脑究竟是如何理解世界的？而语言模型，又是如何在没有视觉经验的前提下，构建出与人类视觉系统高度契合的语义空间？ \
  一个令人振奋的启示是：以 LLM 嵌入为训练目标的视觉模型，在数据量远小于传统模型的情况下，依然能学得更接近人脑的表征。这表明强语义目标可能比海量数据更重要。虽然 LLM 本身是通过大规模文本训练得到的，其“语义密度”远高于传统的类别标签，这是否应计入总数据量仍是一个开放问题。但无论如何，这种训练方式为数据效率提供了新的思路。
* 35.
* 36.

# 8.14 Thur
# 8.15 Fri
# 8.16 Sat
# 8.17 Sun

# 8.18 Mon
# 8.19 Tue
# 8.20 Wed
# 8.21 Thur
# 8.22 Fri
# 8.23 Sat
# 8.24 Sun

# 8.25 Mon
# 8.26 Tue
# 8.27 Wed
# 8.28 Thur
# 8.29 Fri
# 8.30 Sat
# 8.31 Sun
