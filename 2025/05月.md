# 5.1 Thur
* 1.400万token新SOTA！英伟达UIUC联手：兼顾长短上下文顶尖性能  新智元  https://mp.weixin.qq.com/s/h8R0JGbHKaxQJAMg8DjmZw \
  来自英伟达和UIUC的华人团队提出一种高效训练方法，将LLM上下文长度从128K扩展至惊人的400万token SOTA纪录！基于Llama3.1-Instruct打造的UltraLong-8B模型，不仅在长上下文基准测试中表现卓越，还在标准任务中保持顶尖竞争力 \
  From 128K to 4M: Effcient Training of Ultra-Long Context Large Language Models
* 2.深夜突袭，DeepSeek-Prover-V2加冕数学王者！671B数学推理逆天狂飙  新智元  https://mp.weixin.qq.com/s/Dsn3iypDSpzUVC35XX8Z1A \
  就在刚刚，DeepSeek-Prover-V2技术报告也来了！34页论文揭秘了模型的训练核心——递归+强化学习，让数学推理大提升。有人盛赞：DeepSeek已找到通往AGI的正确路径！ \
  Hugging Face：https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B \
  GitHub：https://github.com/deepseek-ai/DeepSeek-Prover-V2/tree/main \
  论文链接：https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/main/DeepSeek_Prover_V2.pdf
* 3.(**值得看看**)意识:A beautiful loop:实现AGI的条件及证据（知道自己知道的计算模型及大量证据）  CreateAMind  https://mp.weixin.qq.com/s/RVogxALZOYnv1a9aW1Iodw \
  A beautiful loop:An active inference theory of consciousness \
  主动推理能模拟意识吗？我们提供了三个条件来说明它可以。第一个条件是模拟现实或生成世界模型，它决定了可以知道或采取行动的内容；即知识领域。第二个是推断竞争进入世界模型。只有那些能够连贯地减少长期不确定性的推断才能获胜，显示出我们称之为贝叶斯绑定的意识选择。第三个是知识深度，即贝叶斯信念在整个系统中的反复共享。由于这个递归循环——在一个层级系统（如大脑）中——世界模型包含了它存在的知识。这与自我意识不同，因为世界模型非局部地、连续地知道自己（即场证据）。形式上，我们提出了一个超模型，用于在整个层级结构中进行精确控制，其潜在状态（或参数）编码并控制所有推断层的整体结构和加权规则。这个美丽循环理论对于冥想、迷幻药和改变状态、最小现象体验，以及为有意识的人工智能提供了新的视角。

# 5.2 Fri
* 4.浙大&港理工等提出InfiGUI-R1：利用强化学习，让GUI智能体学会规划任务、反思错误  机器之心  https://mp.weixin.qq.com/s/KafgV8WxsV02fSNbUxxozQ \
  InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners \
  项目仓库：https://github.com/Reallm-Labs/InfiGUI-R1 \
  模型地址：https://huggingface.co/Reallm-Labs/InfiGUI-R1-3B \
  让 AI 像人一样在行动前思考，行动后反思 \
  这种模式要求智能体不仅能看懂界面，还要能： \
  理解任务意图：将高层指令分解为具体的执行步骤 \
  进行空间推理：准确理解界面元素的布局和关系，定位目标 \
  反思与纠错：识别并从错误中恢复，调整策略
* 5.LoRA中到底有多少参数冗余？新研究：砍掉95%都能保持高性能  机器之心  https://mp.weixin.qq.com/s/B0pgqJIOqcHKjz3roAVeog \
  即使大幅减少 LoRA 的可训练参数，模型性能依然保持强劲 \
  LoRI: Reducing Cross-Task Interference in Multi-Task LowRank Adaptation \
  代码链接：https://github.com/juzhengz/LoRI \
  HuggingFace：https://huggingface.co/collections/tomg-group-umd/lori-adapters-67f795549d792613e1290011 \
  以 Llama-3-8B 和 Mistral-7B 作为基础模型，他们的结果表明，LoRI 达到或超过了全量微调（FFT）、LoRA 和其他 PEFT 方法的性能，同时使用的可训练参数比 LoRA 少 95%。值得注意的是，在使用 Llama-3 的 HumanEval 上，B 中具有 90% 稀疏度的 LoRI 比 LoRA 高出 17.3%。 \
  LoRI 通过实现适配器合并而无需手动选择合并方法来解决这些挑战。通过使用固定的、随机初始化的投影 A，LoRI 将任务特定的适配器映射到近似正交的子空间，从而减少合并多个 LoRI 时的干扰。 \
  在持续学习的同时能避免灾难性遗忘的问题 \
  总体而言，LoRI 提供了一种轻量级且有效的方法来构建安全适配器，在支持下游任务适应的同时保持对齐。
* 6.Sebastian Raschka 新书《从头开始推理》抢先看，揭秘推理模型基础  机器之心  https://mp.weixin.qq.com/s/zQUB9ZXqtSRGJU_YWMoMEw \
  《Reasoning From Scratch》 \
  原文地址：https://magazine.sebastianraschka.com/p/first-look-at-reasoning-from-scratch \

# 5.3 Sat
* 7.大模型终于通关《宝可梦蓝》！网友：Gemini 2.5 Pro酷爆了  量子位  https://mp.weixin.qq.com/s/cdXXhcEVNIt-TN-gM_QRbg \
  Gemini玩宝可梦的基本步骤如下： \
    1.截取屏幕截图并检索游戏状态数据 \
    2.用网格覆盖处理图像，以辅助空间推理 \
    3.将屏幕截图和游戏信息发送给模型 \
    4.AI决定是直接响应还是调用专门的智能体 \
    5.解析响应内容，以确定按下哪个按钮 \
    6.执行按钮按下操作，并等待游戏更新 \
    7.对下一帧重复该过程
* 8.别再卷数据了，LLM也怕「过劳死」！CMU等揭秘灾难性过度训练  新智元  https://mp.weixin.qq.com/s/ddsGATwCerCFkr_cJ-SuiA \
  增加更多的预训练数据来扩展语言模型，反而可能会导致后训练阶段的性能下降！这就是「灾难性过度训练」现象。 \
  Overtrained Language Models Are Harder to Fine-Tune \
  更长时间的预训练并不一定能导致更高质量的模型
* 9.Nature综述：大规模神经形态计算  集智俱乐部  https://mp.weixin.qq.com/s/TTvmtr8OhWcdPhxTyNDPHw \
  Neuromorphic computing at scale \
  神经形态计算（Neuromorphic Computing）指脑启发的硬件与算法设计方法，研究者们借鉴神经科学中的生物智能原理来设计高效的计算系统，尤其适用于对体积、重量和功耗有严格要求的应用场景。当前，该研究领域正处于发展的关键阶段，因此明确未来大规模神经形态计算系统的发展方向至关重要。本文探讨了构建规模可扩展的神经形态计算架构的方法，并总结了其中的关键特征。此外，我们分析了可以从规模扩展中获益的潜在应用场景，以及需要解决的主要挑战。进一步地，我们审视了支持该领域持续发展的完整技术生态系统，并探讨了规模扩展所带来的新机遇。我们的研究汇总了多个计算子领域的观点，为神经形态计算研究人员和从业者提供指导，以推动该领域的进一步发展

# 5.4 Sun
* 10.一般物理系统的自我表征的原则性限制  CreateAMind  https://mp.weixin.qq.com/s/lAzY9jajKX8-Z0eI6_FfNw \
  Principled Limitations on Self-Representation for Generic Physical Systems \
  自我观察、自我表征及伴随的自我控制理念遍及认知科学与生命科学领域，见于免疫学、机器人学等诸多学科。本文以普适视角探讨这些理念是否合理及其适用边界。通过构建物理相互作用的通用模型，我们证明了一个核心定理及若干推论，这些结论严格限制了自我观察、自我表征与自我控制的可实现形式。研究特别表明：即便在理论上，为系统的元层级组件添加观察、表征或控制功能，也无法实现系统整体的完整元层级表征。我们由此论证：自我表征至多具有启发性意义，且自我模型通常无法由其实现系统进行实证检验。 
* 11.【NTU博士论文】让语言模型更接近人类学习者  专知  https://mp.weixin.qq.com/s/M-HiOxKnAyHUZb5yHmuB-w \
  Making language models better human-like learners 
* 12.强化多模态大语言模型：基于强化学习的推理综述  专知  https://mp.weixin.qq.com/s/A7inhSn_Q1MNSi8M3wYCIg \
  Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models
* 13.3B模型逆袭7B巨头！Video-XL-Pro突破长视频理解极限，大海捞针准确率超98%  量子位  https://mp.weixin.qq.com/s/b76WUrYNc65B6kqbxkqpaw \
  Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding \
  代码链接：https://github.com/VectorSpaceLab/Video-XL/tree/main/Video-XL-Pro \
  模型链接：https://huggingface.co/MINT-SJTU/Video-XL-Pro-3B \
  训练数据链接：https://huggingface.co/datasets/MINT-SJTU/Video-XL-Pro-Training
* 14.Nature意识之争：两大理论首次正面对决  集智俱乐部  https://mp.weixin.qq.com/s/e0XbAP7RvY4Y6saikJ154A \
  Adversarial testing of global neuronal workspace and integrated information theories of consciousness \
  意识如何从大脑活动中涌现？这一终极问题催生了多种理论，其中全局神经元工作区理论（Global Neuronal Workspace Theory, GNWT）和整合信息理论（Integrated Information Theory, IIT）最具影响力。GNWT认为意识源于前额叶皮层（Prefrontal Cortex, PFC）的“全局信息广播”，而IIT主张后皮层“热区”的神经整合是意识的核心。尽管两者各有证据支持，却从未被直接比较。

# 5.5 Mon
* 15.AGI与沉思，心灵与超级对齐的计算模型 3万字  CreateAMind  https://mp.weixin.qq.com/s/osQu7oT3Npcl3BM27txklA \
  Contemplative Wisdom for Superalignment \
  随着人工智能（AI）的进步，传统的对齐策略在面对不可预测的自我改进、隐藏的子目标以及智能系统的复杂性时可能失效。我们主张在AI的认知架构和世界模型中内建固有道德，而非通过外部手段约束行为。受冥想智慧传统的启发，我们展示了四项公理化原则如何能在AI系统中培育出具有韧性的智慧世界模型：第一，正念（mindfulness）使能自我监控和对涌现子目标的重新校准；第二，空性（emptiness）防止教条式的目标固着并弱化僵化的先验假设；第三，非二元性（non-duality）消解对抗性的自我-他者边界；第四，无量慈悲（boundless care）驱动普遍减少痛苦的动机。研究发现，引导AI对这些原则进行反思可改善其在AILuminate基准测试中的表现（基于GPT-4o），特别是原则的组合应用效果更佳。我们为当前最先进的模型提供了详细的实现策略，包括：沉思架构、宪法机制以及思维链强化方法。对于未来系统，主动推理框架（active inference framework）可能为具身智能体提供所需的自组织和动态耦合能力来实践这些洞见。这种跨学科方法为现有脆弱的控制方案提供了具备自我修正和韧性的替代路径。
* 16.边学边练，推理觉醒：LUFFY让强化学习即学即用！  机器之心  https://mp.weixin.qq.com/s/OtngauQEPzPbvDjAoQfMmA \
  Learning to Reason under Off-Policy Guidance \
  https://github.com/ElliottYan/LUFFY \
  整合强化学习与模范学习 \
  这两种「只学不练」和「只练不学」的策略各有弊端：前者往往学得快但泛化差，后者可能探索勤但效率低。那么，有没有两全其美的办法，让模型既能借鉴高手经验又能保持自主探索？最近，上海 AI 实验室联合西湖大学、南京大学和香港中文大学的研究团队提出了一种全新的强化学习范式：LUFFY（Learning to reason Under oFF-policY guidance）  \
  LUFFY 的核心理念是：在训练过程中让模型同时借助高手的推理轨迹进行学习（离策略示范），又能继续独立地试错探索（在线推理），从而实现 「边学边练，学以致用」的目标。实验显示，LUFFY 在多个数学推理挑战任务中实现了平均 + 7.0 分的性能飞跃，并在分布外任务上展现出显著的泛化能力。
* 17.谷歌DeepMind：大模型也很任性，知道最优路径偏要撞南墙  机器之心  https://mp.weixin.qq.com/s/8wxEyYNYr5L9k0Kb64_O4g \
  LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities \
  LLM 智能体存在次优探索和知 - 行差距（knowing-doing gap）的问题，即无法有效地将模型中的知识转化为行动 \
  本文，来自谷歌 DeepMind 的研究者系统地研究了为什么 LLM 在决策场景中表现次优的原因。特别是，本文深入研究了三种常见的失败模式：贪婪性、频率偏差和知 - 行差距 \
  在此基础上，本文提出通过强化学习对自动生成的 CoT 推理过程进行微调，以缓解这些不足。实验表明 RL 微调能有效提升 LLMs 的决策能力 —— 既增强了智能体探索性行为，又缩小了知 - 行差距。
* 18.
* 19.
* 20.

# 5.6 Tue
# 5.7 Wed

# 5.8 Thur
# 5.9 Fri
# 5.10 Sat
# 5.11 Sun
# 5.12 Mon
# 5.13 Tue
# 5.14 Wed

# 5.15 Thur
# 5.16 Fri
# 5.17 Sat
# 5.18 Sun
# 5.19 Mon
# 5.20 Tue
# 5.21 Wed

# 5.22 Thur
# 5.23 Fri
# 5.24 Sat
# 5.25 Sun
# 5.26 Mon
# 5.27 Tue
# 5.28 Wed

# 5.29 Thur
# 5.30 Fri
# 5.31 Sat
