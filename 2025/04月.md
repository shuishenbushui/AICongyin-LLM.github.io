# 4.1 Tue
* 1.R1–Zero强化学习路线新发现及R1思路用于GUI Agent动作预测方案  老刘说NLP  https://mp.weixin.qq.com/s/cFkOEyrG3bnFPldX9VWP2w \
  继续来看大模型可解释性，最近的工作 《Understanding R1-Zero-Like Training: A Critical Perspective》（https://arxiv.org/abs/2503.20783，https://github.com/sail-sg/understand-r1-zero） ，结论很有趣，研究了多种基础模型，以了解预训练特性如何影响RL性能。 \
  看第二个工作，《UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning》(https://arxiv.org/pdf/2503.21620，https://github.com/lll6gg/UI-R1)，基于规则的RL如何增强多模态大模型(MLLM)对图形用户界面(GUI)动作预测任务的推理能力。
* 2.给GRPO提提速！厦大 | 提出强化学习算法：CPPO，最高8倍加速！  AINLPer  https://mp.weixin.qq.com/s/ijlMoLwlkG9AXYZC89376A \
  CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models \
  https://github.com/lzhxmu/CPPO

# 4.2 Wed
* 3.主动推理隐含最小意识理论  CreateAMind  https://mp.weixin.qq.com/s/knGg6C2PTTA-D1d9B5eHSA \
  On the minimal theory of consciousness implicit in active inference \
  体验的多面性给意识的研究带来了挑战。传统神经科学方法通常专注于孤立的方面，例如感知意识或意识的整体状态，并围绕相关的经验范式和发现构建理论。因此，意识理论往往难以比较；事实上，这些理论试图解释的现象可能几乎没有重叠之处。在这里，我们采用了一种不同的方法：从主动推理（active inference）开始，这是一种基于贝叶斯推理（Bayesian inference）建模行为的首要原则框架，并逐步构建出一种最小化的意识理论，这种理论从主动推理下推导出的计算模型的共同特征中涌现出来。我们回顾了一系列将主动推理模型应用于意识研究的工作，并认为所有这些模型中都隐含着一组小的理论承诺，这些承诺指向了一种最小化（且可检验的）意识理论。
* 4.是什么、如何、何处，以及效果如何？——大语言模型测试时扩展的调研  专知  https://mp.weixin.qq.com/s/7uTIt9Z3-DFnJtEw4deDnw \
  从四个核心维度来结构化 TTS 研究：扩展什么、如何扩展、在何处扩展、扩展效果如何 \
  What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models
* 5.一个LangChain与MCP结合使用的案例！  Datawhale  https://mp.weixin.qq.com/s/MA17aivO4Oe3iv_trlj2Ag 
* 6.动态场景，开放文本查询！清华哈佛联合建模4D语言场 | CVPR 2025  新智元  https://mp.weixin.qq.com/s/x4JTo-eujTPnPFRP3R-Dhg 
* 7.AI理解27分钟长视频超越GPT-4o，港理工新国立开源新框架：角色化推理+链式LoRA  量子位  https://mp.weixin.qq.com/s/Tw_eZmeCYTTCtIkvbkjX7w \
  港理工、新加坡国立团队推出VideoMind框架，核心创新在于角色化推理（Role-based Reasoning）和链式LoRA（Chain-of-LoRA）策略。 \
  VideoMind的提出不仅在于视频理解性能的突破，更在于提出了一个模块化、可扩展、可解释的多模态推理框架。该框架首次实现了类似人类行为的“指定计划、搜索片段、验证结果、回答问题”流程，真正让AI能“像人类一样理解视频”，为未来的视频理解和多模态智能系统领域奠定了基础。 \
  VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning \
  https://videomind.github.io/
* 8.脑波解码延迟仅80毫秒，实时「意念对话」技术登Nature子刊  机器之心  https://mp.weixin.qq.com/s/HcgdYRHYQxRHuRvE7nbgbg \
  A streaming brain-to-voice neuroprosthesis to restore naturalistic communication
* 9.2025美国最新奥数题，让大模型集体翻车，DeepSeek R1平均分也不到5%  机器之心  https://mp.weixin.qq.com/s/TT1Owl925f4Ky1vReKpYwA \
  PROOF OR BLUFF? EVALUATING LLMS ON 2025 USA MATH OLYMPIAD \
  https://github.com/eth-sri/matharena \
  所有模型都无法解决超过一个问题，这凸显了当前大型语言模型在奥林匹克级数学推理任务中的局限性。这表明现有优化方法如 GRPO 对需要高度逻辑精确性的任务可能尚不足够。
* 10.用 MCP 让大模型自动批量解读文献，保姆级教程来了！  Datawhale  https://mp.weixin.qq.com/s/VWlntkxCtB-FLzAistng_w
* 11.有深度！Transformer | 万字长文：详细了解前馈神经网络（FFN），内含对大模型的理解  AINLPer  https://mp.weixin.qq.com/s/XLW0iU2XLwDmv5jGd6tckg

# 4.3 Thur
* 12.视觉SSL终于追上了CLIP！Yann LeCun、谢赛宁等新作，逆转VQA任务固有认知  机器之心  https://mp.weixin.qq.com/s/V7Ml_xgiiQalxnGQmIWi_Q \
  CLIP被淘汰了？LeCun谢赛宁新作，多模态训练无需语言监督更强！  新智元  https://mp.weixin.qq.com/s/FpisxJQ9AXHV26lHPwzy5A \ 
  在最近的一项研究中，Yann LeCun、谢赛宁等研究者探讨了一个基本问题： 语言监督对于多模态建模的视觉表征预训练是否必要？ \
  Scaling Language-Free Visual Representation Learning
* 13.智能体丝滑玩手机，决策延迟0.7秒！MSRA等提出验证器架构，不直接依赖大模型生成最终操作  量子位  https://mp.weixin.qq.com/s/GxTJrTp-gfg0N1wchpYptg \
  Advancing Mobile GUl Agents: A Verifier-Driven Approach to Practical Deployment
* 14.DeepMind闭关修炼「我的世界」，自学成才挖钻登Nature！人类玩家瑟瑟发抖  新智元  https://mp.weixin.qq.com/s/OgjLy_jE9Rk9iZhioPRTDQ \
  Mastering diverse control tasks through world models
* 15.浙大校友重磅革新Transformer！多token注意力让LLM开挂，错误率归0  新智元  https://mp.weixin.qq.com/s/LoVCN-4WsEEK8ZuEyWCyrA \
  Multi-Token Attention

# 4.4 Fri
* 16.信息分解和大脑的信息架构  CreateAMind  https://mp.weixin.qq.com/s/3tV9f-Vq6E14lLjdOB8OZQ \
  Information decomposition and the informational architecture of the brain \
  为了阐述大脑如何协调信息处理以实现认知功能，我们必须理解信息本身。至关重要的是，信息并非是一个单一的实体。信息分解技术提供了一种将信息拆分为其构成要素的方法：独特信息、冗余信息和协同信息。我们回顾了如何通过区分协同和冗余的相互作用来重新定义我们对大脑整合功能及其神经组织的理解。为了阐释大脑如何在冗余和协同之间权衡取舍，我们回顾了整合多方面证据，包括协同和冗余的结构、分子和功能基础；它们在认知和计算中的作用；以及它们是如何在进化和发育过程中产生的。总体而言，区分协同和冗余信息为理解大脑和认知的信息架构提供了一个指导原则。
* 17.动态框架中的挑战与解决方案  CreateAMind  https://mp.weixin.qq.com/s/QH9JNdUBJIcSdo82i5yWJg \
  Integrated Information Theory and the Phenomenal Binding Problem: Challenges and Solutions in a Dynamic Framework \
  基于神经科学的意识理论必须解释现象学的结合问题，例如微观信息单元是如何组合成人类现象学中常见的宏观意识体验的。一个例子是，视觉场景中的单个“像素”被体验为“心灵之眼”中的单一整体图像，而不是作为个体的、分离的、大规模并行的体验，这些体验可能分别对应于单个神经元激活、神经元集合或中央凹扫视，从信息处理的角度来看，任何一种都可能提供相同的功能。现象学结合问题存在多个有争议的候选解决方案。本文探讨了集成信息理论（IIT）4.0版本的形而上学基础如何提供一种独特的解决方案。这种解决方案——即可以从多个单元聚合而成的特定实体（“复合体”）定义了存在——可能在静态图像中有效，但在动态系统中引入了问题。我们问，当主要复合体在生物神经网络中移动时，我们的现象学自我会发生什么。我们对意识实体随时间发展的描述，导致了 IIT 理论家面临的一个明显困境，即非局域实体转换与连续自我的选择：“动态实体演化问题”。除了明确这一困境外，我们还描述了 IIT 可能在其站稳脚跟之前化解这一困境的三种方式。阐明 IIT 在现象学结合问题上的立场，可能借助新的实证或理论研究作为支撑，有助于研究人员理解 IIT 并评估其合理性。我们认为我们的论文有助于 IIT 当前的研究重点，即从静态分析向动态分析的转变。 \
  像素（感知单元）如何连接成一幅图像（感知整体）
* 18.元认知，元元认知  CreateAMind  https://mp.weixin.qq.com/s/Z3gwFb0mADh8s0O-33GY5g \
  元认知粒子、心理行动与代理感知 \
  主动推理隐含最小意识理论 \
  意识理论列表
* 19.DeepSeek R2来了？全新推理时Scaling论文联手清华震撼发布！  新智元  https://mp.weixin.qq.com/s/kZF1sJlIxIlCXv9j0iBWgQ \
  Inference-Time Scaling for Generalist Reward Modeling

# 4.5 Sat
* 20.7B扩散LLM，居然能跟671B的DeepSeek V3掰手腕，扩散vs自回归，谁才是未来？  机器之心  https://mp.weixin.qq.com/s/CVssj4w-UPXodjIO640_Ww \
  香港大学和华为诺亚方舟实验室的一项研究就是其中之一。他们刚刚发布的扩散推理模型 Dream 7B 拿下了开源扩散语言模型的新 SOTA，在各方面都大幅超越现有的扩散语言模型。 \
  Dream 7B 最终选择了 Qwen2.5 7B 的权重作为初始化基础。在训练过程中，作者发现学习率参数至关重要：设置过高会迅速冲淡初始权重中宝贵的从左到右知识，对扩散训练几无助益；设置过低则会束缚扩散训练的进展。作者精心选择了这个参数以及其他训练参数。 \
  https://hkunlp.github.io/blog/2025/dream/
* 21.三思而后行，让大模型推理更强的秘密是「THINK TWICE」？  机器之心  https://mp.weixin.qq.com/s/xCN70_gwjkRTAh7nWdtggA \
  Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking \
  https://github.com/a-m-team/a-m-models \
  “Think Twice” 展示了一种简单有效的思路：鼓励大模型主动 “反思”，用多轮推理激发更强的认知能力。它不仅提升了准确率，更令模型在语言表达上变得更加理性、紧凑、自信。 \
  在训练成本不断攀升的今天，这种无需再训练的 “轻量级优化” 无疑具有极强的现实吸引力。未来，多轮推理或许能成为一种标准机制，帮助模型更接近真正意义上的 “会思考”。
* 22.对神经流形与认知回路的统一视角  CreateAMind  https://mp.weixin.qq.com/s/68IJc8HzJyqhOCu7RqmCOg \
  A unifying perspective on neural manifolds and circuits for cognition \
  两种不同的视角为解释大脑与行为之间的联系提供了理论基础。一种方法试图识别执行特定功能的神经回路元件，强调神经元之间的连接性作为神经计算的基础。另一种方法则聚焦于神经流形——即在神经群体活动中行为信号的低维表示——并提出神经计算通过涌现动力学来实现。尽管神经流形揭示了异质性神经活动中的可解释结构，但在连接性中找到相应的结构仍然是一项挑战。我们重点介绍了一些例子，在这些例子中，建立低维活动与连接性之间的对应关系是可能的，从而统一了神经流形和神经回路的观点。这种关系在神经反应的几何结构反映其在大脑中空间布局的系统中尤为显著，例如苍蝇导航系统。此外，我们描述了证据表明，在神经反应具有异质性的系统中，回路由通过低秩连接性作用于流形上的活动模式间的相互作用组成。我们认为，如果我们要能够因果性地测试关于行为背后的神经计算理论，那么统一流形和回路的方法至关重要。

# 4.6 Sun
* 23.LLM「想太多」有救了！高效推理让大模型思考过程更精简  新智元  https://mp.weixin.qq.com/s/dTNB7ueI5Vd8OC9TrvqBjA \
  Stop Overthinking: A Survey on Effcient Reasoning for Large Language Models \
  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs
* 24.多模态融合与视觉-语言模型：面向机器人视觉的综述  专知  https://mp.weixin.qq.com/s/DWoXDlh_bBe9-E96v-zKEw \
  Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision \
  https://github.com/Xiaofeng-Han-Res/MF-RV
* 25.Meta深夜开源Llama 4！首次采用MoE，一张H100就能跑，竞技场超越DeepSeek  Datawhale  https://mp.weixin.qq.com/s/m8SWReTOvVyeAhY0uyX4Zg \
  Llama 4 Scout，16位专家的170亿激活参数的多模态模型，单个H100 GPU可运行， 同类SOTA，并拥有10M上下文窗口 \
  Llama 4 Maverick，128位专家的170亿激活参数多模态模型，击败GPT-4o和Gemini 2.0 Flash，与DeepSeek-V3同等代码能力参数只要一半，主打与DeepSeek一样的性价比，单个H100主机即可运行。

# 4.7 Mon
* 26.Science颠覆认知：你的大脑不是“机器”，而是一支交响乐团！  图灵人工智能  https://mp.weixin.qq.com/s/XOcyTVWrMbHnYk_vlJfnSQ \
  传统神经科学聚焦于孤立脑区的功能定位，但行为和认知是多个脑区协同作用的“涌现特性”（emergent properties）。传统神经科学建立在Brodmann分区基础上，其本质是笛卡尔式机械还原论。现代连接组学（Connectomics）揭示：认知功能是分布式网络在相空间中涌现的吸引子状态。oni的整合信息理论）。
* 27.深入解析图神经网络注意力机制：数学原理与可视化实现  图灵人工智能  https://mp.weixin.qq.com/s/LNrek7vcAwvdyB0UtQXsig 
* 28.视觉中的检索增强生成与理解：综述与新展望  专知  https://mp.weixin.qq.com/s/xRP86lob8QmKhiX9_QXiWg \
  Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook
* 29.反向传播、前向传播都不要，这种无梯度学习方法是Hinton想要的吗？  机器之心  https://mp.weixin.qq.com/s/xXOe0Xw9kk3MGKvnoQejyQ \
  最近，来自牛津大学和 Mila 实验室的研究者向这一问题发起了挑战。他们开发了一种名为 NoProp 的新型学习方法，该方法既不依赖前向传播也不依赖反向传播。相反，NoProp 从扩散和流匹配（flow matching）方法中汲取灵感，每一层独立地学习对噪声目标进行去噪。 \
  NOPROP: TRAINING NEURAL NETWORKS WITHOUT BACK-PROPAGATION OR FORWARD-PROPAGATION
* 30.Rule-based强化学习≠古早逻辑规则！万字拆解o1多模态推理最新进展  PaperWeekly  https://mp.weixin.qq.com/s/8pwCPuXzXoMJsDmdL9tPGA \
  rule-based 的强化学习通过基于规则的奖励机制，成功地为模型提供了一种高效且可靠的优化途径 \
  本篇文章将讨论来自 Aligning Multimodal LLM with Human Preference: A Survey（https://arxiv.org/abs/2503.14504）中五篇近期关注多模态 O1-reasoning 相关的文章。
* 31.LLM幻觉，竟因知识「以大欺小」！华人团队祭出对数线性定律与CoDA策略  新智元  https://mp.weixin.qq.com/s/twUUI_esahGg6u3xGqMGIQ \
  「知识遮蔽」，即模型中的主导知识可以在文本生成过程中，掩盖那些不太突出的知识，从而导致模型编造不准确的细节 \
  The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination \
  此研究深入研究了LLM幻觉，有4大亮点： \
  1 发现幻觉的对数线性规律：幻觉率随着相对知识流行度、相对知识长度和模型规模的对数线性增长 \
  2 在训练或推理前预测幻觉：在训练前「知识遮蔽效应」可预测幻觉发生的可能性 \
  3 提出全新解码策略CoDA（Contrastive Decoding with Attenuation）强调被遮蔽的知识，降低主流知识偏差，大幅提升LLM事实性（Factuality） \
  4 更可预测、更可控的语言模型正在成为现实！研究加深了对LLM幻觉机制的理解，为未来的可解释性与可控性研究打开新方向

# 4.8 Tue
* 32.通过三个条件对最小意识进行了特征描述：一个可证伪的新兴感知框架  CreateAMind  https://mp.weixin.qq.com/s/P3WUIWEiZlmVf5ZOCvcOOQ \
  Beyond Imitation Games: A Falsifiable Emergent Sentience Framework \
  自由能原理和意识是什么关系？
* 33.实现类比推理：概念超空间  CreateAMind  https://mp.weixin.qq.com/s/AdwK5rLbclpA1aBV9cwVjA \
  Analogical Reasoning Within a Conceptual Hyperspace \
  ?什么是类比推理
* 34.三个LLM顶一个OpenAI？2亿条性能记录加持，路由n个「小」模型逆袭  新智元  https://mp.weixin.qq.com/s/z8x6tMeqV98xacixq7m8ZA \
  RouterEval:A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs \
  https://github.com/MilkThink-Lab/RouterEval \
  https://github.com/MilkThink-Lab/Awesome-Routing-LLMs
* 35.Science：人类意识感知的“闸门”——高阶丘脑核团  集智俱乐部  https://mp.weixin.qq.com/s/Z43TimwVChC9Auyx8MaX8A \
  Human high-order thalamic nuclei gate conscious perception through the thalamofrontal loop

# 4.9 Wed
* 36.整合信息论IIT与现象绑定问题：自我意识动态演化框架中的挑战与解决方案模型  CreateAMind  https://mp.weixin.qq.com/s/mwiDXq67rHZ7WLTxOkteNg \
  Integrated Information Theory and the Phenomenal Binding Problem: Challenges and Solutions in a Dynamic Framework \
  基于神经科学的意识理论必须解释现象学的绑定结合问题，例如微观信息单元是如何组合成人类现象学中常见的宏观意识体验的。一个例子是，视觉场景中的单个“像素”被体验为“心灵之眼”中的单一整体图像，而不是作为个体的、分离的、大规模并行的体验，这些体验可能分别对应于单个神经元激活、神经元集合或中央凹扫视，从信息处理的角度来看，任何一种都可能提供相同的功能。现象学绑定结合问题存在多个有争议的候选解决方案。本文探讨了整合信息理论（IIT）4.0版本的形而上学基础如何提供一种独特的解决方案。这种解决方案——即可以从多个单元聚合而成的特定实体（“复合体”）定义了存在——可能在静态图像中有效，但在动态系统中引入了问题。我们问，当主要复合体在生物神经网络中移动时，我们的现象学自我会发生什么。我们对意识实体随时间发展的描述，导致了 IIT 理论家面临的一个明显困境，即非局域实体转换与连续自我的选择：“动态实体演化问题”。除了明确这一困境外，我们还描述了 IIT 可能在其站稳脚跟之前化解这一困境的三种方式。阐明 IIT 在现象学结合问题上的立场，可能借助新的实证或理论研究作为支撑，有助于研究人员理解 IIT 并评估其合理性。我们认为我们的论文有助于 IIT 当前的研究重点，即从静态分析向动态分析的转变。
* 37.英伟达253B开源新王登场，Llama 4三天变陪衬！直逼DeepSeek-R1成推理天花板  新智元  https://mp.weixin.qq.com/s/QbUTBKG9vrIVTA-6qII2gg 
* 38.从零搭一套可复现、可教学、可观察的RL for VLM训练流程，我们试了试  机器之心  https://mp.weixin.qq.com/s/SDUbYwWcwJMCZ2hrlnIqVA \
  Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme \
  https://github.com/GAIR-NLP/MAYE

# 4.10 Thur
* 39.统一 世界模型+认知架构+神经网络  CreateAMind  https://mp.weixin.qq.com/s/Xn6tV8FycSGr4W6irGmBPA \
  Bridging Cognitive Architectures and Generative Models with Vector Symbolic Algebras
* 40.上下文长度扩展：从RoPE到YARN  关于NLP那些你不知道的事 
https://mp.weixin.qq.com/s/gHUEufvzCeyNb-kMSjEDlg 
* 41.重磅：建模各种依赖概念信息的推理机制：统一框架，常识的构建  CreateAMind  https://mp.weixin.qq.com/s/J1_s5wYtw6Lefu72EFNVyQ \
  Reasoning with Concepts: A Unifying Framework

# 4.11 Fri
* 42.概念、连接主义与思维语言  CreateAMind  https://mp.weixin.qq.com/s/oE3296E4gfvJE7HkqPkR9g \
  Concepts, Connectionism, and the Language of Thought \
  http://www.mkdavies.net/Martin_Davies/Mind_files/ConceptsConnLoT.pdf
* 43.如何赋予大型语言模型三维能力？—大型语言模型中的空间推理综述  专知  https://mp.weixin.qq.com/s/YDFkVOLfsWJg4O4rQy0K_A \
  How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM

# 4.12 Sat
* 44.意义的几何与动态，概念空间语义学  CreateAMind  https://mp.weixin.qq.com/s/MvgsmJChHiMuBEo6lTQkhw \
  The Geometry and Dynamics of Meaning
* 45.手机实现GPT级智能，比MoE更极致的稀疏技术：省内存效果不减｜对话面壁&清华肖朝军  量子位  https://mp.weixin.qq.com/s/kANQBUD5-Y8I9JdLrSd3hg \
  面壁智能和清华走出了一条与MoE不同的路径——神经元级稀疏激活，让模型在保持性能的同时大幅降低资源消耗。 \
  Configurable Foundation Models: Building LLMs from a Modular Perspective \
  ???CFM（Configurable Foundation Models）
* 46.魔改AlphaZero后，《我的世界》AI老玩家问世，干活不用下指令  机器之心  https://mp.weixin.qq.com/s/Md2FJh0IQ3X-ztpRW4G5mA \
  现在，AI 可以不断主动学习、纠正错误，展现出了此前大模型智能体无法实现的一系列能力。 \
  看起来，新版的 AI 在与我们共同游戏时不再是催一下动一下了，它已经是一个有「主观能动性」的玩家，就像个和你共同玩过几百局游戏的老友一样。 \
  这项技术名为 AssistanceZero，出自加州大学伯克利分校（UC Berkeley）。值得注意的是，它并未接受大模型常见的 RLHF 训练。相反，它是由「assistance games」强化学习驱动的，研究人员认为，这是构建 AI 助手的更好途径。 \
  AI 在这个框架中并不会被动地接受人类反馈，而是寻求主动与人合作，通过推断目标而不断优化行为，这避免了 RLHF 中 AI 可能会出现的作弊行为，让 AI 可以采取更加协作的策略。 \
  AssistanceZero: Scalably Solving Assistance Games \
  https://github.com/cassidylaidlaw/minecraft-building-assistance-game
* 47.谢赛宁等新作上线，多模态理解生成大一统！思路竟与GPT-4o相似？  新智元  https://mp.weixin.qq.com/s/qUE7D8RDcTmmB1ZNNDB4mQ \
  Transfer betweenModalitieswithMetaQueries \
  https://xichenpan.com/metaquery/

# 4.13 Sun
* 48.意识计算模型-最小体验现象  CreateAMind  https://mp.weixin.qq.com/s/iQPPWnkfpbbbTFlN2F_lww \
  A Computational Model of Minimal Phenomenal Experience (MPE) \
  最小现象体验（MPE），或称“纯粹意识”，是一种基础的意识体验形式，其特征是具有反身性元意识，且缺乏常规现象学中的许多特征。它被描述为例如非概念化的、非时间性的、无自我的、无视角的。本文旨在利用自由能量原理（FEP）推导出的变分自由能量最小化的数学方法，开发一个MPE的计算模型。我采用计算神经现象学方法，在主动推断框架内形式化MPE的关键现象学特征。该模型包含参数深度，允许对生成模型参数进行高阶推断。我将特定的模型参数化与报告的MPE品质（如元意识、平静、无努力感和非概念化）联系起来。所提出的模型表明，当一个主体通过自我导向的意识和对其生成模型的调节实现极低的自由能量时，尤其是通过强调对意识本身的意识，MPE就会产生。该模型预测了MPE现象学的元素，包括一种无努力感、无时间感以及“零人称视角”的可能性。文中概述了对所提模型进行模拟的实施细节，以及实证验证的方向。
* 49.解释AGI，实现AGI，联想学习与主动推理  CreateAMind  https://mp.weixin.qq.com/s/bbbp7Ts0_ob3Kkw0AoOhJA \
  Associative Learning and Active Inference \
  联想学习是一种行为现象，个体基于刺激或事件的共同出现而发展出它们之间的联系。最初由巴甫洛夫在他的条件反射实验中研究，学习的基本原则已经通过发现广泛的学习现象而得到扩展。基于最小化奖励预测误差的概念，已经开发出了计算模型。特别是Rescorla-Wagner模型，是一个极大地影响了强化学习领域的著名模型。然而，这些模型的简单性限制了它们充分解释与学习相关的行为现象的多样性。在本研究中，我们采用了自由能原理，该原理表明生物系统努力在其对世界的内部模型下最小化惊讶或不确定性。我们将学习过程视为自由能的最小化，并研究其与Rescorla-Wagner模型的关系，重点关注学习的信息方面、不同类型的惊讶以及基于信念和价值的预测误差。此外，我们探讨了如何在主动推断框架内模拟众所周知的行为现象，如阻断、掩盖和潜在抑制。我们通过使用注意力的信息和新颖性方面来实现这一点，这些方面与看似矛盾的模型（如Mackintosh和Pearce-Hall模型）提出的类似想法共享。因此，我们证明了自由能原理，作为一个从第一性原理推导出的理论框架，可以整合基于经验实验提出的联想学习的思想和模型，并作为更好地理解大脑联想学习背后的计算过程的框架。
* 50.
* 51.
* 52.
* 53.
* 54.
* 55.
* 56.
* 57.
* 58.
* 59.
* 60.

# 4.14 Mon

# 4.15 Tue
# 4.16 Wed
# 4.17 Thur
# 4.18 Fri
# 4.19 Sat
# 4.20 Sun
# 4.21 Mon

# 4.22 Tue
# 4.23 Wed
# 4.24 Thur
# 4.25 Fri
# 4.26 Sat
# 4.27 Sun
# 4.28 Mon

# 4.29 Tue
# 4.30 Wed
