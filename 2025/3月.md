# 3.1 Sat
* 1.多模态基础模型的机制可解释性综述  专知  https://mp.weixin.qq.com/s/gTLYisN09Omty1HD0OTMGA \
  A Survey on Mechanistic Interpretability for Muti-Modal Foundation Models \
  本综述探索了两个关键方面：（1）将LLM可解释性方法适应到多模态模型；（2）理解单模态语言模型与跨模态系统之间的机制差异。
* 2.大模型是否有自知之明？新研究发现LLM可以知晓自己的知识范围  机器之心  https://mp.weixin.qq.com/s/_SGEw75r6SjcB5JUTg9ENw \
  Do Large Language Models Know How Much They Know? \
  「虽然不同架构涌现这种能力的速率不同，但结果表明，知识意识（awareness of knowledge）可能是 LLM 的一个普遍属性。」 \
  整体来说，这项研究证明了足够规模的 LLM 确实具有知识意识（awareness of knowledge），即能够知晓自己的知识范围。
* 3.ICLR 2025｜AI不语，只是一味根据人类意图推理3D空间定位  机器之心  https://mp.weixin.qq.com/s/DXExxwZ7t6lzdfoKa3kJYQ \
  Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention \
   3D 意图定位（3D-IG）
* 4.刚刚，LangGraph官方开源Ultra版本，多智能体能力提升10倍！  探索AGI  https://mp.weixin.qq.com/s/XRoyD5O9o1CqISFkIDeBBA
* 5.Agent or SFT or RL ? 9个多模态R1推理开源项目核心思路解析  老刘说NLP  https://mp.weixin.qq.com/s/yIDqvJLASPWX6gkOoRr1Pg 
* 6.DeepSeek-V3 / R1 推理系统概览  吃果冻不吐果冻皮  https://mp.weixin.qq.com/s/HBuIB1yoXyYKVwqqgt0SFQ
* 7.一个统一的视角理解：计算力学、因果抽象和信息分解  集智俱乐部  https://mp.weixin.qq.com/s/LW3Rvs6JIRFlwTMamZiwLg
  在众多研究涌现的框架里，计算力学的框架相比较而言很宏大，而且讲起来有特别的吸引力。它依托于生物进化论的思想，以信息论为工具，同时有硬核的统计物理解释以及严谨的数学表达。如下图所示，它假设宇宙是一个巨大的确定动力系统，每一个生物体或者说主体，都因其有限的观察能力，只能感知到宇宙的一部分，也就是环境。环境中就有噪音，而这很可能威胁到主体的生存。所以迫于进化的压力，每一个主体都要尽可能有能力预测到环境的变化。对于每一个主体的构造而言，除去那些实际的物理和化学结构以外，计算力学关注的是它的“虚拟层”。主体根据传感器获知环境的历史信息，再根据自己的内在模型做出预测，然后做出相应的行动。这个内在模型的概念，和现在流行的**自由能原理**与**世界模型**理论也是相一致的。 \
  光能做出准确的预测还不够。比如现在的大模型预测能力很强，那是不是说让每一个主体都背着一个大模型就万事大吉了？这太笨重了。如果对环境建模的内在模型非常复杂，就会消耗很多资源，也不利于主体的生存。所以大自然要求主体既要能预测环境变化，又要这个预测模型尽可能简洁。这个既要又要，使得主体不断地动态调整自己的内在模型，总要在一个最合适的尺度上归纳总结周围的环境变化。而这，便是主体为什么进化出识别涌现现象的能力，我们人类也不例外。

# 3.2 Sun
* 8.LeCun世界模型再近一步！Meta研究证明：AI可无先验理解直觉物理  新智元  https://mp.weixin.qq.com/s/OeUYyfEonlKlwQQEwhLVgg \
  V-JEPA不是去生成像素级的精准预测，而是在抽象的表示空间里进行预测。这种方式更接近LeCun所认为的人类大脑处理信息的模式。\
  Intuitive physics understanding emerges from self-supervised pretraining on natural videos \
  这次的主要发现如下： \
  1.V-JEPA能够准确且一致地分辨出，符合物理定律的视频和违反物理定律的视频，远超多模态LLM和像素空间中的视频预测方法。 \
  2.虽然在实验中观察到改变模型的任一组件，都会影响性能，但所有V-JEPA模型都取得了明显高于随机水平的表现。
* 9.【AAAI2025教程】大型语言模型中的知识生命周期：记忆、编辑与超越，216页ppt  专知  https://mp.weixin.qq.com/s/fF4fHQ9tSCs3XgVjJ-ywCQ \
  The Lifecycle of Knowledge in Large Language Models: Memorization, Editing, and Beyond \
  https://llmknowledgelifecycle.github.io/AAAI2025_Tutorial_LLMKnowledge \
  本教程探讨大型语言模型中的知识生命周期，包括：(1) 通过基础模型预训练知识的涌现；(2) 外部知识的注入；(3) 知识的更新与修改；(4) 知识的探测与生成；(5) 多模态知识的整合，包括对物理世界的理解和程序性规划。
* 10.DeepSeek关键RL算法GRPO，有人从头跑通了，贡献完整代码  机器之心  https://mp.weixin.qq.com/s/e0-tVsaIgNajBTOl117ctg \
  DeepSeek关键RL算法GRPO，手把手教你从头跑通！  Datawhale  https://mp.weixin.qq.com/s/gi8ee4m6borLPlPRBcp7Mg \
  GRPO 算法丢弃了 critic model，放弃了价值函数近似，转而通过组内样本的相对比较来计算策略梯度，从而有效降低了训练的不稳定性，同时提高了学习效率。 \
  教程地址：https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb
* 11.超越人类！DeepMind强化学习新突破：AI在「我的世界」中封神！  新智元  https://mp.weixin.qq.com/s/RTEcC56XLXLqMtov3Og1mQ \
  Improving Transformer World Models for Data-Efficient RL \
  Crafter是一个2D版的《我的世界》，具体来说，他们用的是Craftax-classic环境，它是Crafter的快速复刻版。Craftax-classic环境有几个很好的特点：1.每次游戏的环境都是随机生成的，AI需要应对不同的挑战。2.AI只能看到局部视野，就好像只能看到屏幕的一部分，而不是整个地图。3.这是一个以成就层级来设定奖励信号的体系，需要进行深入且广泛的探索才能达成。 \
  DeepMind研究团队的这篇论文主要研究了如何在Craftax-classic环境中改进基于Transformer世界模型（TWM）的强化学习方法。研究人员主要从三个方面入手：如何使用TWM、如何将图像转换成TWM的输入以及如何训练TWM。 \
  研究团队的方法让智能体在仅用100万步环境交互的情况下，就取得了Craftax-classic 67.42%的奖励和 27.91%的得分，这比之前的最佳研究成果（SOTA）——53.20%的奖励和19.4%的得分——都有了显著提升。智能体的表现甚至超越了人类专家！相当炸裂。
* 12.小模型指导大模型！田渊栋等爆锤蒸馏：新方法更高效、更透明、更可控  新智元  https://mp.weixin.qq.com/s/V-zQgo-xc0aDBC4hHSkFaw \
  LLM Pretraining with Continuous Concepts \
  基于连续概念，Meta团队新研究提出了超越「下一个token预测」语言建模新范式。更加重要的是，新方法不仅能增强原有的范式，而且比起知识蒸馏，数据量减少20%，甚至能从小模型提取概念指导更大的模型！
* 13.AGI理论比较：主动推理、强化学习、控制论、贝叶斯大脑、效用决策、有限理性、情感动机、动态体内平衡  CreateAMind  https://mp.weixin.qq.com/s/egj-2woxVpdA8XuGcOVaeQ \
  Active-InferenceThe-Free-Energy-Principle-in-Mind \
  我们总结了主动推理的主要理论要点（来自本书的第一部分）及其实际实现（来自第二部分）。然后，我们将这些点联系起来：我们从前面章节中讨论的特定主动推理模型中抽象出来，专注于框架的集成方面。主动推理的好处之一是它为有感知力的生物体必须解决的适应性问题提供了完整的解决方案。因此，它为感知、行动选择、注意力和情绪调节等问题提供了统一的视角，这些问题通常在心理学和神经科学中被单独处理，并在人工智能中使用不同的计算方法来解决。我们将在控制论、行动思想运动理论、强化学习和最优控制等既定理论的背景下讨论这些问题（以及更多问题）。最后，我们简要讨论如何将主动推理的范围扩展到涵盖本书未深入讨论的其他生物、社会和技术主题 \
  大骂“深度学习是垃圾”的自由能到底是什么？有什么效果？  CreateAMind  https://mp.weixin.qq.com/s/Jjw1BA1ociiCbAxKmjvU6A -> 内含从机器人到AGI，从具身到可解释，从入门到应用实现的最全自由能原理资料
  一个框架整合大脑理论3 概要+公式图表  CreateAMind  https://mp.weixin.qq.com/s/KTzLYx0LVvIHks_KPP0zoA 
* 14.这几天！DeepSeek开源周 | 发布5个重要代码库，涉及AI基础设施建设的关键节点  AINLPer  https://mp.weixin.qq.com/s/XbYp7v2Ls9pw-11FQSZWOA \
  FlashMLA、DeepEP、DeepGEMM、并行策略优化、3FS文件系统、DS-V3/R1推理系统概述
* 15.可视化图解MOE大模型的7个核心问题：专家、路由、负载均衡及其用于视觉模态  老刘说NLP  https://mp.weixin.qq.com/s/-SFFB6gUp0KA4x95lCoxcg \
  原文：https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts \
  主要包括7个问题： \
  1、什么是专家混合模型（Mixture of Experts, MoE）？ \
  2、什么是专家？ \
  3、路由机制如何工作？ \
  4、如何进行负载均衡？ \
  5、如何通过Switch Transformer简化MoE？ \
  6、专家混合模型在视觉模型中如何应用？ \
  7、Mixtral 8x7B中的活跃参数与稀疏参数？
* 16.LLM「啊哈时刻」竟会自我纠正，单体数学性能暴涨！UIUC华人一作  新智元  https://mp.weixin.qq.com/s/nUdj4aVN2Xk1OJur6YDndA \
  该研究团队打造了一款「自我奖励推理模型」，让大模型 (LLM) 从生成推理路径到自我评估，再到纠正错误，全部一气呵成。 \
  Self-rewarding correction for mathematical reasoning \
  https://github.com/RLHFlow/Self-rewarding-reasoning-LLM
* 17.知识蒸馏技术原理详解：从软标签到模型压缩的实现机制  机器学习研究组订阅  https://mp.weixin.qq.com/s/I9aU6eHjhNCA77tc_usP5g 
* 18.Level-Navi Agent：开源AI 搜索智能体框架  大语言模型论文跟踪  https://mp.weixin.qq.com/s/lFqu0COKdvqKXwGwLMs8Ig \
  Level-Navi Agent: A Framework and benchmark for Chinese Web Search Agents \
  https://github.com/chuanruihu/Level-Navi-Agent-Search

# 3.3 Mon
* 19.AI 时代下，技术人如何保持核心竞争力？这位 MIT 毕业生给出了答案。“别再刷 Leetcode 了！”  图灵人工智能  https://mp.weixin.qq.com/s/mXulwIWWXxfpZ20jVzhy_A
* 20.历时6个月，Hugging Face开源LLM「超大规模实战手册」！200页3万字4000次训练  新智元  https://mp.weixin.qq.com/s/QhyCbaCxVXu_DYzMP5RMXw \
  https://huggingface.co/spaces/nanotron/ultrascale-playbook
* 21.​ICLR 2025 | 无需训练！大幅增强多模态大模型对微小视觉细节的感知  PaperWeekly  https://mp.weixin.qq.com/s/VCt-gITmCYp74Ed0IqjxAA \
  MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs \
  https://github.com/saccharomycetes/mllms_know
* 22.灵初智能发布端到端VLA模型Psi R0.5，仅需两小时数据实现物品、场景全面泛化  机器之心  https://mp.weixin.qq.com/s/55l129vnMl3ysoXRFBpp3w \
  DexGraspVLA
* 23.阿里PC-Agent重构人机交互，精准拆解跨应用指令，自动化办公更进一步  量子位  https://mp.weixin.qq.com/s/WrZ8Na4ze2pXb5hU4WKy3A \
  面向复杂PC任务的多模态智能体框架PC-Agent
* 24.系统科学是科学么？——从系统科学基本原理到大脑智能  集智俱乐部  https://mp.weixin.qq.com/s/clRBy6B7HNgyD6dOb_JXbg 
* 25.基于结构化状态空间对偶性的贝叶斯注意力机制设计与实现  机器学习研究组订阅  https://mp.weixin.qq.com/s/96xaNBv0hRsu1qXRAPKb5w \
  ?什么是贝叶斯注意力机制
* 26.多元推理刷新「人类的最后考试」记录，o3-mini(high)准确率最高飙升到37％  机器之心  https://mp.weixin.qq.com/s/ueMlKX-ps9pgia6YqTUgow \
  波士顿 | 提出多元推理方法，结合多种模型，o3-mini(high)准确率飙至到37％  AINLPer  https://mp.weixin.qq.com/s/f2onUBRF1xAczVZeuAo0tA \
  一种在测试时结合多种模型和方法的多元推理方法 \
  Diverse Inference and Verification for Advanced Reasoning
* 27.中科院、百度提出新架构：突破参数限制，实现高效推理  AIGC开放社区  https://mp.weixin.qq.com/s/e1LTA9ZdB8iSrO6Vil2DOA \
  Inner Thinking Transformer架构（简称ITT），通过动态分配计算资源给单个标记，增强了测试性能而无需增加参数 
* 28.什么是LLM后训练？《深入探讨推理大语言模型》综述  专知  https://mp.weixin.qq.com/s/hvteMOLFGDyOWH42N6DeUQ \
  LLM Post-Training: A Deep Dive into ReasoningLarge Language Models
* 29.全息压缩表征 Holographic Reduced Representations  CreateAMind  https://mp.weixin.qq.com/s/HKInUKbG4thPfqA5JGDudQ \
  Holographic Reduced Representations \
  联想记忆：自联想记忆（例如hopfiled霍普菲尔德网络）存储一组无序项目，可以通过其扭曲版本回忆项目。异联想记忆（例如全息记忆和矩阵记忆）存储项目对，可以使用其中一个项目作为提示来回忆另一个项目
* 30.视觉强化微调！DeepSeek R1技术成功迁移到多模态领域，全面开源  机器之心  https://mp.weixin.qq.com/s/VCSUQXV7yv9MdIWQlxh7dQ \
  视觉强化微调开源项目 —— Visual-RFT \
  Visual-RFT: Visual Reinforcement Fine-Tuning \
  https://github.com/Liuziyu77/Visual-RFT
* 31.为DeepSeek MoE模型带来「免费午餐」加速，专家链可大幅提升LLM的信息处理能力  机器之心  https://mp.weixin.qq.com/s/qTD96rcSY1cKNmH8B30V-w \
  专家链（CoE），在性能、扩展策略、资源效率和专家使用效率等多个方面都显著超越先前的 MoE 模型 \
  https://github.com/ZihanWang314/coe \
  中文报告：https://sandy-server-87f.notion.site/1ab9bb750b79801bbfebf01ae9a77b3f \
  英文报告：https://sandy-server-87f.notion.site/Chain-of-Experts-Unlocking-the-Communication-Power-of-MoEs-1ab9bb750b7980048d43e6aab3537cea \
  我们提出专家链 (Chain-of-Experts，CoE) 架构，一种通过在单层内实现专家间串行通信的创新方法，从根本上改变稀疏神经网络的信息处理方式。 \
  MoE 设计中存在专家间独立处理以及显存需求高的问题。与先前 MoE 独立处理每个 token 不同，CoE 引入迭代机制使专家能够 "沟通"，在其他专家的输出之上处理 token。
* 32.全面增强LLM推理/规划/执行力！北航提出全新「内置CoT」思考方法  新智元  https://mp.weixin.qq.com/s/LVwCjuOki2ocdCFwPQ0POw \
  https://github.com/HaunLeung/thinkandaction \
  LLM SHOULD THINK AND ACTION AS A HUMAN \
  这种跨越多轮的会话目前仍然存在一些问题：大语言模型的回答容易出错，不能帮助用户达到目标，且随着会话轮数增加出错概率会增大。 \
  为了解决这些问题，国内学者提出了一个基于内置思维链的思考方法：在多轮会话中，对于每一个用户提示，大语言模型基于会话历史，思考上下文，行动调用，记忆和知识等要素进行思考，进行详细的推理和计划，并根据计划进行行动。大语言模型按照这种思考方法产生的思维链是内置于响应里，由特殊词元包装起来，通称内置思维链。 \
  内置CoT与一般CoT相比有啥特点？ \
  思考方法产生的思考过程被封装在特殊词元<<think>>和<</think>>内，这通常称作内置思维链
* 33.大语言模型的解码策略与关键优化总结  机器学习研究组订阅  https://mp.weixin.qq.com/s/qHNIbcJ6_LVNBz42WXpvAw 
* 34.用极小模型复现R1思维链的失败感悟  吃果冻不吐果冻皮  https://mp.weixin.qq.com/s/aI6MkUJQrLccJfuuPvQjPg \
  探索0.5B模型在KK数据集上的强化学习训练 \
  模型太小，能力不行

# 3.4 Tue
* 35.(**重要**)意识的数学模型 （4万字）  CreateAMind  https://mp.weixin.qq.com/s/ae9ESPT-468MDYVR_SF-zQ \
  Mathematical Models of Consciousness \
  近年来，提出了一些有前景的数学模型，旨在描述意识体验及其与物理领域的关系。尽管这些理论的公理和形而上学观念得到了谨慎的论证，但它们的数学形式化尚未如此。在本文中，我们旨在弥补这一不足。我们阐述了对现象体验进行数学表征的合理性，推导出一个考虑意识认识论背景的一般数学框架，并研究意识体验的一些关键特征所暗示的数学结构，明确指出数学方法在何种程度上能够超越传统方法所能达到的范围。其结果是一个可用于理论构建过程的意识模型的一般数学框架。
* 36.本地部署DeepSeek R1 + Ollama + XRAG：三步搭建RAG系统，并解锁全流自动化评测  AINLPer  https://mp.weixin.qq.com/s/8Zg79SX59DhWxv6JCu7phw \
  本文提供了一个详细操作指南，帮助用户使用Ollama本地部署最新的DeepSeek R1模型，并使用最新的XRAG1.0框架来构建RAG系统并评估你的本地RAG知识库系统。
* 37.全球首次！2B复现DeepSeek-R1「啊哈时刻」，UCLA等用纯RL实现多模态推理  新智元  https://mp.weixin.qq.com/s/7jGwTQKFHZ_4_UeiY_9ULQ \
  博客地址：https://turningpointai.notion.site/the-multimodal-aha-moment-on-2b-model \
  开源项目：https://github.com/turningpoint-ai/VisualThinker-R1-Zero
* 38.多智能体协作机制：大语言模型综述  专知  https://mp.weixin.qq.com/s/XXqvMwKR1ghsc1wqcnQrig \
  Multi-Agent Collaboration Mechanisms: A Survey of LLMs 
* 39.无编码器架构潜力或被低估，首个无编码器3D多模态LLM大模型来了  PaperWeekly  https://mp.weixin.qq.com/s/QR9I6-1TfmtXJvdjsL1dTw \
  Exploring the Potential of Encoder-free Architectures in 3D LMMs \
  首个无编码器架构的 3D LMM—ENEL，其 7B 模型与当前最先进的 ShapeLLM-13B 相媲美，表明无编码器架构的巨大潜力
* 40.上海AI Lab最新推出Mixture-of-Memories：线性注意力也有稀疏记忆了  机器之心  https://mp.weixin.qq.com/s/GRX2HDepqKCN0h1cGO0lDA \
  MoM: Linear Sequence Modeling with Mixture-of-Memories \
  代码地址：https://github.com/OpenSparseLLMs/MoM \
  未来还会集成在：https://github.com/OpenSparseLLMs/Linear-MoE \
  模型权重开源在：https://huggingface.co/linear-moe-hub \
  MoM: Mixture-of-Memories让我们从目前主流线性序列建模方法改 gate 和 RNN 更新规则的套路中跳脱出来，稀疏且无限制地扩大 memory 大小。\
  ???什么是稀疏记忆，和DeepSeek NSA是什么关系？
* 41.为什么Qwen能自我改进推理，Llama却不行？斯坦福找到了原理  机器之心  https://mp.weixin.qq.com/s/OvS61OrDp6rB-R5ELg48Aw \
  Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs \
  AI 模型要想在有更多时间思考时真正变得更聪明，必须先具备一些基本的思考能力（比如检查错误、验证结果的习惯）。如果模型一开始就不会这些基本思考方法，即使给它再多的思考时间和计算资源，它也无法有效利用这些资源来提高自己的表现 \
  这项研究揭示了模型的初始推理行为与其自我改进能力之间存在紧密联系。这种联系有助于解释为什么有些语言模型能够找到有效利用额外计算资源的方法，而另一些模型则停滞不前
* 42.大模型推理新范式！清华&蚂蚁：用编程思维来思考，用自然语言来表达  机器之心  https://mp.weixin.qq.com/s/DOjrAPLDgu7zEWyOXJiuxQ \
  提出了CodePlan，这一创新框架将「代码形式的规划」（Code-Form Planning）引入推理过程，让大模型先用「编程思维」来思考，再用自然语言来表达 \
  CodePlan: Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning \
  CodePlan 的提出为大模型推理能力发展提供了一个新思路。这项创新通过将代码形式规划引入推理过程，成功解决了自然语言表达中的结构化缺陷；更重要的是，它开创了一种全新的方法论，为大模型注入了系统化的问题解决能力。

# 3.5 Wed
* 43.360智脑开源Light-R1！1000美元数学上首次从零超越DeepSeek-R1-Distill  机器之心  https://mp.weixin.qq.com/s/5VPU0C8EK8jxdtm7OJS5lA \
  仅需 12 台 H800 上 6 小时即可训练完成，从没有长思维链的 Qwen2.5-32B-Instruct 出发，仅使用 7 万条数学数据训练，得到 Light-R1-32B \
  模型仓库：https://huggingface.co/qihoo360/Light-R1-32B \
  项目地址：https://github.com/Qihoo360/Light-R1
* 44.阿里推理模型一战封神！32B硬刚671B DeepSeek，1/10成本，苹果笔记本可跑  量子位  https://mp.weixin.qq.com/s/ZtnUV0RLf6_CR04Sbm-Wyw \
  阿里半夜开源全新推理模型，QwQ-32B比肩DeepSeek-R1满血版  机器之心  https://mp.weixin.qq.com/s/oejAfPpireHKUzqTOSPyXw \
  Qwen发布最新32B推理模型，跑分不输671B的满血版DeepSeek R1 \
  QwQ 32B是一个密集模型，没有用到MoE，上下文长度有131k \
  博客：https://qwenlm.github.io/zh/blog/qwq-32b/ 「QwQ-32B: 领略强化学习之力」 \
  Hugging Face：https://huggingface.co/Qwen/QwQ-32B \
  ModelScope：https://modelscope.cn/models/Qwen/QwQ-32B \
  演示：https://huggingface.co/spaces/Qwen/QwQ-32B-Demo \
  Qwen Chat：https://chat.qwen.ai/ \
  本地部署工具 Ollama 也第一时间提供了支持：ollama run qwq \
  QwQ-32B 中还集成了与 Agent（智能体）相关的能力，使其能够在使用工具的同时进行批判性思考，并根据环境反馈调整推理过程。该团队表示：「我们希望我们的一点努力能够证明强大的基础模型叠加大规模强化学习也许是一条通往通用人工智能的可行之路。」
* 45.开启空间智能问答新时代：Spatial-RAG框架来了  机器之心  https://mp.weixin.qq.com/s/IXvhr0rhxzgJ1dOOBbTN7g \
  Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Spatial Reasoning Questions \
  Spatial-RAG 通过结合空间数据库和 LLM 的语义理解能力，显著提升了空间推理任务的性能。实验表明，Spatial-RAG 在真实世界数据集上表现优异，能够有效处理复杂的空间推理问题，为旅游推荐、路径规划等应用提供了强大的支持。
* 46.当持续学习遇上多模态大型语言模型：综述  专知  https://mp.weixin.qq.com/s/WGib2MiIB6fD45zA5Of-2A \
  When Continue Learning Meets Multimoda Large Language Model: A Survey \
  ？当前有啥解决“灾难性遗忘”问题的办法
* 47.AI话痨终结者！UCSD清华提出「思维扫描术」Dynasor-CoT，推理巨省token  新智元  https://mp.weixin.qq.com/s/HVnWueo_9yWzIyS_8aJWOg \
  Dynasor: More Efficient Chain-of-Thought Through Certainty Probing \
  以一个简单问题 (1+2i)*6-3i测试为例：传统Qwen-7B用180个token轻松解题，而升级后的Deepseek版Qwen-7B虽在第340个token就得出正确答案，却硬生生续写1000+token反复验证！这种「学霸强迫症」，让DeepSeek等顶尖模型浪费了高达70%的算力！ \
  为了解决模型的「自我怀疑」问题，研究团队提出了Dynasor-CoT，一种无需训练、侵入性最小且简单的方法，用于长链式推理（CoT）。 \
  团队已将这款「AI话痨终结者」系统全面开源 \
  如果AI连续N次的「CT扫描」结果都显示同一个答案，系统就会判定AI非常自信，并果断按下停止键。坚定地告诉这位同学：「你已经答对了，不用再证明了！」 \
  通过答案一致性进行确定性评估 \
  https://hao-ai-lab.github.io/blogs/ \
  https://github.com/hao-ai-lab/Dynasor \
  https://hao-ai-lab.github.io/demo/dynasor-cot
* 48.智源开源多模态向量模型BGE-VL：多模态检索新突破  机器之心  https://mp.weixin.qq.com/s/iw9BmSDwv6NYtD7pkC5kxQ \
  多模态向量模型 BGE-VL，在图文检索、组合图像检索等主要多模态检索任务中均取得了最佳效果 \
  项目主页：https://github.com/VectorSpaceLab/MegaPairs \
  模型地址：https://huggingface.co/BAAI/BGE-VL-MLLM-S1 
* 49.苹果最强M3 Ultra首发逆天，512GB「桌面超算」在家跑DeepSeek-R1！  机器学习研究组订阅  https://mp.weixin.qq.com/s/cLXEWVb_GwvwzzP0WEH1zg 
* 50.DeepSeek的MLA，任意大模型都能轻松迁移了  机器之心  https://mp.weixin.qq.com/s/UvvHdHb8eeg4qHdkGejxhA \
  Towards Economical Inference: Enabling  DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs \
  https://github.com/JT-Ushio/MHA2MLA \
  多头潜在注意力网络（Multi-head Latent Attention, MLA）是其经济推理架构的核心之一，通过对键值缓存进行低秩压缩，显著降低推理成本 \
  复旦 NLP 实验室、华东师大、上海 AI Lab、海康威视联合提出 MHA2MLA 框架，通过部分 RoPE 保留（Partial-RoPE）和键值联合表示低秩近似（Low-rank Approximation）两个关键步骤，成功将任意 MHA/GQA 架构迁移到 MLA。
* 51.从自我进化视角出发，全面解析LLM的推理能力技术演进路径  机器之心  https://mp.weixin.qq.com/s/hkYW0c26eLEHE9WgIQxwHw \
  A Survey on LLM Complex Reasoning through the Lens of Self-Evolution \
  仓库链接：https://github.com/cs-holder/Reasoning-Self-Evolution-Survey \
  本文系统地综述了 LLM 复杂推理的自我进化方法，从**数据进化**、**模型进化**和**自我进化**三个角度进行了深入分析

# 3.6 Thur

# 3.7 Fri
* 52.Manus爆火，一大批open manus项目如雨后春笋般涌现  探索AGI  https://mp.weixin.qq.com/s/we27Kq-RAHosCRkLxyWm4Q \
  特工宇宙 OpenManus \
  OWL项目 \
  manus教程项目 \
  OpenManus/OpenManus \
  claude computer use demo 
* 53.QwQ-32B 测评和使用教程来了！  Datawhale  https://mp.weixin.qq.com/s/Qj3vlhFrhYzILQCH50707g \
  一张 4090，本地部署了一个 QwQ-32B-AWQ 量化版本 \
  教程地址：https://datawhaler.feishu.cn/docx/Wz2NdqSx1oEZsuxB9zHcEQ20nNe
* 54.FP8训练新范式：减少40%显存占用，训练速度提高1.4倍  机器之心  https://mp.weixin.qq.com/s/na86HSj92MCyvQTzVK_H5g 
* 55.OWL：0天复刻Manus通用智能体，完全开源！GAIA Benchmark最强性能！  Datawhale  https://mp.weixin.qq.com/s/E-HWNzjZdw_0PRHjvlO8bw \
  CAMEL-AI的OWL
  GitHub：https://github.com/camel-ai/owl
* 56.认知单元实现  CreateAMind  https://mp.weixin.qq.com/s/WGrNRXjLqGMEL8QD6HLnKQ \
  Visual motion perception as online hierarchical inference \
  识别环境中运动关系的结构对于导航、追踪、预测和追逐至关重要。然而，关于视觉系统如何从不稳定的视觉信息流中在线推断这种结构的心理和神经计算过程，我们知之甚少。我们提出在线层次贝叶斯推断作为一种合理的方法，解释大脑如何解决这一复杂的知觉任务。
* 57.微软GUI智能体OmniParser二代开源！推理延迟降低60%，大模型玩手机更溜了  新智元  https://mp.weixin.qq.com/s/snWwF9mL9C-kKYr_h0nHxw \
  OmniParser V2可将屏幕截图转换为结构化元素，帮助LLM理解和操作GUI；在检测小图标和推理速度上显著提升，延迟降低60%，与多种LLM结合后表现优异。 \
  代码: https://github.com/microsoft/OmniParser/tree/master \
  模型: https://huggingface.co/microsoft/OmniParser-v2.0 \
  Demo：https://huggingface.co/spaces/microsoft/OmniParser-v2OmniParser方法概述
* 58.32B击败DeepSeek-R1、o3-mini，成本暴降100倍！GRPO让小模型称霸推理  新智元  https://mp.weixin.qq.com/s/YdKDyrVpBZthg4fD29N8Yg \
  在具有挑战性的「时间线索」（Temporal Clue）逻辑谜题中，基于强化学习(**GRPO**)微调后的Qwen 2.5 32B，推理能力完全碾压o1、o3-mini、R1。 \
  https://openpipe.ai/blog/using-grpo-to-beat-o1-o3-mini-and-r1-on-temporal-clue \
  **GRPO让小模型称霸推理**
* 59.美团开源首发INT8无损满血版DeepSeek R1  AGI之美  https://mp.weixin.qq.com/s/yofbSYhHy_KXfRA9hWFF2Q \
  https://huggingface.co/meituan/DeepSeek-R1-Block-INT8   \
  https://huggingface.co/meituan/DeepSeek-R1-Channel-INT8

# 3.8 Sat
* 60.模拟真实世界：多模态生成模型的统一综述  专知  https://mp.weixin.qq.com/s/hrmMhFF4i9MBBvLnehd51w \
  Simulating the Real World: A Unified Survey of Multimodal Generative Models
* 61.7B级形式化推理与验证小模型，媲美满血版DeepSeek-R1，全面开源！  机器之心  https://mp.weixin.qq.com/s/oyhICTRo2fJL5MZkDrutXg \
  近日，由香港科技大学牵头，联合中科院软件所、西安电子科技大学、重庆大学等单位，开源了一系列形式化推理与验证大模型，仅用 7B，即可在相关任务上获得与 671B 满血版 DeepSeek-R1 相当的水平！ \
  From Informal to Formal–Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs \
  Hugging Face 模型链接：https://huggingface.co/fm-universe 
* 62.世界首个人类脑细胞计算机诞生！能编程还能活体计算，**售价3万5美金**  新智元  https://mp.weixin.qq.com/s/dtLbWoK48sHE-_jGmFv45g \
  Cortical lab的CL1 \
  在CL1的硅芯片表面，有实验室培育的人类神经元。这些神经元能够响应电信号，形成与生物大脑类似的信息处理网络。这个系统被设计为允许双向通信，其中电脉冲刺激神经元，并记录和分析它们的反应。为了维持神经元的活力，CL1配备了一个生命维持系统，该系统调节温度、气体交换和其他必要条件。CL1不是一台普通的计算机——它没有 GPU、CPU或RAM，而是依靠真实的神经元形成动态连接，自适应调整，专为需要真实学习能力的生物计算任务设计。
* 63.
* 64.
* 65.
* 66.
* 67.
* 68.
* 69.
* 70.

# 3.9 Sun
# 3.10 Mon
# 3.11 Tue
# 3.12 Wed
# 3.13 Thur
# 3.14 Fri
# 3.15 Sat
# 3.16 Sun
# 3.17 Mon
# 3.18 Tue
# 3.19 Wed
# 3.20 Thur
# 3.21 Fri
# 3.22 Sat
# 3.23 Sun
# 3.24 Mon
# 3.25 Tue
# 3.26 Wed
# 3.27 Thur
# 3.28 Fri
# 3.29 Sat
# 3.30 Sun
# 3.31 Mon
