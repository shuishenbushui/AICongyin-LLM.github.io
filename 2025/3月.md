# 3.1 Sat
* 1.多模态基础模型的机制可解释性综述  专知  https://mp.weixin.qq.com/s/gTLYisN09Omty1HD0OTMGA \
  A Survey on Mechanistic Interpretability for Muti-Modal Foundation Models \
  本综述探索了两个关键方面：（1）将LLM可解释性方法适应到多模态模型；（2）理解单模态语言模型与跨模态系统之间的机制差异。
* 2.大模型是否有自知之明？新研究发现LLM可以知晓自己的知识范围  机器之心  https://mp.weixin.qq.com/s/_SGEw75r6SjcB5JUTg9ENw \
  Do Large Language Models Know How Much They Know? \
  「虽然不同架构涌现这种能力的速率不同，但结果表明，知识意识（awareness of knowledge）可能是 LLM 的一个普遍属性。」 \
  整体来说，这项研究证明了足够规模的 LLM 确实具有知识意识（awareness of knowledge），即能够知晓自己的知识范围。
* 3.ICLR 2025｜AI不语，只是一味根据人类意图推理3D空间定位  机器之心  https://mp.weixin.qq.com/s/DXExxwZ7t6lzdfoKa3kJYQ \
  Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention \
   3D 意图定位（3D-IG）
* 4.刚刚，LangGraph官方开源Ultra版本，多智能体能力提升10倍！  探索AGI  https://mp.weixin.qq.com/s/XRoyD5O9o1CqISFkIDeBBA
* 5.Agent or SFT or RL ? 9个多模态R1推理开源项目核心思路解析  老刘说NLP  https://mp.weixin.qq.com/s/yIDqvJLASPWX6gkOoRr1Pg 
* 6.DeepSeek-V3 / R1 推理系统概览  吃果冻不吐果冻皮  https://mp.weixin.qq.com/s/HBuIB1yoXyYKVwqqgt0SFQ
* 7.一个统一的视角理解：计算力学、因果抽象和信息分解  集智俱乐部  https://mp.weixin.qq.com/s/LW3Rvs6JIRFlwTMamZiwLg
  在众多研究涌现的框架里，计算力学的框架相比较而言很宏大，而且讲起来有特别的吸引力。它依托于生物进化论的思想，以信息论为工具，同时有硬核的统计物理解释以及严谨的数学表达。如下图所示，它假设宇宙是一个巨大的确定动力系统，每一个生物体或者说主体，都因其有限的观察能力，只能感知到宇宙的一部分，也就是环境。环境中就有噪音，而这很可能威胁到主体的生存。所以迫于进化的压力，每一个主体都要尽可能有能力预测到环境的变化。对于每一个主体的构造而言，除去那些实际的物理和化学结构以外，计算力学关注的是它的“虚拟层”。主体根据传感器获知环境的历史信息，再根据自己的内在模型做出预测，然后做出相应的行动。这个内在模型的概念，和现在流行的**自由能原理**与**世界模型**理论也是相一致的。 \
  光能做出准确的预测还不够。比如现在的大模型预测能力很强，那是不是说让每一个主体都背着一个大模型就万事大吉了？这太笨重了。如果对环境建模的内在模型非常复杂，就会消耗很多资源，也不利于主体的生存。所以大自然要求主体既要能预测环境变化，又要这个预测模型尽可能简洁。这个既要又要，使得主体不断地动态调整自己的内在模型，总要在一个最合适的尺度上归纳总结周围的环境变化。而这，便是主体为什么进化出识别涌现现象的能力，我们人类也不例外。

# 3.2 Sun
* 8.LeCun世界模型再近一步！Meta研究证明：AI可无先验理解直觉物理  新智元  https://mp.weixin.qq.com/s/OeUYyfEonlKlwQQEwhLVgg \
  V-JEPA不是去生成像素级的精准预测，而是在抽象的表示空间里进行预测。这种方式更接近LeCun所认为的人类大脑处理信息的模式。\
  Intuitive physics understanding emerges from self-supervised pretraining on natural videos \
  这次的主要发现如下： \
  1.V-JEPA能够准确且一致地分辨出，符合物理定律的视频和违反物理定律的视频，远超多模态LLM和像素空间中的视频预测方法。 \
  2.虽然在实验中观察到改变模型的任一组件，都会影响性能，但所有V-JEPA模型都取得了明显高于随机水平的表现。
* 9.【AAAI2025教程】大型语言模型中的知识生命周期：记忆、编辑与超越，216页ppt  专知  https://mp.weixin.qq.com/s/fF4fHQ9tSCs3XgVjJ-ywCQ \
  The Lifecycle of Knowledge in Large Language Models: Memorization, Editing, and Beyond \
  https://llmknowledgelifecycle.github.io/AAAI2025_Tutorial_LLMKnowledge \
  本教程探讨大型语言模型中的知识生命周期，包括：(1) 通过基础模型预训练知识的涌现；(2) 外部知识的注入；(3) 知识的更新与修改；(4) 知识的探测与生成；(5) 多模态知识的整合，包括对物理世界的理解和程序性规划。
* 10.DeepSeek关键RL算法GRPO，有人从头跑通了，贡献完整代码  机器之心  https://mp.weixin.qq.com/s/e0-tVsaIgNajBTOl117ctg \
  DeepSeek关键RL算法GRPO，手把手教你从头跑通！  Datawhale  https://mp.weixin.qq.com/s/gi8ee4m6borLPlPRBcp7Mg \
  GRPO 算法丢弃了 critic model，放弃了价值函数近似，转而通过组内样本的相对比较来计算策略梯度，从而有效降低了训练的不稳定性，同时提高了学习效率。 \
  教程地址：https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb
* 11.超越人类！DeepMind强化学习新突破：AI在「我的世界」中封神！  新智元  https://mp.weixin.qq.com/s/RTEcC56XLXLqMtov3Og1mQ \
  Improving Transformer World Models for Data-Efficient RL \
  Crafter是一个2D版的《我的世界》，具体来说，他们用的是Craftax-classic环境，它是Crafter的快速复刻版。Craftax-classic环境有几个很好的特点：1.每次游戏的环境都是随机生成的，AI需要应对不同的挑战。2.AI只能看到局部视野，就好像只能看到屏幕的一部分，而不是整个地图。3.这是一个以成就层级来设定奖励信号的体系，需要进行深入且广泛的探索才能达成。 \
  DeepMind研究团队的这篇论文主要研究了如何在Craftax-classic环境中改进基于Transformer世界模型（TWM）的强化学习方法。研究人员主要从三个方面入手：如何使用TWM、如何将图像转换成TWM的输入以及如何训练TWM。 \
  研究团队的方法让智能体在仅用100万步环境交互的情况下，就取得了Craftax-classic 67.42%的奖励和 27.91%的得分，这比之前的最佳研究成果（SOTA）——53.20%的奖励和19.4%的得分——都有了显著提升。智能体的表现甚至超越了人类专家！相当炸裂。
* 12.小模型指导大模型！田渊栋等爆锤蒸馏：新方法更高效、更透明、更可控  新智元  https://mp.weixin.qq.com/s/V-zQgo-xc0aDBC4hHSkFaw \
  LLM Pretraining with Continuous Concepts \
  基于连续概念，Meta团队新研究提出了超越「下一个token预测」语言建模新范式。更加重要的是，新方法不仅能增强原有的范式，而且比起知识蒸馏，数据量减少20%，甚至能从小模型提取概念指导更大的模型！
* 13.AGI理论比较：主动推理、强化学习、控制论、贝叶斯大脑、效用决策、有限理性、情感动机、动态体内平衡  CreateAMind  https://mp.weixin.qq.com/s/egj-2woxVpdA8XuGcOVaeQ \
  Active-InferenceThe-Free-Energy-Principle-in-Mind \
  我们总结了主动推理的主要理论要点（来自本书的第一部分）及其实际实现（来自第二部分）。然后，我们将这些点联系起来：我们从前面章节中讨论的特定主动推理模型中抽象出来，专注于框架的集成方面。主动推理的好处之一是它为有感知力的生物体必须解决的适应性问题提供了完整的解决方案。因此，它为感知、行动选择、注意力和情绪调节等问题提供了统一的视角，这些问题通常在心理学和神经科学中被单独处理，并在人工智能中使用不同的计算方法来解决。我们将在控制论、行动思想运动理论、强化学习和最优控制等既定理论的背景下讨论这些问题（以及更多问题）。最后，我们简要讨论如何将主动推理的范围扩展到涵盖本书未深入讨论的其他生物、社会和技术主题 \
  大骂“深度学习是垃圾”的自由能到底是什么？有什么效果？  CreateAMind  https://mp.weixin.qq.com/s/Jjw1BA1ociiCbAxKmjvU6A -> 内含从机器人到AGI，从具身到可解释，从入门到应用实现的最全自由能原理资料
  一个框架整合大脑理论3 概要+公式图表  CreateAMind  https://mp.weixin.qq.com/s/KTzLYx0LVvIHks_KPP0zoA 
* 14.这几天！DeepSeek开源周 | 发布5个重要代码库，涉及AI基础设施建设的关键节点  AINLPer  https://mp.weixin.qq.com/s/XbYp7v2Ls9pw-11FQSZWOA \
  FlashMLA、DeepEP、DeepGEMM、并行策略优化、3FS文件系统、DS-V3/R1推理系统概述
* 15.可视化图解MOE大模型的7个核心问题：专家、路由、负载均衡及其用于视觉模态  老刘说NLP  https://mp.weixin.qq.com/s/-SFFB6gUp0KA4x95lCoxcg \
  原文：https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts \
  主要包括7个问题： \
  1、什么是专家混合模型（Mixture of Experts, MoE）？ \
  2、什么是专家？ \
  3、路由机制如何工作？ \
  4、如何进行负载均衡？ \
  5、如何通过Switch Transformer简化MoE？ \
  6、专家混合模型在视觉模型中如何应用？ \
  7、Mixtral 8x7B中的活跃参数与稀疏参数？
* 16.LLM「啊哈时刻」竟会自我纠正，单体数学性能暴涨！UIUC华人一作  新智元  https://mp.weixin.qq.com/s/nUdj4aVN2Xk1OJur6YDndA \
  该研究团队打造了一款「自我奖励推理模型」，让大模型 (LLM) 从生成推理路径到自我评估，再到纠正错误，全部一气呵成。 \
  Self-rewarding correction for mathematical reasoning \
  https://github.com/RLHFlow/Self-rewarding-reasoning-LLM
* 17.知识蒸馏技术原理详解：从软标签到模型压缩的实现机制  机器学习研究组订阅  https://mp.weixin.qq.com/s/I9aU6eHjhNCA77tc_usP5g 
* 18.Level-Navi Agent：开源AI 搜索智能体框架  大语言模型论文跟踪  https://mp.weixin.qq.com/s/lFqu0COKdvqKXwGwLMs8Ig \
  Level-Navi Agent: A Framework and benchmark for Chinese Web Search Agents \
  https://github.com/chuanruihu/Level-Navi-Agent-Search

# 3.3 Mon
* 19.AI 时代下，技术人如何保持核心竞争力？这位 MIT 毕业生给出了答案。“别再刷 Leetcode 了！”  图灵人工智能  https://mp.weixin.qq.com/s/mXulwIWWXxfpZ20jVzhy_A
* 20.历时6个月，Hugging Face开源LLM「超大规模实战手册」！200页3万字4000次训练  新智元  https://mp.weixin.qq.com/s/QhyCbaCxVXu_DYzMP5RMXw \
  https://huggingface.co/spaces/nanotron/ultrascale-playbook
* 21.​ICLR 2025 | 无需训练！大幅增强多模态大模型对微小视觉细节的感知  PaperWeekly  https://mp.weixin.qq.com/s/VCt-gITmCYp74Ed0IqjxAA \
  MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs \
  https://github.com/saccharomycetes/mllms_know
* 22.灵初智能发布端到端VLA模型Psi R0.5，仅需两小时数据实现物品、场景全面泛化  机器之心  https://mp.weixin.qq.com/s/55l129vnMl3ysoXRFBpp3w \
  DexGraspVLA
* 23.阿里PC-Agent重构人机交互，精准拆解跨应用指令，自动化办公更进一步  量子位  https://mp.weixin.qq.com/s/WrZ8Na4ze2pXb5hU4WKy3A \
  面向复杂PC任务的多模态智能体框架PC-Agent
* 24.系统科学是科学么？——从系统科学基本原理到大脑智能  集智俱乐部  https://mp.weixin.qq.com/s/clRBy6B7HNgyD6dOb_JXbg 
* 25.基于结构化状态空间对偶性的贝叶斯注意力机制设计与实现  机器学习研究组订阅  https://mp.weixin.qq.com/s/96xaNBv0hRsu1qXRAPKb5w \
  ?什么是贝叶斯注意力机制
* 26.多元推理刷新「人类的最后考试」记录，o3-mini(high)准确率最高飙升到37％  机器之心  https://mp.weixin.qq.com/s/ueMlKX-ps9pgia6YqTUgow \
  波士顿 | 提出多元推理方法，结合多种模型，o3-mini(high)准确率飙至到37％  AINLPer  https://mp.weixin.qq.com/s/f2onUBRF1xAczVZeuAo0tA \
  一种在测试时结合多种模型和方法的多元推理方法 \
  Diverse Inference and Verification for Advanced Reasoning
* 27.中科院、百度提出新架构：突破参数限制，实现高效推理  AIGC开放社区  https://mp.weixin.qq.com/s/e1LTA9ZdB8iSrO6Vil2DOA \
  Inner Thinking Transformer架构（简称ITT），通过动态分配计算资源给单个标记，增强了测试性能而无需增加参数 
* 28.什么是LLM后训练？《深入探讨推理大语言模型》综述  专知  https://mp.weixin.qq.com/s/hvteMOLFGDyOWH42N6DeUQ \
  LLM Post-Training: A Deep Dive into ReasoningLarge Language Models
* 29.全息压缩表征 Holographic Reduced Representations  CreateAMind  https://mp.weixin.qq.com/s/HKInUKbG4thPfqA5JGDudQ \
  Holographic Reduced Representations \
  联想记忆：自联想记忆（例如hopfiled霍普菲尔德网络）存储一组无序项目，可以通过其扭曲版本回忆项目。异联想记忆（例如全息记忆和矩阵记忆）存储项目对，可以使用其中一个项目作为提示来回忆另一个项目
* 30.视觉强化微调！DeepSeek R1技术成功迁移到多模态领域，全面开源  机器之心  https://mp.weixin.qq.com/s/VCSUQXV7yv9MdIWQlxh7dQ \
  视觉强化微调开源项目 —— Visual-RFT \
  Visual-RFT: Visual Reinforcement Fine-Tuning \
  https://github.com/Liuziyu77/Visual-RFT
* 31.为DeepSeek MoE模型带来「免费午餐」加速，专家链可大幅提升LLM的信息处理能力  机器之心  https://mp.weixin.qq.com/s/qTD96rcSY1cKNmH8B30V-w \
  专家链（CoE），在性能、扩展策略、资源效率和专家使用效率等多个方面都显著超越先前的 MoE 模型 \
  https://github.com/ZihanWang314/coe \
  中文报告：https://sandy-server-87f.notion.site/1ab9bb750b79801bbfebf01ae9a77b3f \
  英文报告：https://sandy-server-87f.notion.site/Chain-of-Experts-Unlocking-the-Communication-Power-of-MoEs-1ab9bb750b7980048d43e6aab3537cea \
  我们提出专家链 (Chain-of-Experts，CoE) 架构，一种通过在单层内实现专家间串行通信的创新方法，从根本上改变稀疏神经网络的信息处理方式。 \
  MoE 设计中存在专家间独立处理以及显存需求高的问题。与先前 MoE 独立处理每个 token 不同，CoE 引入迭代机制使专家能够 "沟通"，在其他专家的输出之上处理 token。
* 32.全面增强LLM推理/规划/执行力！北航提出全新「内置CoT」思考方法  新智元  https://mp.weixin.qq.com/s/LVwCjuOki2ocdCFwPQ0POw \
  https://github.com/HaunLeung/thinkandaction \
  LLM SHOULD THINK AND ACTION AS A HUMAN \
  这种跨越多轮的会话目前仍然存在一些问题：大语言模型的回答容易出错，不能帮助用户达到目标，且随着会话轮数增加出错概率会增大。 \
  为了解决这些问题，国内学者提出了一个基于内置思维链的思考方法：在多轮会话中，对于每一个用户提示，大语言模型基于会话历史，思考上下文，行动调用，记忆和知识等要素进行思考，进行详细的推理和计划，并根据计划进行行动。大语言模型按照这种思考方法产生的思维链是内置于响应里，由特殊词元包装起来，通称内置思维链。 \
  内置CoT与一般CoT相比有啥特点？ \
  思考方法产生的思考过程被封装在特殊词元<<think>>和<</think>>内，这通常称作内置思维链
* 33.大语言模型的解码策略与关键优化总结  机器学习研究组订阅  https://mp.weixin.qq.com/s/qHNIbcJ6_LVNBz42WXpvAw 
* 34.用极小模型复现R1思维链的失败感悟  吃果冻不吐果冻皮  https://mp.weixin.qq.com/s/aI6MkUJQrLccJfuuPvQjPg \
  探索0.5B模型在KK数据集上的强化学习训练 \
  模型太小，能力不行

# 3.4 Tue
* 35.(**重要**)意识的数学模型 （4万字）  CreateAMind  https://mp.weixin.qq.com/s/ae9ESPT-468MDYVR_SF-zQ \
  Mathematical Models of Consciousness \
  近年来，提出了一些有前景的数学模型，旨在描述意识体验及其与物理领域的关系。尽管这些理论的公理和形而上学观念得到了谨慎的论证，但它们的数学形式化尚未如此。在本文中，我们旨在弥补这一不足。我们阐述了对现象体验进行数学表征的合理性，推导出一个考虑意识认识论背景的一般数学框架，并研究意识体验的一些关键特征所暗示的数学结构，明确指出数学方法在何种程度上能够超越传统方法所能达到的范围。其结果是一个可用于理论构建过程的意识模型的一般数学框架。
* 36.本地部署DeepSeek R1 + Ollama + XRAG：三步搭建RAG系统，并解锁全流自动化评测  AINLPer  https://mp.weixin.qq.com/s/8Zg79SX59DhWxv6JCu7phw \
  本文提供了一个详细操作指南，帮助用户使用Ollama本地部署最新的DeepSeek R1模型，并使用最新的XRAG1.0框架来构建RAG系统并评估你的本地RAG知识库系统。
* 37.全球首次！2B复现DeepSeek-R1「啊哈时刻」，UCLA等用纯RL实现多模态推理  新智元  https://mp.weixin.qq.com/s/7jGwTQKFHZ_4_UeiY_9ULQ \
  博客地址：https://turningpointai.notion.site/the-multimodal-aha-moment-on-2b-model \
  开源项目：https://github.com/turningpoint-ai/VisualThinker-R1-Zero
* 38.多智能体协作机制：大语言模型综述  专知  https://mp.weixin.qq.com/s/XXqvMwKR1ghsc1wqcnQrig \
  Multi-Agent Collaboration Mechanisms: A Survey of LLMs 
* 39.无编码器架构潜力或被低估，首个无编码器3D多模态LLM大模型来了  PaperWeekly  https://mp.weixin.qq.com/s/QR9I6-1TfmtXJvdjsL1dTw \
  Exploring the Potential of Encoder-free Architectures in 3D LMMs \
  首个无编码器架构的 3D LMM—ENEL，其 7B 模型与当前最先进的 ShapeLLM-13B 相媲美，表明无编码器架构的巨大潜力
* 40.上海AI Lab最新推出Mixture-of-Memories：线性注意力也有稀疏记忆了  机器之心  https://mp.weixin.qq.com/s/GRX2HDepqKCN0h1cGO0lDA \
  MoM: Linear Sequence Modeling with Mixture-of-Memories \
  代码地址：https://github.com/OpenSparseLLMs/MoM \
  未来还会集成在：https://github.com/OpenSparseLLMs/Linear-MoE \
  模型权重开源在：https://huggingface.co/linear-moe-hub \
  MoM: Mixture-of-Memories让我们从目前主流线性序列建模方法改 gate 和 RNN 更新规则的套路中跳脱出来，稀疏且无限制地扩大 memory 大小。\
  ???什么是稀疏记忆，和DeepSeek NSA是什么关系？
* 41.为什么Qwen能自我改进推理，Llama却不行？斯坦福找到了原理  机器之心  https://mp.weixin.qq.com/s/OvS61OrDp6rB-R5ELg48Aw \
  Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs \
  AI 模型要想在有更多时间思考时真正变得更聪明，必须先具备一些基本的思考能力（比如检查错误、验证结果的习惯）。如果模型一开始就不会这些基本思考方法，即使给它再多的思考时间和计算资源，它也无法有效利用这些资源来提高自己的表现 \
  这项研究揭示了模型的初始推理行为与其自我改进能力之间存在紧密联系。这种联系有助于解释为什么有些语言模型能够找到有效利用额外计算资源的方法，而另一些模型则停滞不前
* 42.大模型推理新范式！清华&蚂蚁：用编程思维来思考，用自然语言来表达  机器之心  https://mp.weixin.qq.com/s/DOjrAPLDgu7zEWyOXJiuxQ \
  提出了CodePlan，这一创新框架将「代码形式的规划」（Code-Form Planning）引入推理过程，让大模型先用「编程思维」来思考，再用自然语言来表达 \
  CodePlan: Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning \
  CodePlan 的提出为大模型推理能力发展提供了一个新思路。这项创新通过将代码形式规划引入推理过程，成功解决了自然语言表达中的结构化缺陷；更重要的是，它开创了一种全新的方法论，为大模型注入了系统化的问题解决能力。

# 3.5 Wed
* 43.360智脑开源Light-R1！1000美元数学上首次从零超越DeepSeek-R1-Distill  机器之心  https://mp.weixin.qq.com/s/5VPU0C8EK8jxdtm7OJS5lA \
  仅需 12 台 H800 上 6 小时即可训练完成，从没有长思维链的 Qwen2.5-32B-Instruct 出发，仅使用 7 万条数学数据训练，得到 Light-R1-32B \
  模型仓库：https://huggingface.co/qihoo360/Light-R1-32B \
  项目地址：https://github.com/Qihoo360/Light-R1
* 44.阿里推理模型一战封神！32B硬刚671B DeepSeek，1/10成本，苹果笔记本可跑  量子位  https://mp.weixin.qq.com/s/ZtnUV0RLf6_CR04Sbm-Wyw \
  阿里半夜开源全新推理模型，QwQ-32B比肩DeepSeek-R1满血版  机器之心  https://mp.weixin.qq.com/s/oejAfPpireHKUzqTOSPyXw \
  Qwen发布最新32B推理模型，跑分不输671B的满血版DeepSeek R1 \
  QwQ 32B是一个密集模型，没有用到MoE，上下文长度有131k \
  博客：https://qwenlm.github.io/zh/blog/qwq-32b/ 「QwQ-32B: 领略强化学习之力」 \
  Hugging Face：https://huggingface.co/Qwen/QwQ-32B \
  ModelScope：https://modelscope.cn/models/Qwen/QwQ-32B \
  演示：https://huggingface.co/spaces/Qwen/QwQ-32B-Demo \
  Qwen Chat：https://chat.qwen.ai/ \
  本地部署工具 Ollama 也第一时间提供了支持：ollama run qwq \
  QwQ-32B 中还集成了与 Agent（智能体）相关的能力，使其能够在使用工具的同时进行批判性思考，并根据环境反馈调整推理过程。该团队表示：「我们希望我们的一点努力能够证明强大的基础模型叠加大规模强化学习也许是一条通往通用人工智能的可行之路。」
* 45.开启空间智能问答新时代：Spatial-RAG框架来了  机器之心  https://mp.weixin.qq.com/s/IXvhr0rhxzgJ1dOOBbTN7g \
  革命性RAG框架！Emory & UTA | 提出Spatial-RAG——开启空间智能问答的新时代  AINLPer  https://mp.weixin.qq.com/s/dO1hZYK5lugqsURAEgXetQ \
  Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Spatial Reasoning Questions \
  Spatial-RAG 通过结合空间数据库和 LLM 的语义理解能力，显著提升了空间推理任务的性能。实验表明，Spatial-RAG 在真实世界数据集上表现优异，能够有效处理复杂的空间推理问题，为旅游推荐、路径规划等应用提供了强大的支持。
* 46.当持续学习遇上多模态大型语言模型：综述  专知  https://mp.weixin.qq.com/s/WGib2MiIB6fD45zA5Of-2A \
  When Continue Learning Meets Multimoda Large Language Model: A Survey \
  ？当前有啥解决“灾难性遗忘”问题的办法
* 47.AI话痨终结者！UCSD清华提出「思维扫描术」Dynasor-CoT，推理巨省token  新智元  https://mp.weixin.qq.com/s/HVnWueo_9yWzIyS_8aJWOg \
  Dynasor: More Efficient Chain-of-Thought Through Certainty Probing \
  以一个简单问题 (1+2i)*6-3i测试为例：传统Qwen-7B用180个token轻松解题，而升级后的Deepseek版Qwen-7B虽在第340个token就得出正确答案，却硬生生续写1000+token反复验证！这种「学霸强迫症」，让DeepSeek等顶尖模型浪费了高达70%的算力！ \
  为了解决模型的「自我怀疑」问题，研究团队提出了Dynasor-CoT，一种无需训练、侵入性最小且简单的方法，用于长链式推理（CoT）。 \
  团队已将这款「AI话痨终结者」系统全面开源 \
  如果AI连续N次的「CT扫描」结果都显示同一个答案，系统就会判定AI非常自信，并果断按下停止键。坚定地告诉这位同学：「你已经答对了，不用再证明了！」 \
  通过答案一致性进行确定性评估 \
  https://hao-ai-lab.github.io/blogs/ \
  https://github.com/hao-ai-lab/Dynasor \
  https://hao-ai-lab.github.io/demo/dynasor-cot
* 48.智源开源多模态向量模型BGE-VL：多模态检索新突破  机器之心  https://mp.weixin.qq.com/s/iw9BmSDwv6NYtD7pkC5kxQ \
  多模态向量模型 BGE-VL，在图文检索、组合图像检索等主要多模态检索任务中均取得了最佳效果 \
  项目主页：https://github.com/VectorSpaceLab/MegaPairs \
  模型地址：https://huggingface.co/BAAI/BGE-VL-MLLM-S1 
* 49.苹果最强M3 Ultra首发逆天，512GB「桌面超算」在家跑DeepSeek-R1！  机器学习研究组订阅  https://mp.weixin.qq.com/s/cLXEWVb_GwvwzzP0WEH1zg 
* 50.DeepSeek的MLA，任意大模型都能轻松迁移了  机器之心  https://mp.weixin.qq.com/s/UvvHdHb8eeg4qHdkGejxhA \
  Towards Economical Inference: Enabling  DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs \
  https://github.com/JT-Ushio/MHA2MLA \
  多头潜在注意力网络（Multi-head Latent Attention, MLA）是其经济推理架构的核心之一，通过对键值缓存进行低秩压缩，显著降低推理成本 \
  复旦 NLP 实验室、华东师大、上海 AI Lab、海康威视联合提出 MHA2MLA 框架，通过部分 RoPE 保留（Partial-RoPE）和键值联合表示低秩近似（Low-rank Approximation）两个关键步骤，成功将任意 MHA/GQA 架构迁移到 MLA。
* 51.从自我进化视角出发，全面解析LLM的推理能力技术演进路径  机器之心  https://mp.weixin.qq.com/s/hkYW0c26eLEHE9WgIQxwHw \
  A Survey on LLM Complex Reasoning through the Lens of Self-Evolution \
  仓库链接：https://github.com/cs-holder/Reasoning-Self-Evolution-Survey \
  本文系统地综述了 LLM 复杂推理的自我进化方法，从**数据进化**、**模型进化**和**自我进化**三个角度进行了深入分析

# 3.6 Thur

# 3.7 Fri
* 52.Manus爆火，一大批open manus项目如雨后春笋般涌现  探索AGI  https://mp.weixin.qq.com/s/we27Kq-RAHosCRkLxyWm4Q \
  特工宇宙 OpenManus \
  OWL项目 \
  manus教程项目 \
  OpenManus/OpenManus \
  claude computer use demo 
* 53.QwQ-32B 测评和使用教程来了！  Datawhale  https://mp.weixin.qq.com/s/Qj3vlhFrhYzILQCH50707g \
  一张 4090，本地部署了一个 QwQ-32B-AWQ 量化版本 \
  教程地址：https://datawhaler.feishu.cn/docx/Wz2NdqSx1oEZsuxB9zHcEQ20nNe \
  阿里千问QwQ-32B推理模型开源，比肩671B满血DeepSeek-R1！笔记本就能跑  新智元  https://mp.weixin.qq.com/s/EH3cLd-nnOT5ZBFBts92MA \
* 54.FP8训练新范式：减少40%显存占用，训练速度提高1.4倍  机器之心  https://mp.weixin.qq.com/s/na86HSj92MCyvQTzVK_H5g 
* 55.OWL：0天复刻Manus通用智能体，完全开源！GAIA Benchmark最强性能！  Datawhale  https://mp.weixin.qq.com/s/E-HWNzjZdw_0PRHjvlO8bw \
  CAMEL-AI的OWL
  GitHub：https://github.com/camel-ai/owl
* 56.认知单元实现  CreateAMind  https://mp.weixin.qq.com/s/WGrNRXjLqGMEL8QD6HLnKQ \
  Visual motion perception as online hierarchical inference \
  识别环境中运动关系的结构对于导航、追踪、预测和追逐至关重要。然而，关于视觉系统如何从不稳定的视觉信息流中在线推断这种结构的心理和神经计算过程，我们知之甚少。我们提出在线层次贝叶斯推断作为一种合理的方法，解释大脑如何解决这一复杂的知觉任务。
* 57.微软GUI智能体OmniParser二代开源！推理延迟降低60%，大模型玩手机更溜了  新智元  https://mp.weixin.qq.com/s/snWwF9mL9C-kKYr_h0nHxw \
  OmniParser V2可将屏幕截图转换为结构化元素，帮助LLM理解和操作GUI；在检测小图标和推理速度上显著提升，延迟降低60%，与多种LLM结合后表现优异。 \
  代码: https://github.com/microsoft/OmniParser/tree/master \
  模型: https://huggingface.co/microsoft/OmniParser-v2.0 \
  Demo：https://huggingface.co/spaces/microsoft/OmniParser-v2OmniParser方法概述
* 58.32B击败DeepSeek-R1、o3-mini，成本暴降100倍！GRPO让小模型称霸推理  新智元  https://mp.weixin.qq.com/s/YdKDyrVpBZthg4fD29N8Yg \
  在具有挑战性的「时间线索」（Temporal Clue）逻辑谜题中，基于强化学习(**GRPO**)微调后的Qwen 2.5 32B，推理能力完全碾压o1、o3-mini、R1。 \
  https://openpipe.ai/blog/using-grpo-to-beat-o1-o3-mini-and-r1-on-temporal-clue \
  **GRPO让小模型称霸推理**
* 59.美团开源首发INT8无损满血版DeepSeek R1  AGI之美  https://mp.weixin.qq.com/s/yofbSYhHy_KXfRA9hWFF2Q \
  https://huggingface.co/meituan/DeepSeek-R1-Block-INT8   \
  https://huggingface.co/meituan/DeepSeek-R1-Channel-INT8

# 3.8 Sat
* 60.模拟真实世界：多模态生成模型的统一综述  专知  https://mp.weixin.qq.com/s/hrmMhFF4i9MBBvLnehd51w \
  Simulating the Real World: A Unified Survey of Multimodal Generative Models
* 61.7B级形式化推理与验证小模型，媲美满血版DeepSeek-R1，全面开源！  机器之心  https://mp.weixin.qq.com/s/oyhICTRo2fJL5MZkDrutXg \
  近日，由香港科技大学牵头，联合中科院软件所、西安电子科技大学、重庆大学等单位，开源了一系列形式化推理与验证大模型，仅用 7B，即可在相关任务上获得与 671B 满血版 DeepSeek-R1 相当的水平！ \
  From Informal to Formal–Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs \
  Hugging Face 模型链接：https://huggingface.co/fm-universe 
* 62.世界首个人类脑细胞计算机诞生！能编程还能活体计算，**售价3万5美金**  新智元  https://mp.weixin.qq.com/s/dtLbWoK48sHE-_jGmFv45g \
  Cortical lab的CL1 \
  在CL1的硅芯片表面，有实验室培育的人类神经元。这些神经元能够响应电信号，形成与生物大脑类似的信息处理网络。这个系统被设计为允许双向通信，其中电脉冲刺激神经元，并记录和分析它们的反应。为了维持神经元的活力，CL1配备了一个生命维持系统，该系统调节温度、气体交换和其他必要条件。CL1不是一台普通的计算机——它没有 GPU、CPU或RAM，而是依靠真实的神经元形成动态连接，自适应调整，专为需要真实学习能力的生物计算任务设计。
* 63.获得图灵奖后，强化学习之父最新访谈：AI研究的正确方向  Datawhale  https://mp.weixin.qq.com/s/GrM3YhuyykqU1mspanv5EA \
  强化学习是关于从经验中学习，强化学习关键是从评估性反馈中学习。从经验中学习是AI研究的正确的方向。AI是一场马拉松，而非短跑，Sutton不认同“现在一切发展得太快了”的观点，他表示，AI最具影响力的那些方面尚未到来。
* 64.从刷题到搬砖，通用多模态大模型离具身智能还有多远？  PaperWeekly  https://mp.weixin.qq.com/s/CkthNjVn2h79Iv4ZyxJz-A \
  为 MLLM 驱动的具身智能体提供标准化、多维度评测的框架—— EmbodiedBench \
  SEMBODIEDBENCH: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents \
  六种能力评估：基础任务解决、常识推理、复杂指令理解、空间认知、视觉感知、长期规划 \
  基于 EmbodiedBench，我们能做些什么？ \
  提升低级任务执行和空间推理能力、强化长远规划能力、优化多步/多视角图像理、改进视觉上下文学习（ICL）、训练多模态智能体、提升模型的鲁棒性和泛化能力

# 3.9 Sun
* 65.(**值得看看**)黑暗中的大脑：神经模拟设计原则  CreateAMind  https://mp.weixin.qq.com/s/sGMaCW7pAtsEoIhnwFk4bg \
  Brain in the Dark: Design Principles for Neuromimetic \
  https://github.com/MLDawn/PC-network-NeurIPs-2024 \
  摘要：深度学习通过从原始数据中实现自动特征提取和函数近似，彻底改变了人工智能（AI）。然而，它面临着诸如缺乏分布外泛化能力、灾难性遗忘和可解释性差等挑战。相比之下，生物神经网络（如人脑中的神经网络）并不存在这些问题，这激发了AI研究人员探索神经模拟深度学习，其目标是在AI模型中复制大脑机制。这种方法的一个基础理论是自由能原理（FEP），尽管它具有潜力，但由于需要跨多个领域的跨学科理解，通常被认为在AI中理解和实现起来过于复杂。本文旨在揭开FEP的神秘面纱，并为设计具有类人感知能力的神经模拟模型提供一个全面的框架。我们提出了一个实现这些模型的路线图，并提供了一个Pytorch代码库，用于在预测编码网络中应用FEP。 \
  本文贡献了以下内容：1. 一个准确且高效地使用自由能原理设计神经模拟AI的路线图。2. 一个轻量级且基于CPU的Pytorch代码库，实现了在预测编码（PC）网络中的自由能原理 \
  结论：神经模拟人工智能（Neuromimetic AI）旨在赋予传统人工智能模型（例如深度学习）类似大脑的神经元消息传递和类人推理能力。自由能原理（FEP）是实现这一目标最有前景的方向之一。然而，由于其数学上的复杂性和多学科的性质，沿着自由能原理的路径探索神经模拟、理解它以及当然，实现它，仍然是研究人员面临的艰巨任务。本文详细阐述了基于自由能原理设计神经模拟人工智能模型的设计原则，该原理应用于预测编码（PC）网络。最后但同样重要的是，我们提供了一个基于自由能原理实现预测编码网络的Pytorch代码库，该网络模拟了人类的感知能力。
* 66.让SFT重新伟大！CMU等华人学者提出全新「批判式微调」，媲美复刻版DeepSeek  新智元  https://mp.weixin.qq.com/s/l1DdkoHp36g05dRm2a3gDg \
  Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate \
  随着数据集规模和质量的提升，SFT面临着边际收益递减的问题，尤其是在训练本身效果就不错的基础模型时，使用SFT甚至可能会导致性能下降。最近，CMU、滑铁卢大学等机构的3名华人学者就发表了一篇论文，针对SFT做出了更进一步的改进，提出批判式监督微调方法（CFT，Critique Fine-Tuning），旨在让模型更有效地模仿模仿数据集。 \
  CFT将重点从简单模仿转向基于批判的学习，核心思想是让模型学会批判，而不是简单地模仿，其灵感来源于人类的学习过程。
* 67.万字梳理：揭秘 DeepSeek 中的 RL 与 AGI 下一步丨AIR 2025  图灵人工智能  https://mp.weixin.qq.com/s/GXr6NAUviVtqwKbCca41-A \
  (1)DeepSeek 的语言模型推理开放训练方法 \
  (2)领悟的 Transformer 是隐式推理器 \
  Understanding Reasoning in LLMs and Agents: From Grokking of lmplicit Reasoning to Test-Time Scaling with Verifiers \
  Transformer 架构的非递归设计阻碍了跨层的记忆共享，从而限制了模型在 OOD 任务中的泛化能力\
  (3)统一符号结构与神经网络表示 \
  Towards a unified framework of Neural and Symbolic Decision Making \
  (4)基于 API 的网络智能体  LLM Agents that Learn from Experience \
  Beyond Browsing: API-based Web Agents \
  (5)AI 新前沿：形式化数学推理 \
  (6)Goedel-Prover：自动定理证明领域的开源模型 
* 68.对话DeepSeek研发团队前成员辛华剑：如何用大模型把数学家从细节中解放出来  图灵人工智能  https://mp.weixin.qq.com/s/PBHnHaJfQAv1eRteaFNTHw \
  DeepSeek-Prover-V1.5 \
  这个项目的理想是，最终能够推出一种服务或产品，帮助数学家快速验证一些比较简单的猜想，把数学家从细节当中解放出来。
* 69.10²⁶参数，AGI还需70年！清华人大预测届时GPU总价达4000万倍苹果市值  新智元  https://mp.weixin.qq.com/s/yc7PopYMkv0G8a1NMX8ufA \
  Evaluating Intelligence via Trial and Error \
  研究人员提出了「生存游戏」这一框架，用以量化并评估智能。 \
  基于失败次数的期望和方差，研究人员将智能分为三个层次：有限级、胜任级、自主级。 \
  LLM目前停留在「有限级」 \
  他们预测，要在通用语言任务中达到「自主级」，AI系统需要惊人的10²⁶个参数。这一规模相当于全人类大脑神经元总数的10⁵倍！若要加载如此庞大的模型需要5×10¹⁵张H100 GPU，其总成本高达苹果公司市值的4×10⁷倍。 \
* 70.AI玩手机越玩越6！西湖大学发布新智能体：会自我进化的AppAgentX  量子位  https://mp.weixin.qq.com/s/tqu36cirzyBhfTAZVOE2GQ \
  能够在不断执行任务的过程中学习并优化自身的行为模式，实现更加高效的操作。 \
  创新点： \
  1.自动归纳高效操作模式：代理能够在执行任务时，检测重复性操作模式，并自动总结成更高级别的“一键”操作。 \
  2.任务执行更快，减少重复计算：传统的LLM代理每次执行任务都需要重新思考操作流程，而AppAgentX能够记住并复用执行策略，从而避免重复推理，使得任务执行更加流畅高效。 \
  3.完全基于视觉操作，适用于各种软件：传统自动化方法通常需要访问后端API，而AppAgentX仅依赖屏幕视觉信息进行操作，无需后端访问，因此能够在不同软件、不同设备上通用，真正做到“即插即用”。 \
  像人类一样，通过屏幕视觉、鼠标和键盘直接操作软件界面。这意味着，智能体可以自主学习如何操作各种应用程序，甚至能够在不同软件之间切换，执行复杂的跨应用任务。 
* 71.1.5B硬刚GPT-4o，CMU祭出LCPO提示可控思考！每token性能较S1暴涨2倍  新智元  https://mp.weixin.qq.com/s/bsByK9TbwFn3b9RxxhTYEA \
  L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning \
  CMU团队用LCPO训练了一个15亿参数的L1模型，结果令人震惊：在数学推理任务中，它比S1相对提升100%以上，在逻辑推理和MMLU等非训练任务上也能稳定发挥。更厉害的是，要求短推理时，甚至击败了GPT-4o——用的还是相同的token预算！ \
  最近，CMU团队推出了「长度控制策略优化」（LCPO），它让AI的推理过程不再是「一刀切」，而是像个聪明管家，能根据任务需求灵活调整「思考」长度。更惊艳的是，它还能把长推理的智慧「浓缩」到短答案中，使模型的效率和准确性双双飙升。 
* 72.DualPipe深入浅出：没有分布式训练基础也能看懂的DualPipe全方位讲解  PaperWeekly  https://mp.weixin.qq.com/s/FveZS364dbV0XKwcl4USsg
* 73.一句话全自动创建AI智能体，港大AutoAgent打造开源最强Deep Research  PaperWeekly  https://mp.weixin.qq.com/s/R-jhfR7d79HDdKpHocFQ5Q \
  只要用自然语言就能创建 AI 助手 \
  AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents \
  自研框架 AutoAgent：https://github.com/HKUDS/AutoAgent \
  Auto-Deep-Research：https://github.com/HKUDS/Auto-Deep-Research
* 74.Manus开源复刻框架OWL，测评和使用教程来了！  Datawhale  https://mp.weixin.qq.com/s/lvs2y2ZnSJo5GZ7gbVkQLQ

# 3.10 Mon
* 75.草稿链代替思维链，推理token砍掉80%，显著降低算力成本和延迟  量子位  https://mp.weixin.qq.com/s/Jy6RSAyd_ioWiArkUJ-6kg \
  全新CoD颠覆推理范式，准确率接近但token消耗成倍降低  机器之心  https://mp.weixin.qq.com/s/AFrNUVGOi4vPAVzCWukdzw \
  Chain of Draft: Thinking Faster by Writing Less \
  https://ajithp.com/2025/03/02/chain-of-draft-llm-prompting/
* 76.稚晖君的「好东西」揭晓！首个通用具身基座模型，机器人告别「看得懂做不来」  机器之心  https://mp.weixin.qq.com/s/pErwkCKNeuEvimrAfKcbWA \
  AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems \
  https://agibot-world.com/blog/agibot_go1.pdf
* 77.「古董」GPU也能跑DeepSeek同款GRPO！显存只需1/10，上下文爆涨10倍  新智元  https://mp.weixin.qq.com/s/9rFinH1LCW5yja6LeK6Rfw \
  开源微调神器Unsloth带着黑科技又来了：短短两周后，再次优化DeepSeek-R1同款GRPO训练算法，上下文变长10倍，而显存只需原来的1/10！
* 78.1次搭建完胜1亿次编码，MCP硅谷疯传！Anthropic协议解锁智能体「万能手」  新智元  https://mp.weixin.qq.com/s/YRcZTU-uzXNG64fv5HIcwg 
* 79.优于o1预览版，推理阶段KV缓存缩减一半，LightTransfer降本还能增效
  机器之心  https://mp.weixin.qq.com/s/J8RLwdzbRhGSPqhYYAGPkw \
  项目主页：https://sites.google.com/view/lighttransfer \
  Huggingface 模型：cxdu/QwQ-32B-LightTransfer \
  github 代码：https://github.com/sail-sg/LightTrans
* 80.LeCun最新访谈对DeepSeek一顿猛夸，还谈了AI发展需理解物理世界  量子位  https://mp.weixin.qq.com/s/9kDLyi1Jf3fcbRASStzVYw
* 81.已节省数百万GPU小时！字节再砍MoE训练成本，核心代码全开源  量子位  https://mp.weixin.qq.com/s/HGETri4o7yG5efO0x44JYg \
  豆包大模型团队在GitHub上开源了叫做COMET的MoE优化技术
* 82.机器人泛化能力大幅提升：HAMSTER层次化方法和VLA尺度轨迹预测，显著提升开放世界任务成功率  机器之心  https://mp.weixin.qq.com/s/yaMbDgEfmJ990cqcKofiVw \
  HAMSTER（Hierarchical Action Models with Separated Path Representations）通过层次化架构，在高层利用域外数据微调的大模型（VLM）生成二维路径，中间表示解耦了任务规划与具体执行，让低层控制模块专注于实际动作控制
* 83.概念的内在规范性 The Inherent Normativity of Concepts  CreateAMind  https://mp.weixin.qq.com/s/Wp0sKCvHC0RUWMSl7bYfDw \
  使用概念进行思考——在更根本的层面上——是一种与世界互动和参与的基本方式，这一观点恰如其分地被“通过（物理上）抓取某物来理解它”的隐喻所概括，例如“把握”生活世界 \
  本文建立了一个关于概念的主动推断视角，其主要目标是强调概念的内在规范性。在这个框架中，**概念被视为从代理与环境互动的独特动态中涌现出来的**
* 84.1000 token/s的「扩散LLM」凭什么倒逼AI走出舒适区？  夕小瑶科技说  https://mp.weixin.qq.com/s/Zm4RQO67kVNnrapEbZI_og \
  Large Language Diffusion Models 

# 3.11 Tue
* 85.ActiveInference.jl主动推理进行仿真和参数估计  CreateAMind  https://mp.weixin.qq.com/s/KKkKoCw47KKRaWjjSExJJw \
  Introducing ActiveInference.jl: A Julia Library for Simulation and Parameter Estimation with Active Inference Models \
  ???主动推理 AIF \
  ???Julia语言
* 86.阿里通义、港科大等提出RAG与长文本对比新框架，助力智能路由决策机制设计  PaperWeekly  https://mp.weixin.qq.com/s/2JMSMo0DUPq8ZzUjGOt_lw \
  LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing \
  研究结果表明，当前 LC LLMs 在处理超长文本时并未展现出对 RAG 的全面碾压性优势，两者在不同应用场景中呈现出显著的互补性特征。
* 87.从感官信息直接构建概念：实现模型  CreateAMind  https://mp.weixin.qq.com/s/PkEUogud685y3ZQy7JCvqg \
  Concept Emergence from Complex Sensory Data: A Connectionist Model 

# 3.12 Wed
* 88.李飞飞团队「具身智能」新作：机器人接手所有家务  图灵人工智能  https://mp.weixin.qq.com/s/SZW07nraBpqC3FV2odDZnQ \
  BEHAVIOR Robot Suite（简称 BRS） \
  论文主页：https://behavior-robot-suite.github.io/ \
  JoyLo（Joy-Con on Low-Cost Kinematic-Twin Arms） \
  WB-VIMA（Whole-Body VisuoMotor Attention）
* 89.多步推理碾压GPT-4o，无需训练性能提升10%！斯坦福开源通用框架OctoTools  新智元  https://mp.weixin.qq.com/s/KlqFub_Z-JfcQHIQZgP8Cw \
  OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning \
  https://github.com/octotools/octotools \
  OctoTools框架的核心设计理念是让人工智能系统能够高效地使用外部工具，而无需额外的训练或框架调整。主要包含工具卡片、规划器、执行器，还包含一个任务指定的工具集优化算法，能够学习为下游任务选择最有用的工具子集。整个过程不断迭代，直到找到完整的解决方案。
* 90.从空间感官数据中生成语义形式概念，例子  CreateAMind  https://mp.weixin.qq.com/s/bLuX1sbBOCjeUJhoAvMWiw \
  A Recursive Bateson-Inspired Model for the Generation ofSemantic Formal Concepts from Spatial Sensory Data \
  一种受贝茨理论启发的递归模型，用于从空间感官数据中生成语义形式概念 \
  ???贝茨理论
* 91.Manus半小时复刻自己，24小时狂揽1.4K星，完成度究竟如何？  探索AGI  https://mp.weixin.qq.com/s/3gtqJQi8F7AL4M66SXeuZQ \
  https://github.com/nikmcfly/ANUS/tree/main \
  Anus整体完成度并不高，并不能达到预期
* 92.什么是后训练？大语言模型训练后优化方法综述，87页pdf  专知  https://mp.weixin.qq.com/s/UzZXSwZXMLUUaVJPV-Jkng \
  A SURVEY ON POST-TRAINING OF LARGE LANGUAGE MODELS \
  Post-training Language Models, PoLMs 训练后语言模型
* 93.90分钟生成10万Token，新框架实现3倍无损加速超长文本生成，支持DeepSeek-R1和QwQ！  量子位  https://mp.weixin.qq.com/s/Icx9sTwykZOLWVgA_qENmg \
  From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens \
  从模型加载、KV缓存管理到Token生成策略进行了全方位的优化，实现了无损加速
* 94.（**值得看看**）将哈密顿力学泛化到神经算子，何恺明团队又发新作，实现更高级物理推理 
 机器之心  https://mp.weixin.qq.com/s/NRAsYrA-fEU692_WxrVZ4w \
  驯服AI，更懂物理！何恺明团队提出全新DHN「去噪哈密顿网络」  新智元  https://mp.weixin.qq.com/s/0Wj5BOgFlmrkbDgu2gpc-A \
  Denoising Hamiltonian Network for Physical Reasoning \
  何恺明团队提出的去噪哈密顿网络（DHN），将哈密顿力学融入神经网络，突破传统局部时间步限制，还有独特去噪机制，在物理推理任务中表现卓越。 \
  传统的机器学习方法虽然能处理一些简单的物理关系，但面对复杂的物理系统时，却显得力不从心。来自MIT、斯坦福、西北大学等的研究者将哈密顿力学算子推广到神经网络中，不仅能捕捉非局部时间关系，还能通过去噪机制减轻数值积分误差。 \
  实验结果表明，在处理与训练集初始状态相同的轨迹时，DHN和基于CNN的方法都能取得较好的插值效果。但在处理具有未见过初始状态的轨迹时，CNN由于严重依赖训练分布，难以泛化，而DHN凭借其受物理约束的表示，能够推断出合理的中间状态，展现出了强大的泛化能力。 \
  总结：DHN在物理推理上展现出更强的泛化能力
* 95.用PyTorch从零构建 DeepSeek R1：模型架构和分步训练详解  机器学习研究组订阅  https://mp.weixin.qq.com/s/TXlU0REQ8fA2at5zYu_g9A 
* 96.上海AI Lab等提出GENOME(+)框架：单卡4090玩转10+大模型的种群进化  PaperWeekly  https://mp.weixin.qq.com/s/2s9GpAuv8__6HjCIESmigw \
  Nature-Inspired Population-Based Evolution of Large Language Models
* 97.2025年的风口！| 万字长文，带你纵观大模型Agent，涉及研究痛点、应用场景、发展方向  AINLPer  https://mp.weixin.qq.com/s/cq9TBA2q0Iz-b8w890JWaQ
* 98.上海AI Lab等提出GENOME(+)框架：单卡4090玩转10+大模型的种群进化 
 PaperWeekly  https://mp.weixin.qq.com/s/2s9GpAuv8__6HjCIESmigw \
  Nature-Inspired Population-Based Evolution of Large Language Models \
  提出了名为 GENOME（+）的大语言模型优化框架，以创新性的视角系统地将进化算法引入到大语言模型的种群优化问题中，无需梯度优化，让大语言模型种群经过交叉、变异、选择、继承以及集成等经典进化操作，仅用极少的样本即可提高模型性能 \
  种群进化替代了梯度传播算法，大幅降低计算资源消耗与成本 \
  ???真的比梯度反向传播高效吗

# 3.13 Thur
* 99.谷歌开源Gemma-3：媲美DeepSeek，算力暴降10倍  AIGC开放社区  https://mp.weixin.qq.com/s/8HusRonfYRZO97tXCHwKew \
  Gemma-3共有10亿、40亿、120亿和270亿四种参数
* 100.世界模型：感觉的含义：形式化了'“理解”感觉数据流的含义'(意识) 理论及实现  CreateAMind  https://mp.weixin.qq.com/s/huqCIZ8k6s3B9ppx6ACIJw \
  Making sense of sensory input \
  理解感官输入 \
  如果给定X和P，我们希望生成Y，那么我们正在进行演绎。如果给定P和Y，我们希望生成X，那么我们正在进行演绎。如果给定X和Y，我们希望生成P，那么我们正在进行归纳。最后，如果只给定Y，我们希望生成X和P，那么我们正在联合进行演绎和归纳。这就是感知引擎所做的 ??? \
  摘要：这篇论文试图回答无监督学习中的一个核心问题：对感觉序列“有意义”意味着什么？在我们的形式化中，有意义的理解涉及构建一个符号因果理论，该理论不仅解释了感觉序列，还满足一组统一条件。统一条件坚持因果理论的组成部分——对象、属性和法则——必须整合成一个连贯的整体。在我们的账户中，对感觉输入的理解是一种程序合成，但它是无监督的程序合成。我们的第二个贡献是计算机实现，即感知引擎，它被设计来满足上述要求。我们的系统能够从非常少量的数据中产生可解释的、人类可读的因果理论，这得益于统一条件提供的强烈归纳偏见。我们系统产生的因果理论能够预测未来的传感器读数，以及回溯早期读数，并填补（填补空白）缺失的感觉读数，以任何组合。事实上，它能够同时完成所有三项任务。
* 101.Open-Sora 2.0全面开源，20万复刻百万级大片！11B媲美闭源巨头，224张GPU创奇迹  新智元  https://mp.weixin.qq.com/s/n-n4dA0j4I9wvUKSL5DtSw \
  全面开源模型权重、推理代码及分布式训练全流程 \
  https://github.com/hpcaitech/Open-Sora \
  技术报告：https://github.com/hpcaitech/Open-Sora-Demo/blob/main/paper/Open_Sora_2_tech_report.pdf
* 102.奥特曼自曝全新OpenAI写作模型：第一次被AI震撼！网友：AI写的坚决不看  新智元  https://mp.weixin.qq.com/s/OCsPK4s4EwJLdYfyEHM7KA
* 103.超越DeepSeek-R1关键RL算法GRPO，CMU「元强化微调」新范式登场  机器之心  https://mp.weixin.qq.com/s/forC9l_zd3a_p7HemHmggw \
  Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning
* 104.长链推理表象下，大模型精细表征张冠李戴的本质  机器之心 
 https://mp.weixin.qq.com/s/RYTDrcqJEjFNttxxPRPY4Q \
  是否就是幻觉导致的？
* 105.R1-GRPO用于多模态、ChatBI、Gemma3等前沿进展：兼看KTransformers技术分享回顾  老刘说NLP  https://mp.weixin.qq.com/s/SxL0EeJjMLC_vNLtdDHQhw
* 106.“谷歌版DeepSeek”接入机器人，思维链解锁折纸系鞋带技能，推理模型代入物理世界  量子位  https://mp.weixin.qq.com/s/2zIIFXwSM5r0It_DcPzQ6A \
  https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf \
  https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/ \
  Gemini Robotics，一款视觉-语言-动作 (VLA) 模型，增加一种新的输出方式“物理动作”，可以直接控制机器人 \
  Gemini Robotics-ER，额外增强对空间和时间理解，解锁指向、多视图3D理解以及抓握预测等能力 \
  Gemini Robotics由云端的VLA骨干网络和机器人本地芯片上运行的本地动作解码器组成，经过优化后延迟从几秒钟降到160毫秒以下。 
* 107.自动调整推理链长度，SCoT来了！为激发推理能力研究还提出了一个新架构  量子位  https://mp.weixin.qq.com/s/x7ULVxhD9LTHCFUtzaSXRw \
  Can Atomic Step Decomposition Enhance the Self-structured Reasoning ofMultimodal Large Models? \
  SCoT，即自结构化推理链（Self-structured Chain of Thought ） \
  通过将推理过程分解为最小语义原子步骤，能动态生成适配不同复杂度问题的CoT结构，解决了现有方法在推理多样性和效率上的不足 \
  另外，为了激发推理能力，研究人员还提出了**AtomThink**，这是一个包含数据构造、训练、推理和评估的全过程框架，用来提升多模态大模型在复杂推理任务上的表现 、
  解决过度思考的问题
* 108.揭示显式CoT训练机制：思维链如何增强推理泛化能力  机器之心  https://mp.weixin.qq.com/s/sE2ckVDJPdMPrC9dfcvSKA \
  Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization \
  为了全面理解采用 CoT 训练的策略，需要解决两个关键问题： \
  Q1：与无 CoT 训练相比，采用 CoT 训练有哪些优势？ \
  Q2：如果存在优势，显式 CoT 训练的潜在机制是什么？ \
  本文通过在受控和可解释的环境中展示系统性组合泛化如何通过显式思维链（CoT）训练在 Transformer 中产生，揭示了思维链训练的核心机制。具体而言：
\
（1）与无思维链训练相比，思维链训练显著增强了推理泛化能力，使其从仅限分布内（ID）泛化扩展到同时涵盖分布内和分布外（OOD）场景。 \
（2）通过 logit lens 和 causal tracing 实验，我们发现思维链训练（使用两跳事实）将推理步骤内化到 Transformer 中，形成了一个两阶段泛化电路。然而，模型的推理能力受训练数据复杂性的限制，因为它难以从两跳情况泛化到三跳情况。**这表明思维链推理主要是重现了训练集中存在的推理模式**。 \
（3）我们进一步将分析扩展到推理过程中存在错误的训练数据分布，证明当噪声保持在一定范围内时，思维链训练仍能使模型实现系统性泛化，此类噪声数据的结构或许有助于泛化电路的形成。

# 3.14 Fri
* 109.一文了解人工智能(AI)算法及GPU运行原理  图灵人工智能  https://mp.weixin.qq.com/s/ZaYghk1qD3YYbSiFwfkHFA 
* 110.1W2000字 深度剖析：为何扩散模型会成为语言模型的未来？  图灵人工智能  https://mp.weixin.qq.com/s/AnmhDdK2sLpgDgGHqQGyLA 
* 111.迈向推理时代：大型语言模型的长链推理研究综述  专知  https://mp.weixin.qq.com/s/G-2tlmXh-Yg2Gf4kuGeBJQ \
  Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models \
  https://long-cot.github.io/ \
  长链推理的三大关键特性——深度推理、广泛探索和可行反思
* 112.进阶2 端到端世界模型：感觉的含义：形式化了'“理解”感觉数据流的含义'(意识) 理论及实现  CreateAMind  https://mp.weixin.qq.com/s/oA1r66Iu6aHBPm0PVrzPMQ \
  理解原始输入 Making sense of raw input 
* 113.All in VLM！华为诺亚推出记忆增强的VLM决策方案Mem2Ego，刷新具身导航SOTA记录  PaperWeekly  https://mp.weixin.qq.com/s/EwemNMA4Ec6YxzcGR1pRhQ \
  华为诺亚方舟实验室的研究人员们提出了一种基于视觉语言模型（VLMs）的导航框架 Mem2Ego，通过自适应检索全局记忆模块中的任务相关线索，并将这些线索映射到智能体的第一视角图像中，从而增强智能体的环境感知和决策能力。该方法能够动态地对齐全局语义信息与局部感知，从而优化导航路径并提升长序列任务的执行效率。 \
  Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation \
  本研究提出了一种高效的融合策略，将任务相关的全局记忆信息映射到第一人称视角，从而克服了现有多模态导航框架因局部视野限制而导致的次优解问题。此外，该方法能够同时激活并利用 VLM 的复杂空间理解、空间推理以及常识推理能力，从而显著提升智能体在复杂空间场景中的导航决策能力和效率。
* 114.何恺明LeCun联手改造Transformer！9行代码替代归一化层，性能不减还加速  量子位  https://mp.weixin.qq.com/s/JKnh-35CabjnB-3hT6U1og \
  没有归一化层的Transformer！刘壮带队，何恺明、Yann LeCun都参与了  机器之心  https://mp.weixin.qq.com/s/gKVkJzbHHtVDhiYhG8FCbw \
  Transformers without Normalization \
  归一化长期以来一直被认为是必不可少的，在现代神经网络中无处不在。但团队认为可以换用一种非常简单的技术，他们提出DyT（Dynamic Tanh)，直接替代Layer Norm或RMSNorm，性能达到或超过标准Transformer。 \
  本文中，研究者提出了 Transformer 中归一化层的一种简单平替。他们的探索始于以下观察：LN 层使用类 tanh 的 S 形曲线将其输入映射到输出，同时缩放输入激活并压缩极值。 \
  受此启发，研究者提出了一种元素级运算，称为 Dynamic Tanh（DyT），定义为：DyT (x) = tanh (αx)，其中 α 是一个可学习参数。此运算旨在通过 α 学习适当的缩放因子并通过有界 tanh 函数压缩极值来模拟 LN 的行为。值得注意的是，与归一化层不同，DyT 可以实现这两种效果，而无需计算激活数据。 \
  归一化层有什么作用？ -> 这种对极端值的非线性和不成比例的压缩效应正是归一化层的关键之处，这种压缩行为还反映了生物神经元对大输入的饱和（saturation）特性
* 115.人类秒懂，AI却懵圈：VLM²-Bench揭示视觉语言模型「视觉关联」能力短板  机器之心  https://mp.weixin.qq.com/s/1JxiDNlE3WhKue7zCAmUrQ \
  如果一项能力对人类而言是 “无需思考” 的本能，但对 AI 却是巨大挑战，它是否才是 VLMs 亟待突破的核心瓶颈？ 
* 116.游戏也可以Copilot！微软CEO纳德拉亲自站台：玩家有福了  量子位  https://mp.weixin.qq.com/s/Mcv7XsjjZzx1IZPmq3mkOw 
* 117.AI沉思：康德与AGI 1 概述 思维、大脑与深度学习  CreateAMind  https://mp.weixin.qq.com/s/9PAhdpvyAfNat8KM53ge1Q \
  Kant and Artificial Intelligence  康德与人工智能 \
  https://library.oapen.org/bitstream/handle/20.500.12657/57223/1/9783110706611.pdf \
  第一章 概述 思维、大脑与深度学习
  本文回顾了康德认知方法对认知科学诸多范式发展（如功能主义、具身认知和心智预测处理模型）产生的重要影响。第二部分探讨了人工智能最新发展引发的哲学问题与康德认知观的关联。具体而言，文章研究了在深度神经网络架构背景下关于感知、认知、学习、理解以及经验主义者与理性主义者之间古老争论的问题，以及康德认知观对这些问题的意义。
* 118.AI沉思：康德与AGI 2 自我意识引擎  CreateAMind  https://mp.weixin.qq.com/s/h29LZFxRBXyCiDEtQQVw9g \
  第二章 自我意识引擎 \
  本文描述了一项尝试，即将康德的先验心理学重新用作机器学习系统的建筑蓝图。首先，它描述了代理必须满足的条件，以实现经验的统一：直观必须通过二元关系连接，以满足各种统一条件。其次，它展示了在这个模型中范畴是如何被推导出来的：范畴是从纯粹的二元关系中推导出来的纯粹一元谓词。第三，我描述了康德的认知架构是如何被实现在一个计算机系统（自我意识引擎）中的，并详细展示了系统是如何从一连串原始感觉输入中构建统一经验的。
* 119.逐字生成非最优？试试逐「块」生成！Block Diffusion打通了自回归与扩散  机器之心  https://mp.weixin.qq.com/s/dxkz_5UWYtHLR-SpWlnqXw \
  与自回归模型相比，扩散模型具有**加速生成**和提高模型**输出可控性**的潜力。  \
  目前，离散扩散模型目前面临至少**三个限制**。首先，在聊天系统等应用中，模型必须生成任意长度的输出序列（例如对用户问题的回答）。但是，大多数最新的扩散架构仅能生成固定长度的向量。其次，离散扩散模型在生成过程中使用双向上下文，因此无法使用 KV 缓存重用以前的计算，这会降低推理效率。第三，以困惑度等标准指标衡量的离散扩散模型，质量落后于自回归方法，进一步限制了其适用性。 \
  来自 Cornell Tech、斯坦福大学、Cohere 的研究者提出通过块离散去噪扩散语言模型（Block Discrete Denoising Diffusion Language Models，BD3-LMs）来解决以上限制，该模型在扩散和自回归模型之间进行插值。 \
  扩散语言模型在并行文本生成领域正在崛起，但与自回归模型相比，它们存在质量、固定长度限制和缺乏 KV 缓存等问题。本文 Block Diffusion 将自回归和扩散模型结合了起来，实现了两全其美。 \
  总结：用自回归生成block，每一个block用扩散生成一小段文本
* 120.AI大佬曼宁转赞，MetaGPT团队首提「Atom of Thoughts」，原子化思考让4o-mini暴打推理模型？  机器之心  https://mp.weixin.qq.com/s/UPMC6aQ1zHtpHO4FL1eVdA \
  Atom of Thoughts for Markov LLM Test-Time Scaling \
  相比之下，人类推理倾向于将复杂问题拆分为独立的子问题，逐步解决并整合后续推理所需的信息，而不执着于保留每步细节。这种 “原子化思考” 启发了 AoT 的设计，使其专注当前状态的推理，摒弃历史依赖。 \
  AoT 的核心洞察是：复杂推理可通过一系列轻量的 “原子问题” 实现，这些问题的执行仅依赖自身，摆脱历史信息依赖。AoT 将推理过程构建为马尔可夫过程（Markov process），通过不断进行状态转移，逐步简化问题并始终保持和原问题等价，最终求解轻量的原子问题来回答原问题。 \
  推理过程具体是如何构建为马尔可夫过程的？
* 121.声音比真人还像真人的Maya，背后模型开源了！跨越语音恐怖谷  机器之心  https://mp.weixin.qq.com/s/kWmvvAMaqcfDGqFQALp1QQ \
  AI 公司 Sesame 发布的逼真语音助手 Maya，通过情感智能、上下文记忆和高保真语音生成技术，成功跨越了语音恐怖谷，使语音交互更加自然、情感丰富。 \
  Sesame 开源了驱动 Maya 的基础模型 CSM-1B（Conversational Speech Model） \
  项目地址：https://github.com/SesameAILabs/csmhuggingface  \
  地址：https://huggingface.co/spaces/sesame/csm-1b
* 122.信息中心法则 The Central Dogma of Information  CreateAMind  https://mp.weixin.qq.com/s/7vkT1pAjZI-Fpb5Nu4EyIg \
  “信息自创生导致内源性语义信息不可逆地成为外源性句法信息” \
  一旦由人类创造的人工句法世界（包括机器）诞生，它只能被他人解释，即它不一定对所有人传达相同的意图意义。此外，这些人工创造只能识别、提取、创造、传输、保护、存储和利用句法信息，而无法将句法信息转化为语义信息。换句话说，我们对句法创造的丰富能力并不允许我们创造出具有与我们相当的意义创造能力的人工生物。这表明，我们对感知型人工通用智能和超级智能的梦想是误入歧途的，并且与分子生物学的中心法则平行，该法则声明“一旦（顺序）信息进入蛋白质，它就无法再被提取出来”。 \
  ？？？什么是信息自创生 \
  本文分为五个部分，以更好地定义和阐述信息自创生过程的影响，即所有生物在其满足生理和/或关系需求的努力中所进行的自我参照、递归和互动的信息自我生产过程。首先，对香农通信理论的批判性回顾使我们能够识别句法信息和语义信息在信息自创生过程中的作用。意义生成（语义信息）是内源性的（内在的），属于个体的内部过程，而外源性（外在的）表达只能以句法信息的形式出现。其次，信息自创生过程被证明是将感官感知转化为意义生成（内源性语义信息）及其以多种形式外化的（外源性句法信息）的关键。第三，对生物体创造的所有外源性信息的仔细考虑导致了信息中心法则的提出。第四，通过考虑分子生物学中心法则的平行构建，可以发现两者之间存在相似之处。最后，讨论研究结果，总结并得出结论。  \
  ???什么是信息的中心法则
  信息的中心法则的一个重要含义是，它不允许具有意义生成能力的人工创造物存在，这表明我们对具有感知能力的人工通用智能的梦想是误导的。 ???
* 123.AI沉思：康德与AGI 3（自我）意识的挑战  CreateAMind  https://mp.weixin.qq.com/s/czxX4Tql-n6EUTtxhfj0FA \
  chapter 3: The Challenge of (Self-)Consciousness:Kant, Artificial Intelligence and Sense-Making \
  我们如何理解从环境中流向我们的无数信息片段？这个问题有时被称为表征问题，是认知科学中最重大的问题之一。一些开创性和重要的工作在尝试解决表征问题时借助了康德的哲学。特别是，有人建议，通过类比康德对感性和知性的区分，我们可以区分高低层次的感知，然后专注于从高层次感知到抽象认知过程的意义生成步骤。这可以通过简化低层次感知提供的输入（例如，将其简化为一串字母）来实现，计算机程序被期望“理解”这些输入。最近，更深入地审视康德的心智模型，在构建用于此类语言推理任务的程序方面取得了突破：这些类型的软件或“康德机器”似乎能够在语言推理任务中达到人类水平的表现。然而，有时这一说法更为强烈，即某些程序不仅与人类认知主体竞争，而且它们本身代表认知主体。我的论文关注这一说法；我认为这一说法是没有根据的，但对其的批判性研究可能会为创造人工智能的项目开辟新的途径。 \
  ???什么是康德机器与康德程序，与自我意识有什么关系？ \
  康德机器：一种具备“先验认知结构”的人工智能或机器，即机器在“感知世界”之前，已经内在具备了某些用于组织和解释感知的框架或规则，类似于康德哲学中的“先验综合判断”或“知性范畴”。 \
  ???什么是认知主体性？康德的认知主体性模型？
* 124.单卡3090纯视觉玩MineCraft！LS-Imagine在开放世界中基于长短期想象进行强化学习  PaperWeekly  https://mp.weixin.qq.com/s/VeSZ1Ol29B_qZ4cir-gCyQ \
  Open-World Reinforcement Learning over Long Short-Term Imagination \
  即时状态转换: 预测下一帧 \
  跳跃式状态转换: 预测跨越较长时间间隔的帧

# 3.15 Sat
* 125.0行代码打造3D游戏，氛围编码席卷全网！开发30分爆赚28万  新智元  https://mp.weixin.qq.com/s/WtjqTpMPNMHre5PPZb46Ng \
  https://github.com/David-Sola/AIGaming \
  vibe coding氛围编码 \
  「氛围编程」的核心理念是，开发者通过自然语言描述软件想法，AI工具生成相应的代码。这种方法减少了对编程技能的需求，让用户专注于功能和体验，而非底层代码的细节。Karpathy通过语音识别技术如SuperWhisper与AI交互，几乎不用键盘。AI工具可以显著加速开发过程，Karpathy展示了如何在一小时内构建一个阅读应用和一个战舰游戏。

# 3.16 Sun
* 126.层次化主动推断中的动态规划  CreateAMind  https://mp.weixin.qq.com/s/eqGDuiU2S9VXzCBAZb4lxw \
  DYNAMIC PLANNING IN HIERARCHICAL ACTIVE INFERENCE \
  ???什么是主动推断，有什么作用 \
  通过动态规划，我们指的是人脑推断和施加与认知决策相关的运动轨迹的能力。最近的范式——主动推断，为生物有机体的适应性提供了基本的见解，它们不断努力最小化预测误差，以将自身限制在与生命相容的状态中。过去几年中，许多研究表明，人类和动物的行为可以通过主动推断来解释——无论是离散的决策制定还是连续的运动控制——这为机器人技术和人工智能提供了创新的解决方案。然而，文献中仍然缺乏一种全面的视角，用于在变化的环境中有效规划现实行动。以建模复杂任务（如工具使用）为目标，我们深入探讨主动推断中的动态规划主题，同时考虑生物行为的两个关键方面：理解和利用物体操作的潜在能力（affordances）的能力，以及学习自我与环境（包括其他主体）之间的层次化互动。我们从一个简单的单元开始，逐步描述更先进的结构，比较最近提出的几种设计选择，并提供基本示例。本研究脱离了以神经网络和强化学习为中心的传统观点，并指向主动推断中尚未探索的方向：层次模型中的混合表示。 \
  ???什么是层次模型中的混合表示？
* 127.弥合差距：神经符号人工智能中的表示空间  CreateAMind  https://mp.weixin.qq.com/s/V5YSngx_QVl_-ddEzYmHXA \
  Bridging the Gap- Representation Spaces in Neuro-Symbolic \
  神经符号人工智能是一种通过结合神经网络和符号学习的优势来提升人工智能模型整体性能的有效方法。然而，二者在处理数据的方式上存在差异，主要原因是它们通常使用不同的数据表示方法，而这往往是限制二者整体性能的一个重要因素。从这个角度出发，我们通过构建一个四级分类框架，分析了2013年以来的191项研究。第一级定义了五种表示空间类型，第二级关注表示空间可以表示的五种信息模态。然后，第三级描述了四种符号逻辑方法。最后，第四级类别提出了三种神经网络与符号学习之间的协作策略。此外，我们还基于其表示空间对46项研究进行了详细分析。
* 128.真正的AI智能体时代即将到来，我们发现了几点「苦涩的教训」  机器之心  https://mp.weixin.qq.com/s/-lOj5naC2Yb3BD32YuUyOw \
  博客地址：https://vintagedata.org/blog/posts/designing-llm-agents \
  「我在周末进行的所有测试都显示出一个结果，即工作流系统存在着一些根本性局限，这些局限早在 AutoGPT 时代就已显现，而在搜索领域表现得尤为明显。」 \
  将LLM作为智能体所存在的问题：1.智能体能记住环境，但基础 LLM 不能，它们只能处理当前窗口内的信息；2.智能体受现实条件限制，但基础 LLM 生成的是概率最高的文本，随时可能「跑题」；3.智能体能规划长期策略，基础 LLM 却只能做好单步推理，面对多步推理任务很快就会「超载」。 \
  强化学习 + 推理：LLM 智能体的「成功秘诀」 \
  每一步操作和整个过程都会作为内部推理轨迹被记录下来，从而在一定程度上为搜索结果提供可解释性
* 129.新注意力让大模型上下文内存占用砍半！精度不减还能加速2倍  量子位  https://mp.weixin.qq.com/s/2avfeFbFWjjQOiQZjkHTUA \
  Slim attention: cut your context memory in half without loss of accuracy-K-cache is all you need for MHA \
  Slim Attention以标准多头注意力（MHA）为基准，对其中的value缓存处理过程进行了调整，实现了更少的内存占用，Slim Attention既可以让KV缓存大小减半，也可以在KV缓存大小不变的情况下让上下文翻倍，都不会带来精度损失。 \
  很简单但却是一个很酷的想法

# 3.17 Mon
* 130.AI沉思：康德与AGI 3（自我）意识的挑战  CreateAMind  https://mp.weixin.qq.com/s/czxX4Tql-n6EUTtxhfj0FA \
  第三章：The Challenge of (Self-)Consciousness:Kant, Artificial Intelligence and Sense-Making \
  我们如何理解从环境中流向我们的无数信息片段？这个问题有时被称为表征问题，是认知科学中最重大的问题之一。一些开创性和重要的工作在尝试解决表征问题时借助了康德的哲学。特别是，有人建议，通过类比康德对感性和知性的区分，我们可以区分高低层次的感知，然后专注于从高层次感知到抽象认知过程的意义生成步骤。这可以通过简化低层次感知提供的输入（例如，将其简化为一串字母）来实现，计算机程序被期望“理解”这些输入。最近，更深入地审视康德的心智模型，在构建用于此类语言推理任务的程序方面取得了突破：这些类型的软件或“康德机器”似乎能够在语言推理任务中达到人类水平的表现。然而，有时这一说法更为强烈，即某些程序不仅与人类认知主体竞争，而且它们本身代表认知主体。我的论文关注这一说法；我认为这一说法是没有根据的，但对其的批判性研究可能会为创造人工智能的项目开辟新的途径。 \
  设想一台康德式计算机——这是一台根据康德在“图型法”“先验演绎”和“原则的分析”中所提供的所有规则生成程序进行编程的计算机。假设它能够为语言推理任务提供良好的答案——事实上，其表现和人类一样出色。这难道不意味着计算机必须将其各种输入统一为其经验的一部分吗？这难道不意味着它可以将其“感知”视为对外部世界的表征吗？ \
  在本文中，我论证了这些隐含在修辞性问题中的主张是站不住脚的。当然，康德机器在解决语言推理任务以及其他与表征问题相关的任务中取得的成功的重要性在这里并不被质疑，而是被理所当然地接受。我所质疑的是进一步的主张，即康德机器或多或少等同于一个认知主体，即具有原始意向性的主体。问题似乎在于一个假设，即康德所提出的某些经验的必要条件被视为充分条件。 \
  我论证了通过规则进行组合对于我的经验是必要的，但并非充分。因此，为计算机配备使其能够比其他计算机更好地应对各种任务的规则或许已经足够，但从这里到让计算机将其“感知”视为对外部世界的表征之间，存在着一个至少对康德而言需要（自我）意识的鸿沟。然而，尚不清楚康德机器如何能够被视为具备这种统觉能力。不过，这或许是一个值得在认知科学和哲学中探索的方向，以解决表征问题。 \
  总结：当前的AI只有智能，没有存在意识的证据

# 3.18 Tue
* 131.上交大等提出MM-Eureka：R1-Zero的「Aha Moment」同样存在于多模态推理  PaperWeekly  https://mp.weixin.qq.com/s/a8SVFA0omcN16jXrTFfa1g \
  MM-EUREKA: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning \
  https://github.com/ModalMinds/MM-EUREKA \
  我们基于 OpenRLHF 构建了一个可扩展的多模态大规模强化学习框架，支持包括 InternVL 在内的多种模型和多种RL算法。与 R1-V 等框架相比，我们的框架具有更强的可扩展性，成功训练了 InternVL2.5-38B 等大型模型。
* 132.NAACL 2025 | 大模型离达到真正智能有多远？深度剖析大模型流体智能水平  PaperWeekly  https://mp.weixin.qq.com/s/4mrABrwTazZOrReouD_uWg \
  晶体智能（crystallized intelligence）与流体智能（fluid intelligence） \
  LLM的晶体智能很强大，缺乏流体智能 \
  Understanding  LLM s’ Fluid Intelligence Deficiency: An Analysis of the ARC Task \
  总的来说，LLM 的内部架构限制了其访问全局信息的能力，而这一能力对于展现流体智能至关重要，因此，这种限制进一步阻碍了 LLM 在流体智能方面的表现。
* 133.多模态也做到了强推理！工业界首个开源的R1V，让视觉思考进入o1时代  机器之心  https://mp.weixin.qq.com/s/IPPDuApVVKNJffAZcJqFRA \
  昆仑万维正式发布 Skywork R1V（以下简称 R1V）系列模型，实现了 SOTA 级别的视觉推理和强大的通用推理能力 \
  Hugging Face 地址：https://huggingface.co/Skywork/Skywork-R1V-38B \
  Github 地址：https://github.com/SkyworkAI/Skywork-R1V \
  技术报告地址：https://github.com/SkyworkAI/Skywork-R1V/blob/main/Skywork_R1V.pdf
* 134.单个4090就能跑，Mistral开源多模态小模型，开发者：用来构建推理模型足够香  机器之心  https://mp.weixin.qq.com/s/t173XAigACpw9NWPePhUUQ \
  法国 AI 创企 Mistral AI 开源了一个 24B 的多模态小模型，该模型在多个基准上击败了 Gemma 3 和 GPT-4o Mini 等同类模型，而且推理速度达到了 150 个 token / 秒，称得上是又好又快。 \
  只需要一个 RTX 4090 或 32GB RAM 的 Mac 就能运行，而且开源协议是 Apache 2.0，因此既能用于研究，也能商用。
* 135.本地也能运行Deep Research！支持arXiv平台，兼容PDF、Markdown等  机器之心  https://mp.weixin.qq.com/s/U5lXj0lhMR6x_Wy3GHpNfQ \
  支持云端本地LLM、本地文档检索和联网搜索的个人研究助理 \
  https://github.com/LearningCircuit/local-deep-research \
  完整报告可参考：https://github.com/LearningCircuit/local-deep-research/blob/main/examples/fusion-energy-research-developments.md

# 3.19 Wed
* 136.通过电路动态变化的视角探究学习  CreateAMind  https://mp.weixin.qq.com/s/591XC3zaOwsBEXwQEhfapA \
  Probing learning through the lens of changes in circuit dynamics
* 137.世界模型在机器人任务规划中的全新范式：NUS邵林团队提出通用机器人规划模型FLIP  机器之心  https://mp.weixin.qq.com/s/53yrBcN9ec6ypf0NRsHjRQ \
  FLIP : Flow-Centric Generative Planning as General-Purpose Manipulation World Model \
  项目主页：https://nus-lins-lab.github.io/flipweb/ \
  代码链接：https://github.com/HeegerGao/FLIP
* 138.240元打造擅长数学的多模态版R1，基于DeepSeek核心思想，两阶段训练提升推理能力至工业级应用标准  量子位  https://mp.weixin.qq.com/s/7_55y_dTroQ4bWpJsgdXDg \
  这是来自东南大学、香港中文大学、蚂蚁集团等研究人员的，两阶段多模态基于规则强化学习的框架LMM-R1，实现多模态大模型的推理性能飞跃。实验数据显示，经LMM-R1框架强化的QwenVL-2.5-3B模型，在推箱子等复杂路径规划任务中，性能显著超越GPT-4o、Claude3.5等100B+参数量产品级大模型。
  项目主页：https://forjadeforest.github.io/LMM-R1-ProjectPage/ \
  项目地址：https://github.com/TideDra/lmm-r1 \
  将DeepSeek-R1的成功经验扩展到多模态领域面临两大关键挑战： \
  1.数据限制：多模态领域中高质量的推理数据十分稀缺，且答案常常模糊不清，难以用于规则奖励 \
  2.基础推理能力薄弱：多模态预训练常常会削弱模型在纯文本任务上的能力，特别是对于参数量有限的小模型 \
  LMM-R1框架包含两个精心设计的阶段： \
  第一阶段：基础推理增强（FRE） 用纯文本推理数据训练 \
  第二阶段：多模态泛化训练（MGT）
* 139.意识研究是“科学”还是“伪科学”？两大意识范式的交锋  集智俱乐部  https://mp.weixin.qq.com/s/7nEDwNJlPGlDSc56NhyDXQ \
  整合信息理论支持者 vs 计算功能主义支持者
* 140.124位科学家批评整合信息论是伪科学：我们该如何探讨意识难题？  集智俱乐部  https://mp.weixin.qq.com/s/OPooY17rSFNPZEjzaMuJQA \
  自指可能是解决意识的一把钥匙，因为它几乎可以和上述的所有理论联系起来。 \
  1.对于更高层次理论来说，泛函动力学的实验[13]说明：自指映射可以自然地涌现出符号，即更高层次理论中所说的「元表示」； \
  2.对于全局工作空间来说，同样是泛函动力学：自指映射可以自发的把功能和数据拆分开，这与全局空间对应了起来； \
  3.对于整合信息理论，自我指涉，本身就是一种回路，有很大希望得到一个比较高的Phi； \
  4.而对于再入与预测加工理论，关系则更为密切：如果一个系统要预测其自身，就需要对自身建立一个抽象的模型，如此，自指便自然产生了。
* 141.丹尼尔·丹尼特：从「笛卡儿剧场」到「多重草稿」| 《意识的解释》  集智俱乐部  https://mp.weixin.qq.com/s/VIWznPGPxQ71P_s023sK6w 

# 3.20 Thur
* 142.数学真理的极限在哪里？希尔伯特第十问题扩展版得到证明  图灵人工智能  https://mp.weixin.qq.com/s/wtWG_0RSuaIMeFiVSX7xXw \
  所有知识都有极限。有些事情是无法做到的 —— 无论你是谁，无论你有怎样的身份或才智。
* 143.英伟达开源通用机器人大模型—GR00T N1  AIGC开放社区  https://mp.weixin.qq.com/s/7yVTyjJq-4gMraFPBO04EQ \
  开源地址：https://huggingface.co/nvidia/GR00T-N1-2B \
  该模型能够处理多模态数据，包括语言、图像、视频，可在家务、工厂等多样化环境中执行复杂操作任务。值得一提的是，GR00T N1的核心架构采用了模拟人类思维的“快慢思考”模式，可以让机器人的做法、思维更像人类，从而提升动作指令准确率。 \
  视觉-语言模块是 GR00T N1 的“大脑”，负责处理和理解输入的图像与语言指令。 \
  扩散变换器模块（System 1）则相当于 GR00T N1 的“四肢”，负责根据视觉-语言模块提供的信息生成具体的动作指令。
* 144.迈向生物智能理论    Towards a theory of biological intelligence  CreateAMind  https://mp.weixin.qq.com/s/J6k_7CtuK4N6rdmqefOF3A \
  智力与感知辨别能力之间存在一个尚未充分研究的相关性。感知辨别可以通过预测处理中的“精确度”这一概念来理解。精确度决定了感觉输入和预测误差的权重，塑造了个体从环境中提取有意义信息的有效性。我们在这里提出了智力与精确度之间的联系，认为动态调节精确度的能力是智能行为的关键决定因素。这一观点将个体认知差异与人类大脑功能的广泛理论模型联系起来，为理解生物系统中智力的表现提供了更全面的理解。
* 145.基于Slot Attention的对象中心学习  CreateAMind  https://mp.weixin.qq.com/s/vBA-ns6QmGxQ5PGcj8gD6w \
  Object-Centric Learning with Slot Attention \
  学习复杂场景的以对象为中心的表示是实现从低级感知特征中进行高效抽象推理的有前途的一步。然而，大多数深度学习方法学习到的是分布式的表示，这些表示无法捕捉自然场景的组合性质。在本文中，我们提出了槽注意力（Slot Attention）模块，这是一个架构组件，它与感知表示（例如卷积神经网络的输出）进行交互，并生成一组任务相关的抽象表示，我们称之为槽。这些槽是可互换的，可以通过多轮注意力的竞争过程来专门化，从而绑定到输入中的任何对象。我们通过实证研究证明，槽注意力能够提取以对象为中心的表示，当在无监督的对象发现任务和有监督的属性预测任务上进行训练时，能够实现对未见组合的泛化。
* 146.被谷歌点名感谢！杭州六小龙开源黑科技，让机器人瞬间「悟透」3D世界  新智元  https://mp.weixin.qq.com/s/phwIARhke5SC0PscYGqEzQ \
  群核科技发了一个空间理解开源模型SpatialLM
* 147.
* 148.
* 149.
* 150.

# 3.21 Fri
# 3.22 Sat
# 3.23 Sun
# 3.24 Mon
# 3.25 Tue
# 3.26 Wed
# 3.27 Thur
# 3.28 Fri
# 3.29 Sat
# 3.30 Sun
# 3.31 Mon
