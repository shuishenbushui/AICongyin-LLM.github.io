# 1.1 Wed
* 1.Meta首席AI科学家LeCun访谈：为什么他一直认为AI还不如一只猫？  图灵人工智能  https://mp.weixin.qq.com/s/Ny6x0sKuakRFxPsE5Ali7A 

# 1.2 Thur
* 2.4o-mini只有8B，o1也才300B！微软论文意外曝光GPT核心机密  量子位  https://mp.weixin.qq.com/s/bT_w-T9ElmPUXbYA1f7kCg 
* 3.比人类神经元快10亿倍！港中文、中科院「超级大脑」：1秒识图3479万张  新智元  https://mp.weixin.qq.com/s/5L1cOPpwQByX1NBPXrV6ig \
  Integrated laser graded neuron enabling high-speed reservoir computing without a feedback loop \
  这个基于芯片的量子点激光器，不仅能完全模仿真实神经细胞功能，更实现了惊人的速度——即10GBaud信号处理速度，也就意味着它比生物神经元快整整10亿倍。

# 1.3 Fri
* 4.(**值得了解**)突破算力限制！Meta开源“记忆层”，重塑Transformer架构大模型  AIGC开放社区  https://mp.weixin.qq.com/s/Zv2oyzLb4bIaq9FrzK3GXw \
  Meta探索大模型记忆层，扩展至1280亿个参数，优于MoE  机器之心  https://mp.weixin.qq.com/s/R5JOMkLVbbI7yPJmdcT2pQ \
  Memory Layers at Scale \
  全球社交巨头Meta分享了一个创新研究——Memory layers（记忆层） \
  目前，Transformer架构的预训练大模型在存储、查询数据时，随着参数的变大对算力的需求呈指数级增长。“记忆层”提出了新的高效查询机制替代了传统的查询方法，通过比较查询键与两个较小集合中的键，可以快速找到最相关的键，而无需遍历模型的整个记忆层。这也就是说，可以在不增加算力的情况下显著增加大模型的参数。例如，研究人员在仅有1.3亿参数的模型中添加了128亿额外的记忆参数，其性能与Meta开源的Llama 2- 70相当，而算力却比它低了10倍左右。\
  开源地址：https://github.com/facebookresearch/memory
* 5.o3来了，通用人工智能真的触手可及吗？  追问  https://mp.weixin.qq.com/s/IYDV5tSDZvOD-aQm2xD4Kw \
  我认为生物系统与由其他材料制成的系统之间不存在任何特别的差异，能够阻止非生物系统变得智能
* 6.(**值得了解，上下文抽象学习**)数据不够致Scaling Law撞墙？CMU和DeepMind新方法可让VLM自己生成记忆 
 机器之心  https://mp.weixin.qq.com/s/bSE8DGf0fkNKhPhe8H4HoA \
  VLM 智能体生成自己的记忆：将经验蒸馏成具身思维程序 \
  VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought \
  https://ical-learning.github.io/ \
  https://github.com/Gabesarch/ICAL \
  In-Context Abstraction Learning（ICAL），即上下文抽象学习
* 7.(**看一看**)如何复现o1模型？复旦最新《搜索与学习的扩展：从强化学习角度重现o1的路线图》  专知  https://mp.weixin.qq.com/s/3tWV5ykUVsI18M7jeDB2-A \
  OpenAI最大秘密，竟被中国研究者破解？复旦等惊人揭秘o1路线图  新智元  https://mp.weixin.qq.com/s/IOKFBgoWyietVe3NNNw9Hg \
  Scaling of Search and Learning: A Roadmap to Reproduce ol from Reinforcement Learning Perspective

# 1.4 Sat
* 8.从理论到实践，中科大、讯飞发布SocraticLM：首个实现苏格拉底教学法的智能辅学大模型  PaperWeekly  https://mp.weixin.qq.com/s/YbyZblgWetWI6RDp45NjLg \
  SocraticLM: Exploring Socratic Personalized Teaching with Large Language Models \
  https://github.com/Ljyustc/SocraticLM
* 9.LoRA进展有哪些？最新《基础模型的低秩适应》综述  专知  https://mp.weixin.qq.com/s/2XQF93S0v9Ofn8LTILQmNg \
  Low-Rank Adaptation for Foundation Models: A Comprehensive Review
* 10.Just keep scaling！思维链作者Jason Wei 40分钟讲座剖析LLM扩展范式  机器之心  https://mp.weixin.qq.com/s/3d2eE_uAPWOY289MCLn09A \
  视频地址：https://www.youtube.com/watch?v=yhpjpNXJDco \
  幻灯片：https://llm-class.github.io/slides/Jason_Wei.pdf 
* 11.从2019年到现在，是时候重新审视Tokenization了  机器之心  https://mp.weixin.qq.com/s/zmeFYfxWD1nZq_MocgGeeQ \
  https://huggingface.co/spaces/huggingface/number-tokenization-blog

# 1.5 Sun
* 12.大规模视觉-语言模型的对齐与失齐：从可解释性的视角进行的综述  专知  https://mp.weixin.qq.com/s/lwJXA_Tz7kLcHd1kLglDrA \
  Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability 
* 13.斯坦福打脸大模型数学水平：题干一改就集体降智，强如o1也失准，能力涌现怕不是检索题库  量子位  https://mp.weixin.qq.com/s/GY9rqxXLyujEes2H3ycMDQ \
  Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning

# 1.6 Mon
* 14.
* 15.
* 16.
* 17.
* 18.
* 19.
* 20.

# 1.7 Tue
# 1.8 Wed
# 1.9 Thur
# 1.10 Fri
# 1.11 Sat
# 1.12 Sun

# 1.13 Mon
# 1.14 Tue
# 1.15 Wed
# 1.16 Thur
# 1.17 Fri
# 1.18 Sat
# 1.19 Sun

# 1.20 Mon
# 1.21 Tue
# 1.22 Wed
# 1.23 Thur
# 1.24 Fri
# 1.25 Sat
# 1.26 Sun

# 1.27 Mon
# 1.28 Tue
# 1.29 Wed
# 1.30 Thur
# 1.31 Fri
