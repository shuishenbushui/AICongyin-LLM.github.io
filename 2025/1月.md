# 1.1 Wed
* 1.Meta首席AI科学家LeCun访谈：为什么他一直认为AI还不如一只猫？  图灵人工智能  https://mp.weixin.qq.com/s/Ny6x0sKuakRFxPsE5Ali7A 

# 1.2 Thur
* 2.4o-mini只有8B，o1也才300B！微软论文意外曝光GPT核心机密  量子位  https://mp.weixin.qq.com/s/bT_w-T9ElmPUXbYA1f7kCg 
* 3.比人类神经元快10亿倍！港中文、中科院「超级大脑」：1秒识图3479万张  新智元  https://mp.weixin.qq.com/s/5L1cOPpwQByX1NBPXrV6ig \
  Integrated laser graded neuron enabling high-speed reservoir computing without a feedback loop \
  这个基于芯片的量子点激光器，不仅能完全模仿真实神经细胞功能，更实现了惊人的速度——即10GBaud信号处理速度，也就意味着它比生物神经元快整整10亿倍。

# 1.3 Fri
* 4.(**值得了解**)突破算力限制！Meta开源“记忆层”，重塑Transformer架构大模型  AIGC开放社区  https://mp.weixin.qq.com/s/Zv2oyzLb4bIaq9FrzK3GXw \
  Meta探索大模型记忆层，扩展至1280亿个参数，优于MoE  机器之心  https://mp.weixin.qq.com/s/R5JOMkLVbbI7yPJmdcT2pQ \
  Memory Layers at Scale \
  全球社交巨头Meta分享了一个创新研究——Memory layers（记忆层） \
  目前，Transformer架构的预训练大模型在存储、查询数据时，随着参数的变大对算力的需求呈指数级增长。“记忆层”提出了新的高效查询机制替代了传统的查询方法，通过比较查询键与两个较小集合中的键，可以快速找到最相关的键，而无需遍历模型的整个记忆层。这也就是说，可以在不增加算力的情况下显著增加大模型的参数。例如，研究人员在仅有1.3亿参数的模型中添加了128亿额外的记忆参数，其性能与Meta开源的Llama 2- 70相当，而算力却比它低了10倍左右。\
  开源地址：https://github.com/facebookresearch/memory
* 5.o3来了，通用人工智能真的触手可及吗？  追问  https://mp.weixin.qq.com/s/IYDV5tSDZvOD-aQm2xD4Kw \
  我认为生物系统与由其他材料制成的系统之间不存在任何特别的差异，能够阻止非生物系统变得智能
* 6.(**值得了解，上下文抽象学习**)数据不够致Scaling Law撞墙？CMU和DeepMind新方法可让VLM自己生成记忆 
 机器之心  https://mp.weixin.qq.com/s/bSE8DGf0fkNKhPhe8H4HoA \
  VLM 智能体生成自己的记忆：将经验蒸馏成具身思维程序 \
  VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought \
  https://ical-learning.github.io/ \
  https://github.com/Gabesarch/ICAL \
  In-Context Abstraction Learning（ICAL），即上下文抽象学习
* 7.(**看一看**)如何复现o1模型？复旦最新《搜索与学习的扩展：从强化学习角度重现o1的路线图》  专知  https://mp.weixin.qq.com/s/3tWV5ykUVsI18M7jeDB2-A \
  OpenAI最大秘密，竟被中国研究者破解？复旦等惊人揭秘o1路线图  新智元  https://mp.weixin.qq.com/s/IOKFBgoWyietVe3NNNw9Hg \
  Scaling of Search and Learning: A Roadmap to Reproduce ol from Reinforcement Learning Perspective

# 1.4 Sat
* 8.从理论到实践，中科大、讯飞发布SocraticLM：首个实现苏格拉底教学法的智能辅学大模型  PaperWeekly  https://mp.weixin.qq.com/s/YbyZblgWetWI6RDp45NjLg \
  SocraticLM: Exploring Socratic Personalized Teaching with Large Language Models \
  https://github.com/Ljyustc/SocraticLM
* 9.LoRA进展有哪些？最新《基础模型的低秩适应》综述  专知  https://mp.weixin.qq.com/s/2XQF93S0v9Ofn8LTILQmNg \
  Low-Rank Adaptation for Foundation Models: A Comprehensive Review
* 10.Just keep scaling！思维链作者Jason Wei 40分钟讲座剖析LLM扩展范式  机器之心  https://mp.weixin.qq.com/s/3d2eE_uAPWOY289MCLn09A \
  视频地址：https://www.youtube.com/watch?v=yhpjpNXJDco \
  幻灯片：https://llm-class.github.io/slides/Jason_Wei.pdf 
* 11.从2019年到现在，是时候重新审视Tokenization了  机器之心  https://mp.weixin.qq.com/s/zmeFYfxWD1nZq_MocgGeeQ \
  https://huggingface.co/spaces/huggingface/number-tokenization-blog

# 1.5 Sun
* 12.大规模视觉-语言模型的对齐与失齐：从可解释性的视角进行的综述  专知  https://mp.weixin.qq.com/s/lwJXA_Tz7kLcHd1kLglDrA \
  Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability 
* 13.斯坦福打脸大模型数学水平：题干一改就集体降智，强如o1也失准，能力涌现怕不是检索题库  量子位  https://mp.weixin.qq.com/s/GY9rqxXLyujEes2H3ycMDQ \
  Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning

# 1.6 Mon
* 14.内在主动推理1：基本原理  CreateAMind  https://mp.weixin.qq.com/s/KLc5kXehLofcil6JMcMxpw \
  Intra-Active Inference I: Fundamentals \
  完全不懂
* 15.仅需一万块钱！清华团队靠强化学习让 7B模型数学打败GPT-4o  量子位  https://mp.weixin.qq.com/s/s-DeQCAX1gth82YkABxLLA \
  1/10训练数据超越GPT-4o！清华等提出隐式过程奖励模型PRIME，在线刷SOTA 
 新智元  https://mp.weixin.qq.com/s/bogt5zl7rytcz-FhNECTNg \
  清华大学NLP实验室、上海AI Lab、清华大学电子系、OpenBMB社区等团队提出一种新的结合过程奖励的强化学习方法——PRIME（Process Reinforcement through IMplicit REwards）\
  Process Reinforcement through Implicit Rewards \
  https://github.com/PRIME-RL/PRIME \
  blog链接：https://curvy-check-498.notion.site/Process-Reinforcement-through-Implicit-Rewards-15f4fcb9c42180f1b498cc9b2eaf896f

# 1.7 Tue
* 16.AGI理论比较：主动推理、强化学习、控制论、贝叶斯大脑、效用决策、有限理性、情感动机、动态体内平衡  CreateAMind  https://mp.weixin.qq.com/s/egj-2woxVpdA8XuGcOVaeQ \
  Active-InferenceThe-Free-Energy-Principle-in-Mind 
* 17.伯克利最新《机器人》课程  专知  https://mp.weixin.qq.com/s/1lo_pHSYiXiPKtoZK8K2Qw 
* 18.单张图像探索3D奇境：Wonderland让高质量3D场景生成更高效  机器之心  https://mp.weixin.qq.com/s/ViSQcx3UNLjRb7goIRhQFg \
  Wonderland: Navigating 3D Scenes from a Single lmage \
  项目主页：https://snap-research.github.io/wonderland/
* 19.手机「自动驾驶」大揭秘！vivo万字综述探讨大模型手机自动化  机器之心  https://mp.weixin.qq.com/s/Cmq4qidvlLB5ZL5OZ90uoA \
  LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects
* 20.(**非常值得深入研究！！！**)Tokenization，再见！Meta提出大概念模型LCM，1B模型干翻70B？  新智元  https://mp.weixin.qq.com/s/TpfRePjwzHdSIXJYCDCIJw \
  Meta提出大概念模型，抛弃token，采用更高级别的「概念」在句子嵌入空间上建模，彻底摆脱语言和模态对模型的制约 \
  受人类构思交流的高层级思路启发，Meta AI研究员提出全新语言建模新范式「大概念模型」，解耦语言表示与推理 \
  网友Chuby兴奋地表示：「如果Meta的大概念模型真的有用，那么同等或更高效率的模型，其规模将更小。比如说1B模型将堪比70B的Llama 4。进步如此之大！」 \
  新系统将不再单纯基于下一个token预测，而是像婴儿和小动物那样通过观察和互动来理解世界 \
  简而言之，「大概念模型」（LCM）是在「句子表示空间」对推理（reasoning）建模，抛弃token，直接操作高层级显式语义表示信息，彻底让推理摆脱语言和模态制约 \
  Large Concept Models \
  代码链接：https://github.com/facebookresearch/large_concept_model
* 21.学习离散世界模型用于启发式搜索  CreateAMind  https://mp.weixin.qq.com/s/UF06zEVXDESdl72u-afTVA \
  Learning Discrete World Models for Heuristic Search

# 1.8 Wed
* 22.(**值得试试**)机器人ChatGPT时刻！英伟达开源世界大模型，完美模拟物理世界！  AIGC开放社区  https://mp.weixin.qq.com/s/UdgdJjdpRzFj3z8HBc5sOg \
  英伟达「世界基础模型」诞生，引爆物理AI革命！75页报告出炉，GitHub狂飙2k星  新智元  https://mp.weixin.qq.com/s/wJRDkwt-XvxJR5JhYLU6ZQ \
  NVIDIA Cosmos \
  一共包含了四大功能模块：扩散模型、自回归模型、视频分词器，以及视频处理与编辑流程 \
  开源项目：https://github.com/NVIDIA/Cosmos \
  Cosmos World Foundation Model Platform for Physical AI
* 23.(**值得看看**)用于人工通用智能（AGI）的大型语言模型：基础原则和方法综述  专知  https://mp.weixin.qq.com/s/oa_5OIBBlw4cnkJMVk-tnw \
  Large language models for artificial general inteligence(AGl): A survey of foundationa.principles and approaches \
  使LLM达到AGI还需要的解决的问题
* 24.最新AI Agent万字综述分享！  Datawhale  https://mp.weixin.qq.com/s/HrKOGXuI8wVM6qgylKZ8qw \
  视频链接：https://www.bilibili.com/video/BV17wrpYKE4V \
  百宝箱地址： https://tbox.alipay.com/pro-about
* 25.纯视觉方案，精准操控电脑和手机！港大Aria-UI登顶，超越Claude 3.5  新智元  https://mp.weixin.qq.com/s/2U4A6gYI5_G-k-jFzWMz8w \
  GitHub仓库：https://github.com/AriaUI/Aria-UI \
  项目主页：https://ariaui.github.io \
  Aria-UI: Visual Grounding for GUI Instructions
* 26.o1也会「想太多」？腾讯AI Lab与上海交大揭秘o1模型过度思考问题  机器之心  https://mp.weixin.qq.com/s/ofTfoFNIgGO2ZIsWVnjfAA \
  Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs
* 27.老婆饼里没有老婆，RLHF里也没有真正的RL  机器之心  https://mp.weixin.qq.com/s/dVCD0crxwCThE7-XVvoemg \
  博客链接：https://www.linkedin.com/pulse/why-rlhf-other-rl-like-methods-dont-bring-true-rl-llmsand-atlas-wang-s1efc/ \
  Why RLHF (and Other RL-Like Methods) Don' t Bring "True RL" to LLMs-and Why It Matters \
  RLHF（基于人类反馈的强化学习）和其他类似的方法并没有为大型语言模型（LLM）带来真正的强化学习（RL），因为它们缺乏 RL 的核心特征：持续的环境交互和长期目标的追求 \
  虽然 RLHF、DPO 和相关方法提供了一种实用的方法，可以在短期环境中使 LLM 与人类偏好保持一致，但它们无法赋予 LLM 真正、持久的目标或意图。这些方法也只是与经典的 RL 或 IRL 范式略有对应。未来的系统若能在真正的多步骤 RL 循环中使用 LLM，就能解锁更多自主的、类似智能体的行为，但同时也会引发新的安全和一致性问题

# 1.9 Thur
* 28.重磅！微软开源最强小模型Phi-4，超GPT-4o、可商用  AIGC开放社区  https://mp.weixin.qq.com/s/Rk-XBLd3PosXCJhH_mth1w \
  开源地址：https://huggingface.co/microsoft/phi-4/tree/main 
* 29.引领人机交互革命？微软研究团队发布80页的大模型GUI智能体综述  机器之心  https://mp.weixin.qq.com/s/WhWGjLrYraomGz7yI8R5_A \
  Large Language Model-Brained GUl Agents A Survey
* 30.具身智能新高度！智元机器人推出全球首个4D世界模型EnerVerse  机器之心  https://mp.weixin.qq.com/s/B6MfkL_SxZ5BrR--PYqYiQ \
  主页地址：https://sites.google.com/view/enerverse/home \
  EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation
* 31.「知识蒸馏+持续学习」最新综述！哈工大、中科院出品：全新分类体系，十大数据集全面实验  新智元  https://mp.weixin.qq.com/s/1vX-wH38uNqo-YMKRmvLrQ \
  Continual Learning With Knowledge Distillation:A Survey

# 1.10 Fri
* 32.计算之物理：灵魂耗能吗？  图灵人工智能  https://mp.weixin.qq.com/s/c1OR1Z4ogC2rqFR-NVk7ZA \
  西拉德的智能体（intelligent being）到底是什么？那时还没有图灵机。西拉德把麦克斯韦的多分子简化为单个分子。西拉德的思想实验可以把信息转化为能量。西拉德认为信息的获取，即测量，需要付出代价，即消耗能量，从而使得周边环境的熵增加
* 33.图灵奖得主杨立昆最新万字实录：我们离真正的Agent还差几大关键技术  图灵人工智能  https://mp.weixin.qq.com/s/7wUd3zoh8rvIaOa9kxBx0w 
* 34.(**值得了解，小模型深度思考能力**)重大突破！微软发布“自我进化”，帮小模型超OpenAI-o1  AIGC开放社区  https://mp.weixin.qq.com/s/WDJRcLUFwQe3cLJj5ToLPQ \
  让Qwen2.5 7B超越o1，微软干的！MSRA推出小模型数学推理自我进化新方法  量子位  https://mp.weixin.qq.com/s/URAhVZkVcMQBRnBdd-7F9Q \
  7B模型数学推理击穿o1，直逼全美20%尖子生！四轮进化，微软华人新作爆火  新智元  https://mp.weixin.qq.com/s/KKoQILXVstwE-BAN_-VCvg \
  微软亚洲研究院发布了一种创新算法——rStar-Math \
  rStar-Math通过代码增强CoT、蒙特卡洛树搜索等，可以帮助小参数模型在不依赖老师模型蒸馏的情况下，实现多轮自我思维深度进化，极大增强模型的数学推理能力 \
  代码地址: https://github.com/microsoft/rStar \
  rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking \
  rStar-Math核心在于，让小模型具备「深度思考」的能力。团队借鉴了AlphaGo中蒙特卡洛树搜索（MCTS）技术，设计了一个由2个协同工作的SLM组成的系统：一个数学策略小语言模型（SLM）一个过程奖励模型（PRM） \
  通过代码增强CoT、蒙特卡洛树搜索（MCTS）等，**rStar-Math能让小·大模型在不依赖蒸馏教师模型的情况下，通过多轮自我进化的深度思维，掌握数学推理**

# 1.11 Sat
* 35. GAN已死？GAN万岁！布朗康奈尔新作爆火，一夜碾压扩散模型  新智元  https://mp.weixin.qq.com/s/48oNvSSzrj7rX2wz-MYsaQ \
  GAN已死？不，它卷土重来了！布朗大学和康奈尔大学的研究者刚刚提出了R3GAN，充分利用现代架构设计，彻底摒弃临时技巧，一半参数就能碾压扩散模型。网友惊呼：游戏规则要改变了！\
  The GAN is dead; long live the GAN! A Modern Baseline GAN
* 36.(**非常值得看看**)迈向System 2推理，100页论文硬核讲述Meta-CoT  机器之心  https://mp.weixin.qq.com/s/L_tErITBzUZ75GVGtbtdDQ \
  o1推理框架最新成果：斯坦福&伯克利提出元链式思维，升级模型推理能力  量子位  https://mp.weixin.qq.com/s/-MOS6jshCM0aJdhQBAVJ8g \
  Meta-CoT 通过显式建模生成特定思维链（CoT）所需的底层推理过程，扩展了传统的思维链方法 \
  元链式思维（Meta-CoT）是通往超级智能（Superintelligence）的正确道路。下一波人工智能就是元链式思维（Meta-CoT）循环 \
  Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought \
  Rafailov 所说的 Meta-CoT，是一种新颖的框架，它通过显式建模生成特定思维链（CoT）所需的底层推理过程，扩展了传统的思维链方法 \
  该研究从认知科学的双过程理论中汲取灵感，将 Meta-CoT 框架看作为一种 System 2 推理形式。本文奠定了 Meta-CoT 理论基础，展示了如何通过系统搜索过程实现这一框架，以及如何将这些过程内化到一个单一的自回归模型中。随后，本文提供了实证证据，包括对 OpenAI 的 o1 和 DeepSeek-R1 等顶尖模型的分析，这些模型展现出了与内化（上下文）搜索一致的行为。接着本文进一步探索了通过过程监督来训练 Meta-CoT 模型的方法，以及通过蒙特卡洛树搜索（MCTS）和 A * 等搜索算法生成合成数据的技术 \
  作者认为传统的思维链并不能完全代表推理问题背后的数据生成过程。通过融入搜索、验证和迭代优化的概念，Meta-CoT 为高级问题解决所需的认知过程提供了一个更完整的模型

# 1.12 Sun
* 37.(**非常值得一试**)450美元训练一个「o1-preview」？UC伯克利开源32B推理模型Sky-T1，AI社区沸腾了  机器之心  https://mp.weixin.qq.com/s/aRUHeDheE4nwncbCLakgIQ \
  项目主页：https://novasky-ai.github.io/posts/sky-t1/ \
  开源地址：https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview \
  Sky-T1-32B-Preview 的训练成本不到 450 美元，这表明可以经济、高效地复制高级推理能力 \
  Sky-T1 似乎是第一个真正开源的推理模型，因为团队发布了训练数据集以及必要的训练代码，任何人都可以从头开始复制

# 1.13 Mon
* 38.辛顿与李飞飞对谈：AI是否具备语言理解和推理能力？  图灵人工智能  https://mp.weixin.qq.com/s/pwWP-avZxjeB0mW8Awb7kw 
* 39.(**值得了解**)AI博士智能体自主科研，o1-preview封神成本暴降84%！AMD霍普金斯新作爆火  新智元  https://mp.weixin.qq.com/s/cndCOqRu1d_sIPDmOfafxA \
  Hyperbolic联创Jasper Zhang在采访中称，AI智能体已经可以自主租用GPU，利用PyTorch进行开发了 \
  Agent Laboratory: Using LLM Agents as Research Assistants \
  Agent Laboratory有三个关键阶段：文献综述、实验设计和报告撰写 \
  由LLM驱动的专业智能体（如博士、博士后等）协同工作，承担文献综述、实验规划、数据准备和结果解释等工作。这些智能体还会集成arXiv、Hugging Face、Python和LaTeX等外部工具，来优化结果
* 40.在线持续学习：方法、挑战与基准的系统性文献综述，46页pdf  专知  https://mp.weixin.qq.com/s/UrJkitQSRqWlRWXL1TjbfQ \
  Online Continual Learning: A Systematic Literature Review of Approaches, Challenges, and Benchmarks
* 41.AI4Physics？【MIT博士论文】探索物理建模与表示学习的交汇点  专知  https://mp.weixin.qq.com/s/ZMhMmsh1LOwJmQjAG8VoIA \
  Exploring the Intersection of Physics Modeling and Representation Learning
* 42.聊一聊PRM（过程奖励模型）  关于NLP那些你不知道的事  https://mp.weixin.qq.com/s/5DHYmOxyAmuj7SNVGHvLwQ 
* 43.神经符号编程在 Scallop 中的原理与实践（上,1-4章）  CreateAMind  https://mp.weixin.qq.com/s/BYOAaKYHccJC2eM-m58Vaw \
  神经符号编程在 Scallop 中的原理与实践 5-7章：  CreateAMind  https://mp.weixin.qq.com/s/lhPxhcm467pRwjnkbJYwog \
  Neurosymbolic Programming inScallop: Principles and Practice \
  神经符号编程结合了深度学习和符号推理这两个互补的领域，从而为人工智能任务提供了更准确、可解释且具有领域感知能力的解决方案 \
  ???不明白神经符号编程有什么意义

# 1.14 Tue
* 44.抢先OpenAI“虚拟员工”！清华复旦斯坦福联手，让Agent接管电脑帮你工作 
 量子位  https://mp.weixin.qq.com/s/tUYjchA4ySYmdDIAB8FV6g \
  一句话让Agent自主干活，清华复旦斯坦福等开源的智能体开发框架抢先了OpenAI  机器之心  https://mp.weixin.qq.com/s/Ia0xrgkUrUljGeIxuN7FpQ \
  提出了一个名为“Eko”的Agent开发框架，开发者只需用简洁的代码和自然语言，就能快速构建可用于生产的“虚拟员工”：Agent可以接管用户的电脑和浏览器，代替人类完成各种任务 \
  Homepage：https://eko.fellou.ai/ \
  Github link：https://github.com/FellouAI/eko \
  Docs：https://eko.fellou.ai/docs/
* 45.(**非常值得研究**)谷歌新架构一战成名，打破Transformer记忆瓶颈，姚班校友钟沛林新作  量子位 
  https://mp.weixin.qq.com/s/APE_CJ4rEQYV8ngyaTAXWw \
  谷歌新架构终结Transformer，长序列处理王者诞生？清华姚班校友新作  新智元  https://mp.weixin.qq.com/s/LnUchUuiJvQBX1zx0cZnpw \
  近8年后，谷歌Transformer继任者「Titans」来了，上下文记忆瓶颈被打破  机器之心  https://mp.weixin.qq.com/s/EUCZ1oSuyzR9M9X9r5SYBw \
  Titans: Learning to Memorize at Test Time \
  https://github.com/lucidrains/titans-pytorch \
  Titans 比 Transformers 和现代线性 RNN 更高效，并且可以有效地扩展到超过 200 万上下文窗口，性能比 GPT4、Llama3 等大模型更好 \
  他还解释了这篇研究的动机，团队认为 Transformer 中的注意力机制表现为短期记忆，因此还需要一个能记住很久以前信息的神经记忆模块 \
  谷歌提出了一种新的长期神经记忆模块（neural memory module），它能够学习记忆历史上下文，并帮助注意力机制在利用过去已久信息的同时处理当前上下文 \
  从记忆的角度来看，谷歌认为注意力机制虽然受限于上下文但可以更准确地建模依赖关系，因此可以起到短期记忆的作用；而神经记忆能够对数据进行记忆，起到了长期、更持久的记忆作用。基于这两个模块，谷歌引入了一个全新的系列架构 —— Titans，通过三种变体有效地将记忆融合到该系统架构中，它们分别是记忆作为上下文（Memory as a Context，MAC）、记忆作为门（Memory as a Gate，MAG）和记忆作为层（Memory as a Layer，MAL）
* 46.微软华人团队最新研究：从LLM到LAM，让大模型真正具有「行动力」！  新智元  https://mp.weixin.qq.com/s/BVFV8v6KGcpKoz_TYdBdAA \
  Large Action Models: From Inception to lmplementation 
* 47.余弦相似度可能没用？对于某些线性模型，相似度甚至不唯一  机器之心  https://mp.weixin.qq.com/s/xlcHdXdjan9Ll2YD0mu3WA \
  Is Cosine-Similarity of Embeddings Really About Similarity?
* 48.思维链？思维树？华为诺亚：现在到了思维森林时刻！  机器之心  https://mp.weixin.qq.com/s/iWgpcXnGuVgbondK2JrLIQ \
  Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning \
  项目链接：https://github.com/iamhankai/Forest-of-Thought
* 49.SNeRL:用于强化学习的语义感知神经辐射场  CreateAMind  https://mp.weixin.qq.com/s/NiUUYCM-cz7RMb4LetaQfQ \
  SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning

# 1.15 Wed
* 50.微软开源AI Agent大更新，重塑智能体！多层级更强架构  AIGC开放社区  https://mp.weixin.qq.com/s/TlGdQt86yhWI-asBpy9lRw \
  https://github.com/microsoft/autogen
* 51.仅缩小视觉Token位置编码间隔，轻松让多模态大模型理解百万Token！清华大学，香港大学，上海AI Lab新突破  机器之心  https://mp.weixin.qq.com/s/mD0WQe4KBrDVCJdTtpk5Bw \
  V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding \
  项目主页：https://zzdhybthu.github.io/V2PE.github.io/ \
  开源代码：https://github.com/OpenGVLab/V2PE
* 52.MiniMax震撼开源，突破传统Transformer架构，4560亿参数，支持400万长上下文  机器之心  https://mp.weixin.qq.com/s/O7xaTDWmuZfzMUMTBWOhBw 
* 53.Transformer²要做「活」的AI模型，动态调整权重，像章鱼一样适应环境 
 机器之心  https://mp.weixin.qq.com/s/_vdA_KygkFWqFE5Xm6CDRg \
  Transformer作者初创重磅发布Transformer²！AI模型活了，动态调整自己权重  新智元  https://mp.weixin.qq.com/s/EUt3g8BeLCkiKO-tdMYcTg \
  TRANSFORMER2 : SELF-ADAPTIVE LLMS \
  可以根据不同任务动态调整模型权重的机器学习系统 \
  Transformer^2 这个名称反映了它的两步过程：首先，模型分析传入的任务以了解其要求，然后应用特定于任务的调整来生成最佳结果。通过有选择地调整模型权重的关键组成部分，该框架允许 LLM 实时动态地适应新任务 \
  Transformer^2 在各种任务（例如数学、编程、推理和视觉理解）上展示了显著进步，在效率和特定于任务的性能方面优于 LoRA 等传统静态方法，同时需要的参数少得多 \
  https://github.com/SakanaAI/self-adaptive-llms
* 54.姚期智团队开源新型注意力，节省90%内存不降性能，一个框架统一MHA/MQA/GQA  量子位  https://mp.weixin.qq.com/s/3VS54ZY-vo9AVZiqOdC3-g \
  TPA \
  TPA对每个token做动态的张量分解，不存储完整的静态KV，而是保留分解的版本，内存占用节省90%（或者更多），而不会牺牲性能\
  Tensor Product Attention ls All You Need \
  https://github.com/tensorgi/T6
* 55.构建超维预测处理认知架构的核心  CreateAMind  https://mp.weixin.qq.com/s/PMSSrqzR8qWFhtB9VAIvUw \
  CogNGen: Constructing the Kernel of a Hyperdimensional Predictive Processing Cognitive Architecture

# 1.16 Thur
* 56.线性化注意力综述：突破Softmax二次复杂度瓶颈的高效计算方案  机器学习研究组订阅  https://mp.weixin.qq.com/s/ck_MfRcRQw8pAUVh6IzZKQ

# 1.17 Fri
* 57.大规模语言模型智能体的终身学习：发展路线图  专知  https://mp.weixin.qq.com/s/IPIcAYr5MgK8kc8xqeN0Ig \
  360篇文献！从终生学习视角出发，华南理工团队发布全新Agent综述 
 PaperWeekly  https://mp.weixin.qq.com/s/svub9VZGXkbFWH2A7p91SQ \
  Lifelong Learning of Large Language Model based Agents: A Roadmap \
  目前的 AI 系统在终生学习中面临两个主要挑战：灾难性遗忘和可塑性丧失，这两个挑战构成了稳定性-可塑性困境
* 58.突破数据思维密度，仅4T数据让8B模型逼近GPT-4o-mini  夕小瑶科技说  https://mp.weixin.qq.com/s/z9QD5hsDuWsGX9V6Pl9I7Q 
* 59.游戏结束了？OpenAI可能已经突破，跨过起飞的最后临界阈值  机器之心  https://mp.weixin.qq.com/s/07tm1kqwp_tiQ9OiMT2t-g \
  OpenAI员工疯狂暗示，内部已成功开发ASI？被曝训出GPT-5但雪藏  新智元  https://mp.weixin.qq.com/s/uOlQ7ZbbsnC-5mvXtGADww 
  
# 1.18 Sat
* 60.谢赛宁新作爆火，扩散模型新赛道诞生！测试时计算带飞，性能飙到天花板 
 新智元  https://mp.weixin.qq.com/s/JuylS5tFOygHdHnL5V3NVg \
  Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps \
  https://inference-scale-diffusion.github.io/
* 61.迈向大型推理模型：基于大型语言模型的强化推理综述  专知  https://mp.weixin.qq.com/s/1BiVI7lfFI2fE7SipWYYbg \
  Towards Large Reasoning Models: A Survey ofReinforced Reasoning with Large Language Models 
* 62.Search版o1：推理过程会主动查资料，整体性能优于人类专家，清华人大出品  量子位  https://mp.weixin.qq.com/s/J_n5cn_Zp4lRs8ESqFEFmg \
  人大清华提出自主搜索版「Search-o1」！解决知识困境，大幅提升推理模型可靠性  新智元  https://mp.weixin.qq.com/s/ytAqw5TNF2JD7UXS-S17gg \
  Agentic搜索增强推理模型框架 \
  Search-o1: Agentic Search-Enhanced Large Reasoning Models \
  o1和o1类模型在推理过程中表现突出，但却存在“知识不足”的明显缺陷——推理步骤太长/模型知识不足时，推理过程就很容易卡壳，导致推理链中的错误传递 \
  Search-o1的解题思路是：暂停推理，去搜索查找缺少的知识/资料，然后再回来继续推理 \
  https://github.com/sunnynexus/Search-o1

# 1.19 Sun
* 63.脉冲神经网络中的预测编码：综述  CreateAMind  https://mp.weixin.qq.com/s/673ljNRPmv4mymg0FL9k5g \
  PREDICTIVE CODING WITH SPIKING NEURALNETWORKS: A SURVEY

# 1.20 Mon
* 64.OpenAI博士级「超级智能体」即将登场？与ChatGPT深度集成，可操控计算机  新智元  https://mp.weixin.qq.com/s/DJ6kgmu4TwF3OQQh-2YlSA \
  https://www.axios.com/2025/01/19/ai-superagent-openai-meta
* 65.(**值得看看**)万字长文：意识的大一统理论要来了吗？| 追问顶刊  追问nextquestion  https://mp.weixin.qq.com/s/OM-_-yjfHuUug2OXcZEv4w 
* 66.语言与语言行为的整合结构及其心理化路径  CreateAMind  https://mp.weixin.qq.com/s/03GbmdoXxAdPh82nePauLQ \
  语言与语言行为的整合结构及其心理化路径
* 67.追平满血版o1的国产多模态模型终于来了！训练细节全部公开  机器之心  https://mp.weixin.qq.com/s/FOAcS2jsTwNoZA2t1BJ66Q \
  Kimi硬刚多模态满血版o1，首曝训练细节！强化学习scaling新范式诞生  新智元  https://mp.weixin.qq.com/s/E_V9xQ9gTpISlD4gr18paQ \
  技术报告：Kimi k1.5：借助大语言模型实现强化学习的 Scaling \
  报告链接：https://github.com/MoonshotAI/kimi-k1.5

# 1.21 Tue
* 68.人工智能本质上是矩阵和向量的计算引擎，在高维空间中找到最佳解  图灵人工智能  https://mp.weixin.qq.com/s/8SZGqCmYlnK3e3y6EHmSeQ 
* 69.(**deepseek R1,非常值得研究实践**)开源版o1！中国大模型让国外陷入疯狂，成本猛降90%  AIGC开放社区  https://mp.weixin.qq.com/s/3C_X7lYy-NX5HvTLjKOxzQ \
  DeepSeek开源o1击毙OpenAI，强化学习惊现「啊哈」时刻！网友：AGI来了  新智元  https://mp.weixin.qq.com/s/MssR1cfg8twTdXW11qkm3g \
  https://github.com/deepseek-ai/DeepSeek-R1
* 70.(**值得看看**)AI智能体2小时击败人类，引爆贝叶斯推理革命！仅用10%数据训练秒杀大模型  新智元  https://mp.weixin.qq.com/s/uayrlvGVjwEhtNij0dne7Q \
  Genius并非仅仅是对以往SOTA的渐进式改进。研究者应用了Karl Friston教授的自由能量原理、主动推断框架和贝叶斯推理架构 \
  自由能量原理的核心观点是：生物体认知与行为底层都遵循着相同的规则，即感官观测的「意外」最小化。在这里，「意外」被用于衡量智能体当前的感官观测与偏好的感官观测之间的差异 \
  Mastering Atari Games with Natural Intelligence \
  博客地址：https://www.verses.ai/blog/mastering-atari-games-with-natural-intelligence
* 71.(**值得看看**)选择/杂交/突变，DeepMind将自然选择引入LLM思维，实现心智进化  机器之心  https://mp.weixin.qq.com/s/RRRKim-1wuJaYZ7lDHHsWg \
  推理模型规划任务成功率从5%到95%，DeepMind遗传算法新研究火了  量子位  https://mp.weixin.qq.com/s/QTVSPYyYACv5D74-OGyRnA \
  Evolving Deeper LLM Thinking \
  提出了一种进化搜索策略，可用于 scaling LLM 的推理时计算（inference time compute）。该方法被命名为 Mind Evolution，即心智进化。实验表明，在同等推理成本下，新方法的自然语言规划任务表现会显著优于 Best-of-N 和 Sequential Revision 等其它推理策略

# 1.22 Wed
* 72.智能体检索增强生成：关于智能体RAG的综述  专知  https://mp.weixin.qq.com/s/dTp1Z8q9MNPK_8ejHXQKQw \
  AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON AGENTIC RAG
* 73.化解机器人的「幻觉」：北大发布OmniManip，VLM结合双闭环系统，3D理解能力大幅提升  机器之心  https://mp.weixin.qq.com/s/nMbWrysJm524vvWOA0C1pA \
  OmniManip: Towards General Robotic Manipulation via Obiect-Centric Interaction Primitives as Spatial Constraints \
  https://omnimanip.github.io
* 74.(**空间智能**)李飞飞：语言之外，另一半的智能还有待实现  机器之心  https://mp.weixin.qq.com/s/7rIhTVoURWSAMuvenTsvDA \
  视频链接：https://www.youtube.com/watch?v=0jMgskLxw3s \
  探究智能的本质，李飞飞认为智能分为说话的能力和做事能力，与之对应的是语言智能和空间智能，语言是人类的语言，而 3D 是自然的语言。而拥有空间智能的 AI，将做到人类从未做到的事：真正地打破物理世界和数字世界的界限 
* 75.1M长上下文，满血版Gemini 2.0又一次登上Chatbot Arena榜首  机器之心  https://mp.weixin.qq.com/s/NqtKUUuM0WrN0oShfba7gQ \
  谷歌发布了 Gemini 2.0 Flash Thinking 推理模型的加强版，并再次登顶 Chatbot Arena 排行榜 \
  试用链接：https://aistudio.google.com/prompts/new_chat 
* 76.意识模型解决ARC-AGI基准测试  CreateAMind  https://mp.weixin.qq.com/s/7blcPNbBLMJ6HLB3g_uk6w \
  Solving the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) AI Benchmark with ICOM \
  第八代独立核心观察者模型（ICOM）认知架构的一个片段被应用于ARC-AGI挑战基准测试，而没有接受任何针对ARC-AGI或类似谜题的训练
* 77.(**值得看看**)有意识的AI认知框架，既有智慧，又有感知，所有决策基于内部主观体验   CreateAMind  https://mp.weixin.qq.com/s/Uc3OLvXPTfYe4bT4Scwe5g \
  Problem-Solving and Learning Strategies within the Independent Core Observer Model (ICOM) Cognitive Architecture \
  独立核心观察者模型（ICOM）认知架构是一个旨在创建自我意识、有意识的人工智能系统的认知框架，该系统既具有智慧，又具有感知能力，并且其所有决策都基于内部主观体验。ICOM通过实现与认知相关的几种重要模式或理论来实现这一目标，包括整合信息理论[7]、全局工作空间理论[2, 3, 4, 5]、心智计算理论[27]、层次信息理论[1]和概念依赖理论[Schank]。其中，全局工作空间理论（由安东尼奥·达马西奥[12]阐述）是ICOM架构的主要基础。关于ICOM的更多信息，请参阅现有研究[19, 20]。ICOM在借鉴这些理论的同时，还使用了“抽象意识理论” 
* 78.UC伯克利等提出具身智能「动作Tokenizer」，效率飙升5倍！  新智元  https://mp.weixin.qq.com/s/gR2Mo6m_yu7xdljzsHgLTg \
  提出了FAST，一种高效的动作Tokenizer，显著缩短了训练时间，并且能高效地学习和执行复杂任务 \
  Fast: Effcient Action Tokenization for Vision-Language-Action Models

# 1.23 Thur
* 79.小模型也能玩转RAG！性能仅降1%，存储省75%，边缘设备轻松跑  量子位  https://mp.weixin.qq.com/s/FIe5R_ryfdkBJOG-yI9UYg \
  香港大学黄超教授团队提出MiniRAG，成功将RAG技术的应用门槛降至1.5B参数规模，实现了算力需求的大幅降低。这一突破性成果不仅为边缘计算设备注入新活力，更开启了基于小模型轻量级RAG的探索 \
  项目链接: https://github.com/HKUDS/MiniRAG
* 80.一种可解释的人工智能体，整合了强化学习（RL）和主动推理的关键要素 
 CreateAMind  https://mp.weixin.qq.com/s/qyrP9aFdfIK6ULt7jQSMgA \
  Free Energy Projective Simulation (FEPS): Active inference with interpretability

# 1.24 Fri
* 81.刚刚，OpenAI首个L3级智能体深夜觉醒！AI自己玩电脑引爆全网，AGI一触即发  新智元  https://mp.weixin.qq.com/s/l5i0PgBBKO0U2b1at7fbeg \
  刚刚，OpenAI正式放出智能体Operator！能推理、联网自主执行任务  机器之心  https://mp.weixin.qq.com/s/hNZ0KNwuMjyCW0bM3AemXA \
  演示中，AI智能体不仅可以精准理解指令，还能自主完成各类任务。而它的独特之处在于，可以直接与网页交互——打字、点击、滚动，几乎一气呵成。比如，自动填写繁琐的在线表单、上网购物、创建表情包、处理重复性浏览器任务等等
* 82.阿里云通义大模型新技术：MoE模型训练专家平衡的关键细节  机器之心  https://mp.weixin.qq.com/s/WHWtNrIErYuC3GzyIFEgTw \
  Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models \
  在阿里云通义千问 Qwen 团队提交的一篇论文中，研究人员发现了目前最热门的 MoE（混合专家模型）训练中存在的一个普遍关键问题，并提出一种全新的方法——通过轻量的通信将局部均衡放松为全局均衡，使得 MoE 模型的性能和专家特异性都得到了显著的提升
* 83.(**想法很好，值得看看**)向视觉版o1出击，阶跃张祥雨团队推出“慢感知”，实现感知层面的推理时scaling  量子位  https://mp.weixin.qq.com/s/UfuARViUmUqwL1sdPbwSNw \
  视觉版o1的初步探索，阶跃星辰&北航团队推出“慢感知” \
  Slow Perception: Let's Perceive Geometric Figures Step-by-step \
  当前基于system1感知的多模态大模型，看图过轻，感知不够精细，这限制了其进一步发展：当我们拿着一张片子给医生看，而医生不到1秒钟就看完了，告诉你啥事没有，我们会请他再看看，要求他看的再仔细点。视觉语言模型想要有更多的落地场景，system2感知能力是第一步，感知要慢下来。slow perception是研究人员基于几何parsing任务，在视觉sys2上的初步探索，他们也在积极往更通用的任务上迁移，并取得了初步的效果。大家敬请期待 \
  开源地址：https://github.com/Ucas-HaoranWei/Slow-Perception 
* 84.(**值得看看**)一文详尽之LLM-Based Agent  Datawhale  https://mp.weixin.qq.com/s/tprYtnARTwtSQwx3AZX4Zg 

# 1.25 Sat
* 85.物理测试暴击AI圈，DeepSeek R1稳超o1、Claude，我们已进入RL黄金时代  机器之心  https://mp.weixin.qq.com/s/caXFLM2WAfQfZvkLZ9MSsw 
* 86.视觉基础模型的可解释性：综述  专知  https://mp.weixin.qq.com/s/_nIdG6pWD55JG7myKpEwHg \
  Explainability for Vision Foundation Models: A Survey
* 87.《面向基础模型的高效参数微调》综述  专知  https://mp.weixin.qq.com/s/i3U8ViDsXBKTmJ3Pqdk9XQ \
  Parameter-Effcient Fine-Tuning forFoundation Models

# 1.26 Sun
* 88.DeepSeek-R1持续刷屏，连Open R1都来了！抱抱脸发起，1天狂揽1.9k星 
 量子位  https://mp.weixin.qq.com/s/BX2iTak6bPAKdj6Lv1Lt3A \
  全球掀DeepSeek复现狂潮！硅谷巨头神话崩塌，30刀见证啊哈时刻  新智元  https://mp.weixin.qq.com/s/o41vPh9eJCVjCRUE4u5npA \
  DeepSeek开源了6个用R1蒸馏的小模型，其中蒸馏版Qwen-1.5甚至能在部分任务上超过GPT-4o \
  https://github.com/huggingface/open-r1 \
  https://github.com/hkust-nlp/simpleRL-reason \
  自我反思机制的涌现在训练到第 40 步左右时，研究者观察到：模型开始形成自我反思模式，这正是DeepSeek-R1论文中所描述的「aha moment」（顿悟时刻）
* 89.顶级AI智能体不会社交，创业远不如人类！CMU等：最多完成24%任务  新智元  https://mp.weixin.qq.com/s/VhKa6rZURgeK3OEebRkuxg \
  THEAGENTCOMPANY:BENCHMARKING LLM AGENTS ON CONSEOUENTIAL REAL WORLD TASKS
* 90.迈向强人工智能：变革性信念与科学创造力  CreateAMind  https://mp.weixin.qq.com/s/G2T4xPv9lWXPJ_52W-UXsw \
  Towards Strong AI: Transformational Beliefsand Scientific Creativity 
* 91.8B模型超越GPT-4o！通义实验室提出多轮对齐SDPO，让LLM更擅长多轮交互 
 PaperWeekly  https://mp.weixin.qq.com/s/kw34untBieXEUaEgCXPHlA \
  SDPO: Segment-Level Direct Preference Optimization for Social Agents \
  https://huggingface.co/datasets/Tongyi-ConvAI/SDPO

# 1.27 Mon
* 92.因果涌现与“时间倒流”：基于可逆性的因果涌现新理论｜集智科学研究中心最新成果  集智俱乐部  https://mp.weixin.qq.com/s/r46w4_XcdIVwygqq_zfofg \
  世界的本源是可逆的，智能体构建世界模型的本质是为了恢复被他自己打破了的时间反演对称性
* 93.DeepSeek-R1解读：纯强化学习，模型推理能力提升的新范式？  Datawhale  https://mp.weixin.qq.com/s/tfza3D7eOTdPcCB14XaxCg \
  1.LLM模型推理能力提升: \
  a.在LLM模型post-training中，仅使用强化学习（reinforcement learning，RL） 提升模型推理能力，不再依赖有监督微调训练（supervised fine-tuning，SFT）。 \
  b.证明了LLM模型具有自行探索长思维链（chain-of-thought，COT） 的能力。 \
  2.端侧模型（小模型）推理能力提升: 相对于使用RL进行训练，基于大模型进行蒸馏（Distillation）的方式，是提升端侧模型推理能力更有效的途径。 \
  DeepSeek-R1-Zero展示出了自我进化（self-evolution） 能力，在没有监督数据的情况下，随着强化学习训练进程的深入，模型的思考时间在增加，并自发出现了诸如reflectio（反射，模型重新审视和重新评估其先前步骤）以及探索解决问题的替代方法等更加复杂的行为 \
  在DeepSeek-R1-Zero的训练过程中出现了Aha Moment（顿悟时刻），代表RL有可能在人工系统中解锁新的智能水平，为未来更加自主和自适应的模型铺平道路

# 1.28 Tue
* 94.
* 95.

# 1.29 Wed
# 1.30 Thur
# 1.31 Fri
