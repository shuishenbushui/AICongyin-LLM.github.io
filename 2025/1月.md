# 1.1 Wed
* 1.Meta首席AI科学家LeCun访谈：为什么他一直认为AI还不如一只猫？  图灵人工智能  https://mp.weixin.qq.com/s/Ny6x0sKuakRFxPsE5Ali7A 

# 1.2 Thur
* 2.4o-mini只有8B，o1也才300B！微软论文意外曝光GPT核心机密  量子位  https://mp.weixin.qq.com/s/bT_w-T9ElmPUXbYA1f7kCg 
* 3.比人类神经元快10亿倍！港中文、中科院「超级大脑」：1秒识图3479万张  新智元  https://mp.weixin.qq.com/s/5L1cOPpwQByX1NBPXrV6ig \
  Integrated laser graded neuron enabling high-speed reservoir computing without a feedback loop \
  这个基于芯片的量子点激光器，不仅能完全模仿真实神经细胞功能，更实现了惊人的速度——即10GBaud信号处理速度，也就意味着它比生物神经元快整整10亿倍。

# 1.3 Fri
* 4.(**值得了解**)突破算力限制！Meta开源“记忆层”，重塑Transformer架构大模型  AIGC开放社区  https://mp.weixin.qq.com/s/Zv2oyzLb4bIaq9FrzK3GXw \
  Meta探索大模型记忆层，扩展至1280亿个参数，优于MoE  机器之心  https://mp.weixin.qq.com/s/R5JOMkLVbbI7yPJmdcT2pQ \
  Memory Layers at Scale \
  全球社交巨头Meta分享了一个创新研究——Memory layers（记忆层） \
  目前，Transformer架构的预训练大模型在存储、查询数据时，随着参数的变大对算力的需求呈指数级增长。“记忆层”提出了新的高效查询机制替代了传统的查询方法，通过比较查询键与两个较小集合中的键，可以快速找到最相关的键，而无需遍历模型的整个记忆层。这也就是说，可以在不增加算力的情况下显著增加大模型的参数。例如，研究人员在仅有1.3亿参数的模型中添加了128亿额外的记忆参数，其性能与Meta开源的Llama 2- 70相当，而算力却比它低了10倍左右。\
  开源地址：https://github.com/facebookresearch/memory
* 5.o3来了，通用人工智能真的触手可及吗？  追问  https://mp.weixin.qq.com/s/IYDV5tSDZvOD-aQm2xD4Kw \
  我认为生物系统与由其他材料制成的系统之间不存在任何特别的差异，能够阻止非生物系统变得智能
* 6.(**值得了解，上下文抽象学习**)数据不够致Scaling Law撞墙？CMU和DeepMind新方法可让VLM自己生成记忆 
 机器之心  https://mp.weixin.qq.com/s/bSE8DGf0fkNKhPhe8H4HoA \
  VLM 智能体生成自己的记忆：将经验蒸馏成具身思维程序 \
  VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought \
  https://ical-learning.github.io/ \
  https://github.com/Gabesarch/ICAL \
  In-Context Abstraction Learning（ICAL），即上下文抽象学习
* 7.(**看一看**)如何复现o1模型？复旦最新《搜索与学习的扩展：从强化学习角度重现o1的路线图》  专知  https://mp.weixin.qq.com/s/3tWV5ykUVsI18M7jeDB2-A \
  OpenAI最大秘密，竟被中国研究者破解？复旦等惊人揭秘o1路线图  新智元  https://mp.weixin.qq.com/s/IOKFBgoWyietVe3NNNw9Hg \
  Scaling of Search and Learning: A Roadmap to Reproduce ol from Reinforcement Learning Perspective

# 1.4 Sat
* 8.从理论到实践，中科大、讯飞发布SocraticLM：首个实现苏格拉底教学法的智能辅学大模型  PaperWeekly  https://mp.weixin.qq.com/s/YbyZblgWetWI6RDp45NjLg \
  SocraticLM: Exploring Socratic Personalized Teaching with Large Language Models \
  https://github.com/Ljyustc/SocraticLM
* 9.LoRA进展有哪些？最新《基础模型的低秩适应》综述  专知  https://mp.weixin.qq.com/s/2XQF93S0v9Ofn8LTILQmNg \
  Low-Rank Adaptation for Foundation Models: A Comprehensive Review
* 10.Just keep scaling！思维链作者Jason Wei 40分钟讲座剖析LLM扩展范式  机器之心  https://mp.weixin.qq.com/s/3d2eE_uAPWOY289MCLn09A \
  视频地址：https://www.youtube.com/watch?v=yhpjpNXJDco \
  幻灯片：https://llm-class.github.io/slides/Jason_Wei.pdf 
* 11.从2019年到现在，是时候重新审视Tokenization了  机器之心  https://mp.weixin.qq.com/s/zmeFYfxWD1nZq_MocgGeeQ \
  https://huggingface.co/spaces/huggingface/number-tokenization-blog

# 1.5 Sun
* 12.大规模视觉-语言模型的对齐与失齐：从可解释性的视角进行的综述  专知  https://mp.weixin.qq.com/s/lwJXA_Tz7kLcHd1kLglDrA \
  Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability 
* 13.斯坦福打脸大模型数学水平：题干一改就集体降智，强如o1也失准，能力涌现怕不是检索题库  量子位  https://mp.weixin.qq.com/s/GY9rqxXLyujEes2H3ycMDQ \
  Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning

# 1.6 Mon
* 14.内在主动推理1：基本原理  CreateAMind  https://mp.weixin.qq.com/s/KLc5kXehLofcil6JMcMxpw \
  Intra-Active Inference I: Fundamentals \
  完全不懂
* 15.仅需一万块钱！清华团队靠强化学习让 7B模型数学打败GPT-4o  量子位  https://mp.weixin.qq.com/s/s-DeQCAX1gth82YkABxLLA \
  1/10训练数据超越GPT-4o！清华等提出隐式过程奖励模型PRIME，在线刷SOTA 
 新智元  https://mp.weixin.qq.com/s/bogt5zl7rytcz-FhNECTNg \
  清华大学NLP实验室、上海AI Lab、清华大学电子系、OpenBMB社区等团队提出一种新的结合过程奖励的强化学习方法——PRIME（Process Reinforcement through IMplicit REwards）\
  Process Reinforcement through Implicit Rewards \
  https://github.com/PRIME-RL/PRIME \
  blog链接：https://curvy-check-498.notion.site/Process-Reinforcement-through-Implicit-Rewards-15f4fcb9c42180f1b498cc9b2eaf896f

# 1.7 Tue
* 16.AGI理论比较：主动推理、强化学习、控制论、贝叶斯大脑、效用决策、有限理性、情感动机、动态体内平衡  CreateAMind  https://mp.weixin.qq.com/s/egj-2woxVpdA8XuGcOVaeQ \
  Active-InferenceThe-Free-Energy-Principle-in-Mind 
* 17.伯克利最新《机器人》课程  专知  https://mp.weixin.qq.com/s/1lo_pHSYiXiPKtoZK8K2Qw 
* 18.单张图像探索3D奇境：Wonderland让高质量3D场景生成更高效  机器之心  https://mp.weixin.qq.com/s/ViSQcx3UNLjRb7goIRhQFg \
  Wonderland: Navigating 3D Scenes from a Single lmage \
  项目主页：https://snap-research.github.io/wonderland/
* 19.手机「自动驾驶」大揭秘！vivo万字综述探讨大模型手机自动化  机器之心  https://mp.weixin.qq.com/s/Cmq4qidvlLB5ZL5OZ90uoA \
  LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects
* 20.(**非常值得深入研究！！！**)Tokenization，再见！Meta提出大概念模型LCM，1B模型干翻70B？  新智元  https://mp.weixin.qq.com/s/TpfRePjwzHdSIXJYCDCIJw \
  Meta提出大概念模型，抛弃token，采用更高级别的「概念」在句子嵌入空间上建模，彻底摆脱语言和模态对模型的制约 \
  受人类构思交流的高层级思路启发，Meta AI研究员提出全新语言建模新范式「大概念模型」，解耦语言表示与推理 \
  网友Chuby兴奋地表示：「如果Meta的大概念模型真的有用，那么同等或更高效率的模型，其规模将更小。比如说1B模型将堪比70B的Llama 4。进步如此之大！」 \
  新系统将不再单纯基于下一个token预测，而是像婴儿和小动物那样通过观察和互动来理解世界 \
  简而言之，「大概念模型」（LCM）是在「句子表示空间」对推理（reasoning）建模，抛弃token，直接操作高层级显式语义表示信息，彻底让推理摆脱语言和模态制约 \
  Large Concept Models \
  代码链接：https://github.com/facebookresearch/large_concept_model
* 21.学习离散世界模型用于启发式搜索  CreateAMind  https://mp.weixin.qq.com/s/UF06zEVXDESdl72u-afTVA \
  Learning Discrete World Models for Heuristic Search

# 1.8 Wed
* 22.(**值得试试**)机器人ChatGPT时刻！英伟达开源世界大模型，完美模拟物理世界！  AIGC开放社区  https://mp.weixin.qq.com/s/UdgdJjdpRzFj3z8HBc5sOg \
  英伟达「世界基础模型」诞生，引爆物理AI革命！75页报告出炉，GitHub狂飙2k星  新智元  https://mp.weixin.qq.com/s/wJRDkwt-XvxJR5JhYLU6ZQ \
  NVIDIA Cosmos \
  一共包含了四大功能模块：扩散模型、自回归模型、视频分词器，以及视频处理与编辑流程 \
  开源项目：https://github.com/NVIDIA/Cosmos \
  Cosmos World Foundation Model Platform for Physical AI
* 23.(**值得看看**)用于人工通用智能（AGI）的大型语言模型：基础原则和方法综述  专知  https://mp.weixin.qq.com/s/oa_5OIBBlw4cnkJMVk-tnw \
  Large language models for artificial general inteligence(AGl): A survey of foundationa.principles and approaches \
  使LLM达到AGI还需要的解决的问题
* 24.最新AI Agent万字综述分享！  Datawhale  https://mp.weixin.qq.com/s/HrKOGXuI8wVM6qgylKZ8qw \
  视频链接：https://www.bilibili.com/video/BV17wrpYKE4V \
  百宝箱地址： https://tbox.alipay.com/pro-about
* 25.纯视觉方案，精准操控电脑和手机！港大Aria-UI登顶，超越Claude 3.5  新智元  https://mp.weixin.qq.com/s/2U4A6gYI5_G-k-jFzWMz8w \
  GitHub仓库：https://github.com/AriaUI/Aria-UI \
  项目主页：https://ariaui.github.io \
  Aria-UI: Visual Grounding for GUI Instructions
* 26.o1也会「想太多」？腾讯AI Lab与上海交大揭秘o1模型过度思考问题  机器之心  https://mp.weixin.qq.com/s/ofTfoFNIgGO2ZIsWVnjfAA \
  Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs
* 27.老婆饼里没有老婆，RLHF里也没有真正的RL  机器之心  https://mp.weixin.qq.com/s/dVCD0crxwCThE7-XVvoemg \
  博客链接：https://www.linkedin.com/pulse/why-rlhf-other-rl-like-methods-dont-bring-true-rl-llmsand-atlas-wang-s1efc/ \
  Why RLHF (and Other RL-Like Methods) Don' t Bring "True RL" to LLMs-and Why It Matters \
  RLHF（基于人类反馈的强化学习）和其他类似的方法并没有为大型语言模型（LLM）带来真正的强化学习（RL），因为它们缺乏 RL 的核心特征：持续的环境交互和长期目标的追求 \
  虽然 RLHF、DPO 和相关方法提供了一种实用的方法，可以在短期环境中使 LLM 与人类偏好保持一致，但它们无法赋予 LLM 真正、持久的目标或意图。这些方法也只是与经典的 RL 或 IRL 范式略有对应。未来的系统若能在真正的多步骤 RL 循环中使用 LLM，就能解锁更多自主的、类似智能体的行为，但同时也会引发新的安全和一致性问题

# 1.9 Thur
* 28.重磅！微软开源最强小模型Phi-4，超GPT-4o、可商用  AIGC开放社区  https://mp.weixin.qq.com/s/Rk-XBLd3PosXCJhH_mth1w \
  开源地址：https://huggingface.co/microsoft/phi-4/tree/main 
* 29.引领人机交互革命？微软研究团队发布80页的大模型GUI智能体综述  机器之心  https://mp.weixin.qq.com/s/WhWGjLrYraomGz7yI8R5_A \
  Large Language Model-Brained GUl Agents A Survey
* 30.具身智能新高度！智元机器人推出全球首个4D世界模型EnerVerse  机器之心  https://mp.weixin.qq.com/s/B6MfkL_SxZ5BrR--PYqYiQ \
  主页地址：https://sites.google.com/view/enerverse/home \
  EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation
* 31.「知识蒸馏+持续学习」最新综述！哈工大、中科院出品：全新分类体系，十大数据集全面实验  新智元  https://mp.weixin.qq.com/s/1vX-wH38uNqo-YMKRmvLrQ \
  Continual Learning With Knowledge Distillation:A Survey

# 1.10 Fri
* 32.计算之物理：灵魂耗能吗？  图灵人工智能  https://mp.weixin.qq.com/s/c1OR1Z4ogC2rqFR-NVk7ZA \
  西拉德的智能体（intelligent being）到底是什么？那时还没有图灵机。西拉德把麦克斯韦的多分子简化为单个分子。西拉德的思想实验可以把信息转化为能量。西拉德认为信息的获取，即测量，需要付出代价，即消耗能量，从而使得周边环境的熵增加
* 33.图灵奖得主杨立昆最新万字实录：我们离真正的Agent还差几大关键技术  图灵人工智能  https://mp.weixin.qq.com/s/7wUd3zoh8rvIaOa9kxBx0w 
* 34.(**值得了解，小模型深度思考能力**)重大突破！微软发布“自我进化”，帮小模型超OpenAI-o1  AIGC开放社区  https://mp.weixin.qq.com/s/WDJRcLUFwQe3cLJj5ToLPQ \
  让Qwen2.5 7B超越o1，微软干的！MSRA推出小模型数学推理自我进化新方法  量子位  https://mp.weixin.qq.com/s/URAhVZkVcMQBRnBdd-7F9Q \
  7B模型数学推理击穿o1，直逼全美20%尖子生！四轮进化，微软华人新作爆火  新智元  https://mp.weixin.qq.com/s/KKoQILXVstwE-BAN_-VCvg \
  微软亚洲研究院发布了一种创新算法——rStar-Math \
  rStar-Math通过代码增强CoT、蒙特卡洛树搜索等，可以帮助小参数模型在不依赖老师模型蒸馏的情况下，实现多轮自我思维深度进化，极大增强模型的数学推理能力 \
  代码地址: https://github.com/microsoft/rStar \
  rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking \
  rStar-Math核心在于，让小模型具备「深度思考」的能力。团队借鉴了AlphaGo中蒙特卡洛树搜索（MCTS）技术，设计了一个由2个协同工作的SLM组成的系统：一个数学策略小语言模型（SLM）一个过程奖励模型（PRM） \
  通过代码增强CoT、蒙特卡洛树搜索（MCTS）等，**rStar-Math能让小·大模型在不依赖蒸馏教师模型的情况下，通过多轮自我进化的深度思维，掌握数学推理**

# 1.11 Sat
* 35. GAN已死？GAN万岁！布朗康奈尔新作爆火，一夜碾压扩散模型  新智元  https://mp.weixin.qq.com/s/48oNvSSzrj7rX2wz-MYsaQ \
  GAN已死？不，它卷土重来了！布朗大学和康奈尔大学的研究者刚刚提出了R3GAN，充分利用现代架构设计，彻底摒弃临时技巧，一半参数就能碾压扩散模型。网友惊呼：游戏规则要改变了！\
  The GAN is dead; long live the GAN! A Modern Baseline GAN
* 36.(**非常值得看看**)迈向System 2推理，100页论文硬核讲述Meta-CoT  机器之心  https://mp.weixin.qq.com/s/L_tErITBzUZ75GVGtbtdDQ \
  Meta-CoT 通过显式建模生成特定思维链（CoT）所需的底层推理过程，扩展了传统的思维链方法 \
  Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought \
  Rafailov 所说的 Meta-CoT，是一种新颖的框架，它通过显式建模生成特定思维链（CoT）所需的底层推理过程，扩展了传统的思维链方法 \
  该研究从认知科学的双过程理论中汲取灵感，将 Meta-CoT 框架看作为一种 System 2 推理形式。本文奠定了 Meta-CoT 理论基础，展示了如何通过系统搜索过程实现这一框架，以及如何将这些过程内化到一个单一的自回归模型中。随后，本文提供了实证证据，包括对 OpenAI 的 o1 和 DeepSeek-R1 等顶尖模型的分析，这些模型展现出了与内化（上下文）搜索一致的行为。接着本文进一步探索了通过过程监督来训练 Meta-CoT 模型的方法，以及通过蒙特卡洛树搜索（MCTS）和 A * 等搜索算法生成合成数据的技术 \
  作者认为传统的思维链并不能完全代表推理问题背后的数据生成过程。通过融入搜索、验证和迭代优化的概念，Meta-CoT 为高级问题解决所需的认知过程提供了一个更完整的模型

# 1.12 Sun
* 37.(**非常值得一试**)450美元训练一个「o1-preview」？UC伯克利开源32B推理模型Sky-T1，AI社区沸腾了  机器之心  https://mp.weixin.qq.com/s/aRUHeDheE4nwncbCLakgIQ \
  项目主页：https://novasky-ai.github.io/posts/sky-t1/ \
  开源地址：https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview \
  Sky-T1-32B-Preview 的训练成本不到 450 美元，这表明可以经济、高效地复制高级推理能力 \
  Sky-T1 似乎是第一个真正开源的推理模型，因为团队发布了训练数据集以及必要的训练代码，任何人都可以从头开始复制

# 1.13 Mon
* 38.辛顿与李飞飞对谈：AI是否具备语言理解和推理能力？  图灵人工智能  https://mp.weixin.qq.com/s/pwWP-avZxjeB0mW8Awb7kw 
* 39.(**值得了解**)AI博士智能体自主科研，o1-preview封神成本暴降84%！AMD霍普金斯新作爆火  新智元  https://mp.weixin.qq.com/s/cndCOqRu1d_sIPDmOfafxA \
  Hyperbolic联创Jasper Zhang在采访中称，AI智能体已经可以自主租用GPU，利用PyTorch进行开发了 \
  Agent Laboratory: Using LLM Agents as Research Assistants \
  Agent Laboratory有三个关键阶段：文献综述、实验设计和报告撰写 \
  由LLM驱动的专业智能体（如博士、博士后等）协同工作，承担文献综述、实验规划、数据准备和结果解释等工作。这些智能体还会集成arXiv、Hugging Face、Python和LaTeX等外部工具，来优化结果
* 40.在线持续学习：方法、挑战与基准的系统性文献综述，46页pdf  专知  https://mp.weixin.qq.com/s/UrJkitQSRqWlRWXL1TjbfQ \
  Online Continual Learning: A Systematic Literature Review of Approaches, Challenges, and Benchmarks
* 41.AI4Physics？【MIT博士论文】探索物理建模与表示学习的交汇点  专知  https://mp.weixin.qq.com/s/ZMhMmsh1LOwJmQjAG8VoIA \
  Exploring the Intersection of Physics Modeling and Representation Learning
* 42.聊一聊PRM（过程奖励模型）  关于NLP那些你不知道的事  https://mp.weixin.qq.com/s/5DHYmOxyAmuj7SNVGHvLwQ 
* 43.神经符号编程在 Scallop 中的原理与实践（上,1-4章）  CreateAMind  https://mp.weixin.qq.com/s/BYOAaKYHccJC2eM-m58Vaw \
  神经符号编程在 Scallop 中的原理与实践 5-7章：  CreateAMind  https://mp.weixin.qq.com/s/lhPxhcm467pRwjnkbJYwog \
  Neurosymbolic Programming inScallop: Principles and Practice \
  神经符号编程结合了深度学习和符号推理这两个互补的领域，从而为人工智能任务提供了更准确、可解释且具有领域感知能力的解决方案 \
  ???不明白神经符号编程有什么意义

# 1.14 Tue
* 44.抢先OpenAI“虚拟员工”！清华复旦斯坦福联手，让Agent接管电脑帮你工作 
 量子位  https://mp.weixin.qq.com/s/tUYjchA4ySYmdDIAB8FV6g \
  一句话让Agent自主干活，清华复旦斯坦福等开源的智能体开发框架抢先了OpenAI  机器之心  https://mp.weixin.qq.com/s/Ia0xrgkUrUljGeIxuN7FpQ \
  提出了一个名为“Eko”的Agent开发框架，开发者只需用简洁的代码和自然语言，就能快速构建可用于生产的“虚拟员工”：Agent可以接管用户的电脑和浏览器，代替人类完成各种任务 \
  Homepage：https://eko.fellou.ai/ \
  Github link：https://github.com/FellouAI/eko \
  Docs：https://eko.fellou.ai/docs/
* 45.(**非常值得研究**)谷歌新架构一战成名，打破Transformer记忆瓶颈，姚班校友钟沛林新作  量子位 
  https://mp.weixin.qq.com/s/APE_CJ4rEQYV8ngyaTAXWw \
  谷歌新架构终结Transformer，长序列处理王者诞生？清华姚班校友新作  新智元  https://mp.weixin.qq.com/s/LnUchUuiJvQBX1zx0cZnpw \
  近8年后，谷歌Transformer继任者「Titans」来了，上下文记忆瓶颈被打破  机器之心  https://mp.weixin.qq.com/s/EUCZ1oSuyzR9M9X9r5SYBw \
  Titans: Learning to Memorize at Test Time \
  https://github.com/lucidrains/titans-pytorch \
  Titans 比 Transformers 和现代线性 RNN 更高效，并且可以有效地扩展到超过 200 万上下文窗口，性能比 GPT4、Llama3 等大模型更好 \
  他还解释了这篇研究的动机，团队认为 Transformer 中的注意力机制表现为短期记忆，因此还需要一个能记住很久以前信息的神经记忆模块 \
  谷歌提出了一种新的长期神经记忆模块（neural memory module），它能够学习记忆历史上下文，并帮助注意力机制在利用过去已久信息的同时处理当前上下文 \
  从记忆的角度来看，谷歌认为注意力机制虽然受限于上下文但可以更准确地建模依赖关系，因此可以起到短期记忆的作用；而神经记忆能够对数据进行记忆，起到了长期、更持久的记忆作用。基于这两个模块，谷歌引入了一个全新的系列架构 —— Titans，通过三种变体有效地将记忆融合到该系统架构中，它们分别是记忆作为上下文（Memory as a Context，MAC）、记忆作为门（Memory as a Gate，MAG）和记忆作为层（Memory as a Layer，MAL）
* 46.微软华人团队最新研究：从LLM到LAM，让大模型真正具有「行动力」！  新智元  https://mp.weixin.qq.com/s/BVFV8v6KGcpKoz_TYdBdAA \
  Large Action Models: From Inception to lmplementation 
* 47.余弦相似度可能没用？对于某些线性模型，相似度甚至不唯一  机器之心  https://mp.weixin.qq.com/s/xlcHdXdjan9Ll2YD0mu3WA \
  Is Cosine-Similarity of Embeddings Really About Similarity?
* 48.思维链？思维树？华为诺亚：现在到了思维森林时刻！  机器之心  https://mp.weixin.qq.com/s/iWgpcXnGuVgbondK2JrLIQ \
  Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning \
  项目链接：https://github.com/iamhankai/Forest-of-Thought
* 49.SNeRL:用于强化学习的语义感知神经辐射场  CreateAMind  https://mp.weixin.qq.com/s/NiUUYCM-cz7RMb4LetaQfQ \
  SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning

# 1.15 Wed
* 50.微软开源AI Agent大更新，重塑智能体！多层级更强架构  AIGC开放社区  https://mp.weixin.qq.com/s/TlGdQt86yhWI-asBpy9lRw \
  https://github.com/microsoft/autogen
* 51.仅缩小视觉Token位置编码间隔，轻松让多模态大模型理解百万Token！清华大学，香港大学，上海AI Lab新突破  机器之心  https://mp.weixin.qq.com/s/mD0WQe4KBrDVCJdTtpk5Bw \
  V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding \
  项目主页：https://zzdhybthu.github.io/V2PE.github.io/ \
  开源代码：https://github.com/OpenGVLab/V2PE
* 52.MiniMax震撼开源，突破传统Transformer架构，4560亿参数，支持400万长上下文  机器之心  https://mp.weixin.qq.com/s/O7xaTDWmuZfzMUMTBWOhBw 
* 53.Transformer²要做「活」的AI模型，动态调整权重，像章鱼一样适应环境 
 机器之心  https://mp.weixin.qq.com/s/_vdA_KygkFWqFE5Xm6CDRg \
  Transformer作者初创重磅发布Transformer²！AI模型活了，动态调整自己权重  新智元  https://mp.weixin.qq.com/s/EUt3g8BeLCkiKO-tdMYcTg \
  TRANSFORMER2 : SELF-ADAPTIVE LLMS \
  可以根据不同任务动态调整模型权重的机器学习系统 \
  Transformer^2 这个名称反映了它的两步过程：首先，模型分析传入的任务以了解其要求，然后应用特定于任务的调整来生成最佳结果。通过有选择地调整模型权重的关键组成部分，该框架允许 LLM 实时动态地适应新任务 \
  Transformer^2 在各种任务（例如数学、编程、推理和视觉理解）上展示了显著进步，在效率和特定于任务的性能方面优于 LoRA 等传统静态方法，同时需要的参数少得多 \
  https://github.com/SakanaAI/self-adaptive-llms
* 54.姚期智团队开源新型注意力，节省90%内存不降性能，一个框架统一MHA/MQA/GQA  量子位  https://mp.weixin.qq.com/s/3VS54ZY-vo9AVZiqOdC3-g \
  TPA \
  TPA对每个token做动态的张量分解，不存储完整的静态KV，而是保留分解的版本，内存占用节省90%（或者更多），而不会牺牲性能\
  Tensor Product Attention ls All You Need \
  https://github.com/tensorgi/T6
* 55.构建超维预测处理认知架构的核心  CreateAMind  https://mp.weixin.qq.com/s/PMSSrqzR8qWFhtB9VAIvUw \
  CogNGen: Constructing the Kernel of a Hyperdimensional Predictive Processing Cognitive Architecture

# 1.16 Thur
* 56.
* 57.
* 58.
* 59.
* 60.

# 1.17 Fri
# 1.18 Sat
# 1.19 Sun

# 1.20 Mon
# 1.21 Tue
# 1.22 Wed
# 1.23 Thur
# 1.24 Fri
# 1.25 Sat
# 1.26 Sun

# 1.27 Mon
# 1.28 Tue
# 1.29 Wed
# 1.30 Thur
# 1.31 Fri
