# 8.1 周二
* 1、自适应YOLO：恶劣天气下的目标检测（附源代码） https://mp.weixin.qq.com/s/K5GKLW_-BQ1gOD0D-OOU8w
* 2、通俗解构语言大模型的工作原理 OneFlow https://mp.weixin.qq.com/s/21V8g_7teuRgHLWUej1NzA
* 3、放弃Softmax！首个线性注意力Transformer大模型！1750亿参数，速度和精度更优 计算机视觉daily https://mp.weixin.qq.com/s/NEIiFPcLoh0br8GtHoVtPA \
GPT 等大型语言模型（LLM）的成功离不开 Softmax 注意力机制，但这一机制也存在着成本高等一些缺点。\
近日，上海人工智能实验室和 OpenNLPLab 的一个研究团队提出了一种新的大型语言模型 TransNormerLLM，其中完全抛弃了基于 Softmax 的注意力机制，而是使用了新提出的线性注意力。据介绍，TransNormerLLM 是首个基于线性注意力的大型语言模型（LLM），其在准确度和效率方面的表现优于传统的基于 Softmax 注意力的模型。研究者也将发布其预训练模型的开源版本。
![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/f054c26c-c76f-4062-b635-010ec6a8fea8) \
论文：https://arxiv.org/abs/2307.14995 \
模型：https://github.com/OpenNLPLab/TransnormerLLM

# 8.2 周三
* 4、语言模型做先验，统一强化学习智能体，DeepMind选择走这条通用AI之路 图灵人工智能 https://mp.weixin.qq.com/s/5S7yMkXJn7_FXEoD7py-2w
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/8ba7f6e2-311a-4e93-83d5-20e9a88f3bf9)
  论文地址：https://arxiv.org/pdf/2307.09668.pdf
* 5、突破自监督学习效率极限！马毅、LeCun联合发布EMP-SSL：无需花哨trick，30个epoch即可实现SOTA XRer https://mp.weixin.qq.com/s/PcDa3xA8u7pGE0fxHhiMiQ
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/6b58d12f-f5a5-4191-9aa6-4d6b9b2b3495)
  论文链接：https://arxiv.org/pdf/2304.03977.pdf \
  代码链接：https://github.com/tsb0601/EMP-SSL
* 6、首发！国内最大Llama开源社区发布首个预训练中文版Llama2 夕小瑶科技说 https://mp.weixin.qq.com/s/JXyPAgJaX4GvvohJO_Nlyw \
  社区链接：https://github.com/FlagAlpha/Llama2-Chinese
* 7、清华等提出新框架：ToolLLM，增强大模型API调用能力，性能堪比ChatGPT！ AINLPer https://mp.weixin.qq.com/s/xRIJX2GOlLw-GF1T0njCDg
  ![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/7df32f70-b4fd-4c02-9c94-60d433ed5550)
  论文地址：https://arxiv.org/pdf/2307.16789.pdf \
  项目地址：https://github.com/OpenBMB/ToolBench
  
