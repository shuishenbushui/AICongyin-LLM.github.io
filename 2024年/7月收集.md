# 7.1 Mon
* 1.等不来OpenAI的Q*，华为诺亚探索LLM推理的秘密武器MindStar先来了  机器之心  https://mp.weixin.qq.com/s/bXZLC1ogVgqwFpNToGIBxw 
* 2.ICML 2024 Spotlight | 在解码中重新对齐，让语言模型更少幻觉、更符合人类偏好  机器之心  https://mp.weixin.qq.com/s/-9MjgNOLRrUdaQUF5tVv9w

# 7.2 Tue
* 3.大型语言模型中的人格综述  专知  https://mp.weixin.qq.com/s/G046kA5aVcMSSUB6HT-FZg 
* 4.只需将感知推理能力拆分，2B大模型就能战胜20B！国产新框架高效处理视觉任务  量子位  https://mp.weixin.qq.com/s/wiTLIio53j7fEZBBkUAZ-A \
  Prism: 显式地解耦了视觉语言模型（VLM） 的感知和推理
* 5.73年前，香农已经给大模型发展埋下一颗种子  机器之心  https://mp.weixin.qq.com/s/3VIdDWkk4iISNiXPOl-Tfg \
  普林斯顿大学教授承现峻（Sebastian Seung）抛出了这样一个观点：1951 年，在贝尔实验室（总部位于新泽西州 Murray Hill）工作的克劳德・香农提出了预测下一个单词的问题，这成为了当前大语言模型（LLM）的种子
* 6.哈工大提出创新迭代推理框架 DPE-MNER ：充分发挥多模态表示潜力  机器之心  https://mp.weixin.qq.com/s/nKQGTaTHjkW6daZfFKAi_g
* 7.神经网络可能不再需要激活函数？Layer Normalization也具有非线性表达！  机器之心  https://mp.weixin.qq.com/s/YRAcAKouScQGt3lOxe8fBQ \
  《On the Nonlinearity of Layer Normalization》 \
  论文地址：https://arxiv.org/abs/2406.01255
* 8.Andrej Karpathy提出未来计算机2.0构想： 完全由神经网络驱动！网友炸锅了  夕小瑶科技说  https://mp.weixin.qq.com/s/ZOhvA66bB2r6eD2eQFEjqA
* 9.谷歌重磅：告别RAG，长上下文的大语言模型无需检索增强  夕小瑶科技说  https://mp.weixin.qq.com/s/lOORnoyrA8lqOEXWKxhAQg \
  论文标题：Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More? \
  论文链接：https://arxiv.org/pdf/2406.13121
* 10.扩散模型与表示学习：综述  专知  https://mp.weixin.qq.com/s/jWMrOTlMLNY7yRtzBLoi0Q \
  Diffusion Models and Representation Learning: A Survey

# 7.3 Wed
* 11.大语言模型增强知识表示学习综述  专知  https://mp.weixin.qq.com/s/AmttGtvKGIxdUKCKoNAnBw \
  大语言模型（LLMs）与知识表示学习（KRL）的整合，标志着人工智能领域的重要进展，增强了捕捉和利用复杂知识结构的能力。这种协同作用利用了LLMs的高级语言和语境理解能力，以提升KRL的准确性、适应性和效能，从而扩展其应用和潜力
* 12.参数少80%，效果仍超LoRA！上交大&上海AI Lab推出高效微调框架FLoRA  量子位  https://mp.weixin.qq.com/s/x2NID0EsUiNLC5RJ01s-wg
* 13.复旦大学：一个小技巧探测大模型的知识边界，有效消除幻觉  夕小瑶科技说  https://mp.weixin.qq.com/s/nrRZwvsxXW2TT_So2jlB-w \
  复旦大学提出了COKE方法( Confidence-derived Knowledge boundary Expression)，通过利用模型内部的置信信号，教导LLMs识别和表达其知识边界，从而减少幻觉现象。实验结果表明，COKE显著提升了模型在域内和域外的表现，使模型能够在回答已知问题的同时，坦诚面对其未知领域的问题。\
  简而言之，就是增加大模型“知道自己不知道”的知识，减少大模型“不知道自己不知道”的知识。就比如图中的大模型在面对自己未知的问题“《迷失太空2018》中的机器人是谁”时，它如果不知道，就应该给出“不知道”，而不是编造一个“Max Robinson”的答案。\
  论文标题：Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals \
  论文链接https://arxiv.org/abs/2406.10881
* 14.万字长文：意识的大一统理论要来了吗？| 追问顶刊   追问nextquestion  https://mp.weixin.qq.com/s/OM-_-yjfHuUug2OXcZEv4w
* 15.通研院研究发现大语言模型在心智推理和行为规划上显著落后于人类  北京通用人工智能研究院  https://mp.weixin.qq.com/s/paYcSJbiZMbU3Vdv2-5x8Q \
  Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities

# 7.4 Thur
* 16.多模态视觉-语言大模型的架构演进 Ⅱ  关于NLP那些你不知道的事  https://mp.weixin.qq.com/s/BALAiCIANhLBx8lSIrwahQ
* 17.13瓦功耗处理10亿参数，接近大脑效率，消除LLM中的矩阵乘法来颠覆AI现状  ScienceAI  https://mp.weixin.qq.com/s/xZuLePQ9g_1Zru2rrHNN-A
* 18.强强联合！当RAG遇到长上下文，滑铁卢大学发布LongRAG，效果领先GPT-4 Turbo 50%  夕小瑶科技说  https://mp.weixin.qq.com/s/e7yvnj9UK3KLcUkwnqjGaw \
  LongRAG \
  https://arxiv.org/pdf/2406.15319.pdf
* 19.Kimi论文自曝推理架构，80%流量都靠它承担  量子位  https://mp.weixin.qq.com/s/u9RG7zuDUO62CbrmGyZtOA \
  Kimi背后的推理架构名叫Mooncake（月饼），主要特点是采取了分离式的设计方案
* 20.8人半年肝出开源版GPT-4o，0延迟演示全网沸腾！背后技术揭秘，人人免费用  机器学习研究组订阅  https://mp.weixin.qq.com/s/CSa5euhZReHXOGS_bUvoYw \
  传送门：https://moshi.chat/?queue_id=talktomoshi

# 7.5 Fri
* 21.「吗喽」在想啥？AI读心术精准重建猕猴大脑图像，网友：我们成三体人了  新智元  https://mp.weixin.qq.com/s/Sl-qTrcjYc17yQUDCGdQqA

# 7.6 Sat
* 22.Adam有了mini版：内存占用少一半，吞吐量提升50%  机器之心  https://mp.weixin.qq.com/s/LOhtmS9MCZSsRBDAq2mEuQ \
  Adam-mini: Use Fewer Learning Rates To Gain More
* 23.参数更新量仅为LoRA的5%，性能不减反升！南加大提出高效精调法LaMDA  夕小瑶科技说  https://mp.weixin.qq.com/s/H8mQxtpRj0P8sRnkCg0bIg \
  LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation
* 24.上交&阿里：掀开多模态大模型的头盖骨，解密黑盒模型推理过程  夕小瑶科技说  https://mp.weixin.qq.com/s/dF4QltattjndvXMX92gutA \
  From Redundancy to Relevance: Enhancing Explainability in Multimodal Large Language Models \
  本文运用Grad-CAM方法对MLLMs的复杂推理过程进行了逐层可视化研究，深入探究了图像与用户提示之间的交互和信息流动，得出了一些有趣的结论：如LLaVA1.5和Qwen-vl等模型中，浅层图像Token的信息流存在明显冗余。MLLMs可能在浅层捕获输入的含义而在深层进行推理获得答案等。相信该研究有助于人们更好地理解多模态复杂推理的内在机制。
* 25.清华北航博士生「强迫」Gemma-2说中文！弱智吧、角色扮演、数学问题表现惊喜  机器学习研究组订阅  https://mp.weixin.qq.com/s/-QgtXysWHhzD-KQV5OLkOg \
  清华大学的一名人工智能博士生王慎执就在X上隆重推出了一款微调模型Gemma-2-9B-Chinese-Chat

# 7.7 Sun

# 7.8 Mon
* 26.核心代码仅三行！即插即用的视觉语言连接器，一键提升多模态大模型  PaperWeekly  https://mp.weixin.qq.com/s/lEMHTxKrK1er1EmzGC4uSw \
  Dense Connector for MLLMs
* 27.(**有趣**)哈佛DeepMind开辟「虚拟神经科学」新领域！在世界模拟器驯养「赛博老鼠」  新智元  https://mp.weixin.qq.com/s/Tk9U4pS7WNqRX8gZklkREQ \
* 28.像生物网络一样「生长」，具备「结构可塑性」的自组织神经网络来了  机器之心  https://mp.weixin.qq.com/s/fdQcKpuu75C3lgFkxfpwdQ \
  Evolving Self-Assembling Neural Networks: From Spontaneous Activity to Experience-Dependent Learning \
  论文链接：https://arxiv.org/pdf/2406.09787 \
  项目链接：https://github.com/erwanplantec/LNDP
* 29.充分利用视觉信息多问多答合成数据，提升多模态大模型数学推理能力   PaperWeekly  https://mp.weixin.qq.com/s/UcRF5iQWUYo1kCkp_QuXcw \
  Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models \
  https://github.com/HZQ950419/Math-LLaVA
* 30.微软 | 提出MInference算法，单卡A100实现百万token推理，速度快10倍！  AINLPer  https://mp.weixin.qq.com/s/42dyAmsFVleqgexoDez3JQ \
  MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention

# 7.9 Tue
* 31.(**非常重要**)彻底改变语言模型：全新架构TTT超越Transformer，ML模型代替RNN隐藏状态  机器之心  https://mp.weixin.qq.com/s/QSw9PKB_HhSxeO7agnzBgQ \
  大模型最强架构TTT问世！斯坦福UCSD等5年磨一剑， 一夜推翻Transformer  新智元  https://mp.weixin.qq.com/s/Z8BVt7g6rnuAFzoca1fjfg \
  新架构RNN反超Transformer：每个隐藏状态都是一个模型，一作：从根本上改变语言模型  量子位  https://mp.weixin.qq.com/s/iqy29NHgZumPXHVU_0C72A \
  Learning to (Learn at Test Time): RNNs with Expressive Hidden States \
  https://arxiv.org/abs/2407.04620
* 32.RAG微调Llama 3竟超越GPT-4！英伟达GaTech华人学者提出RankRAG框架  新智元  https://mp.weixin.qq.com/s/87qeqDSwtitYsruH_2Jdww \
  RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs \
  https://arxiv.org/abs/2407.02485
* 33.Meta新研究挑战CV领域基操：ViT根本不用patch，用像素做token效果更佳  新智元  https://mp.weixin.qq.com/s/o_Wb3Bt9Maipgczokeinrg \
  An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels \
  https://arxiv.org/abs/2406.09415
* 34.鄂维南院士领衔新作：大模型不止有RAG、参数存储，还有第3种记忆  机器之心  https://mp.weixin.qq.com/s/_7mpswMvpg5sRrIKsF-Vvw \
  Memory3 : Language Modeling with Explicit Memory
* 35.第一次，语言的神经激活被定位到细胞级  机器之心  https://mp.weixin.qq.com/s/Zk82EBEYaenRrBNF4Yy5jQ \
* 36.没想到！AlphaZero式树搜索也能用来增强大语言模型推理与训练  机器之心  https://mp.weixin.qq.com/s/k3HuSPGJFJ223thy316eCg \
  论文名称：AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training \
  论文链接：https://arxiv.org/abs/2309.17179 \
  代码链接：https://github.com/waterhorse1/LLM_Tree_Search
* 37.微软&清华提出全新预训练范式，指令预训练让8B模型实力暴涨！实力碾压70B模型  夕小瑶科技说  https://mp.weixin.qq.com/s/y21ii5AxErt-x3mFsAXW2w \
  Instruction Pre-Training: Language Models are Supervised Multitask Learners

# 7.10 Wed
* 38.DeepMind新方法：训练时间减少13倍，算力降低90%  量子位  https://mp.weixin.qq.com/s/8rkE6Rp2yw31gw0XhFcZXg \
  DeepMind团队提出了一种新的数据筛选方法**JEST**——将AI训练时间减少13倍，并将算力需求降低90%。\
  Data curation via joint example selection further accelerates multimodal learning
* 39.语义熵识破LLM幻觉！牛津大学新研究登Nature  新智元  https://mp.weixin.qq.com/s/fdLZ9DDqG9C_uxAAlKgQbw \
  Detecting hallucinations in large language models using semantic entropy \
  https://www.nature.com/articles/s41586-024-07421-0
* 40.(**值得看看**)LeCun新作：神经网络在实践中的灵活性到底有多大？  新智元  https://mp.weixin.qq.com/s/PjlXwwG3t5Fqp5MfrBVvBQ \
  Just How Flexible are Neural Networks in Practice? \
  https://arxiv.org/pdf/2406.11463 \
  这个灵活性指的是，神经网络拟合训练数据（样本数量）的能力，在实际应用中受到哪些因素的影响。\
  模型拟合数据的能力（或者说学习信息的能力），由有效模型复杂性（EMC）来表示。\
  这个EMC是怎么算的呢？ \
  一开始，在少量样本上训练模型。如果它在训练后达到100%的训练准确率，则将模型重新初始化并增大训练样本数量。\
  迭代执行此过程，每次逐步增加样本量，直到模型不再完全拟合所有训练样本，将模型能实现完美拟合的最大样本量作为网络的EMC。\
  ——一直喂饭，直到吃撑，则得到饭量大小。
* 41.什么可控学习？人大最新《可控学习》综述，信息检索中的方法和应用  专知  https://mp.weixin.qq.com/s/Vgyk6GMpp5RhnF2XU1P46w \
  可控学习（Controllable Learning，CL）作为可信机器学习的关键组成部分，确保学习者能够满足预定义目标，并且能够根据这些目标的变化自适应调整而无需重新训练。
* 42.谷歌 | 提出新型层设计：PEER，可对百万专家进行稀疏检索，超越密集前馈、稀疏MoE  AINLPer  https://mp.weixin.qq.com/s/lhv1HHRw0EvGK71UwX52Qg

# 7.11 Thur
* 43.(**重要**)视觉语言导航：大模型时代的综述  专知  https://mp.weixin.qq.com/s/fYJrwGiI9EbKg4eqdDmxTw \
  Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models \
  https://arxiv.org/pdf/2407.07035
* 44.(**重要**)具身大模型研究综述  专知  https://mp.weixin.qq.com/s/LUv27n9c2LRpjVTRDM-cHQ \
* 45.综述！清华 && 剑桥 | 深入探讨大模型（LLMs）知识冲突的研究进展及挑战  AINLPer  https://mp.weixin.qq.com/s/oRsNUD8L5Fj3_1oYNBGUpw \
  Knowledge Conflicts for LLMs: A Survey
* 46.DeepMind CEO称AI智力不如猫！LeCun持相同看法！曝光谷歌新项目AI将突破聊天限制  夕小瑶科技说  https://mp.weixin.qq.com/s/1oPI5N_0hxq1DT1cbRjR-A \
  LeCun强调，即使是当今最先进的人工智能系统，也比不上一只家猫的常识。他指出，一只家猫的大脑约有8亿个神经元，而神经元之间的连接数量（即突触数量）约为神经元数量的2000倍，这与LLM中的参数数量相当。例如，为免费版ChatGPT提供支持的OpenAI GPT-3.5模型有1750亿个参数，而据说运行在8个语言模型上的更先进的GPT-4，每个模型有2200亿个参数。\
  "所以我们的规模也许与一只猫相当。但为什么这些系统不如猫聪明呢？"LeCun问道。"一只猫能记忆、理解物理世界、规划复杂的动作，并进行一定程度的推理，实际上比最大的LLM要好得多。这表明，我们在概念上还缺少一些重要的东西，以使机器像动物和人类一样智能。"

# 7.12 Fri
* 47.Mamba一作再祭神作，H100利用率飙至75%！FlashAttention三代性能翻倍，比标准注意力快16倍  新智元  https://mp.weixin.qq.com/s/_8kNN1s-Y3DOkv72I4U-Mg
* 48.(**值得看看**)斯坦福炒虾机器人原班人马新作！最强大脑Gemini加持，机器人炫技导航玩出新花样  新智元  https://mp.weixin.qq.com/s/HEiI21RjriHpYkEPyyAHsQ \
  Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs \
  https://arxiv.org/abs/2407.07775v1
* 49.生成式模型不只会「模仿」！哈佛、UCSB等最新成果：性能可超越训练集专家水平  新智元  https://mp.weixin.qq.com/s/S-C8KyozZ7bWhf5CVi_iwA \
  生成式模型原本被设计来模仿人类的各种复杂行为，但人们普遍认为它们最多只能达到与其训练数据中的专家相当的水平。不过，最新的研究突破了这一限制，表明在特定领域，如国际象棋，通过采用低温采样技术，这些模型能够超越它们所学习的那些专家，展现出更高的能力。\
  最近来自哈佛大学、加州大学圣巴巴拉分校（UCSB）、普林斯顿大学的研究结果表明，模型在某些特定的领域可以实现「超越（transcend）训练数据中的专家水平」的性能，青出于蓝而胜于蓝。\
  Transcendence: Generative Models Can Outperform The Experts That Train Them
* 50.AI Agent满级进化！骑马种田、办公修图，样样精通，昆仑万维等发布通用Agent新框架   新智元  https://mp.weixin.qq.com/s/KYci5XDYS7hGHWYj7PwGhQ \
  昆仑万维携手北京智源人工智能研究院、新加坡南洋理工大学、北京大学等顶尖名校机构，联合提出了迄今为止第一个既能玩多种商业游戏又能操作各种软件应用的AI框架——Cradle \
  CRADLE: Empowering Foundation Agents Towards General Computer Control
* 51.ICML 2024 | 南开大学提出反向传播全新改进策略，不降速、大幅提升显存效率  PaperWeekly  https://mp.weixin.qq.com/s/jEtw0T8C0aiJj9ybNHn32A \
  Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation
* 52.AI慢思考蒸馏进快思考，Llama2跃升至GPT-4水平，不写过程也能做对题  量子位  https://mp.weixin.qq.com/s/l-fGuCMvnRngznYbmqOWhA \
  ??? 好奇怎么做到的 \
  Distilling System 2 into System 1 \
  https://arxiv.org/abs/2407.06023

# 7.13 Sat
* 53.大模型剧本杀开源：6个Claude里藏一个凶手！刚上线服务器就被挤爆，免费免下载免注册  量子位  https://mp.weixin.qq.com/s/N474yzjdPdLVl3mdbgOi6w 
* 54.OpenAI新模型「草莓」曝光：强推理/长任务规划/超大规模训练！还给出AGI分级  量子位  https://mp.weixin.qq.com/s/t8qQCirCG5_JggvBOkGdQQ 
* 55.【新书】构建大型语言模型应用：使用大型语言模型创建智能应用和代理，153页pdf  专知  https://mp.weixin.qq.com/s/DwRSsyA9xliSf9ibSAORCQ \
  构建LLM应用 \
  Building LLM Apps 
* 56.OpenAI翁荔提出大模型「外在幻觉」：万字blog详解抵抗办法、产幻原因和检测方式  量子位  https://mp.weixin.qq.com/s/1RF_TEDHg1HoIr00ubreeA \
  OpenAI Lilian Weng万字长文解读LLM幻觉：从理解到克服  机器之心  https://mp.weixin.qq.com/s/UGcui0rLW2Vz7y2Mt4atqA \
  https://lilianweng.github.io/posts/2024-07-07-hallucination/ \
  幻觉有两种类型： \
  上下文内幻觉：模型输出应该与上下文中的源内容一致（出现上下文内幻觉时，输出与源内容不一致）。 \
  外在幻觉：模型输出应该基于预训练数据集。然而，考虑到预训练数据集的规模，检索并识别每次生成的冲突成本过高。如果将预训练数据集看作是世界知识的象征，那么本质上是试图确保模型输出是事实性的并可以通过外部世界知识进行验证。同样重要的是，当模型不了解某个事实时，它应该明确表示不知道。

# 7.14 Sun
* 57.AI大模型有望再扩1000倍！剑桥耶鲁康奈尔：PNN是变革关键  新智元  https://mp.weixin.qq.com/s/XjU-r2rKGWaatObzdh5TGw \
  Training of Physical Neural Networks \
  https://arxiv.org/abs/2406.03372
* 58.(**值得看看**)6700万参数比肩万亿巨兽GPT-4！微软MIT等联手破解Transformer推理密码  新智元  https://mp.weixin.qq.com/s/ySRE3MaEH539vqrWDi6KBQ \
  来自微软、MIT等机构的学者提出了一种创新的训练范式，攻破了大模型的推理缺陷。他们通过因果模型构建数据集，直接教模型学习公理，结果只有67M参数的微型Transformer竟能媲美GPT-4的推理能力 \
  「因果推理」绝对是当前GenAI热潮下的小众领域，但是它有一个大佬级的坚定支持者——Yann LeCun，他在推特上的日常操作之一，就是炮轰Sora等生成模型，并为自己坚信的因果推理领域摇旗呐喊。 \
  Teaching Transformers Causal Reasoning through Axiomatic Training \
  https://arxiv.org/abs/2407.07612v1
* 59.WizardLM新作！ArenaLearning: 通过模拟LLM竞技场来构建大规模数据飞轮  PaperWeekly  https://mp.weixin.qq.com/s/9k9k9hK9wGF8mcR9VuTsCA \
  Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena

# 7.15 Mon
* 60.
* 61.
* 62.
* 63.
* 64.
* 65.
* 66.
* 67.
* 68.
* 69.
* 70.

# 7.16 Tue
# 7.17 Wed
# 7.18 Thur
# 7.19 Fri
# 7.20 Sat
# 7.21 Sun

# 7.22 Mon
# 7.23 Tue
# 7.24 Wed
# 7.25 Thur
# 7.26 Fri
# 7.27 Sat
# 7.28 Sun

# 7.29 Mon
# 7.30 Tue
# 7.31 Wed
