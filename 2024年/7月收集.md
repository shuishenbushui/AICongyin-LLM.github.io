# 7.1 Mon
* 1.等不来OpenAI的Q*，华为诺亚探索LLM推理的秘密武器MindStar先来了  机器之心  https://mp.weixin.qq.com/s/bXZLC1ogVgqwFpNToGIBxw 
* 2.ICML 2024 Spotlight | 在解码中重新对齐，让语言模型更少幻觉、更符合人类偏好  机器之心  https://mp.weixin.qq.com/s/-9MjgNOLRrUdaQUF5tVv9w

# 7.2 Tue
* 3.大型语言模型中的人格综述  专知  https://mp.weixin.qq.com/s/G046kA5aVcMSSUB6HT-FZg 
* 4.只需将感知推理能力拆分，2B大模型就能战胜20B！国产新框架高效处理视觉任务  量子位  https://mp.weixin.qq.com/s/wiTLIio53j7fEZBBkUAZ-A \
  Prism: 显式地解耦了视觉语言模型（VLM） 的感知和推理
* 5.73年前，香农已经给大模型发展埋下一颗种子  机器之心  https://mp.weixin.qq.com/s/3VIdDWkk4iISNiXPOl-Tfg \
  普林斯顿大学教授承现峻（Sebastian Seung）抛出了这样一个观点：1951 年，在贝尔实验室（总部位于新泽西州 Murray Hill）工作的克劳德・香农提出了预测下一个单词的问题，这成为了当前大语言模型（LLM）的种子
* 6.哈工大提出创新迭代推理框架 DPE-MNER ：充分发挥多模态表示潜力  机器之心  https://mp.weixin.qq.com/s/nKQGTaTHjkW6daZfFKAi_g
* 7.神经网络可能不再需要激活函数？Layer Normalization也具有非线性表达！  机器之心  https://mp.weixin.qq.com/s/YRAcAKouScQGt3lOxe8fBQ \
  《On the Nonlinearity of Layer Normalization》 \
  论文地址：https://arxiv.org/abs/2406.01255
* 8.Andrej Karpathy提出未来计算机2.0构想： 完全由神经网络驱动！网友炸锅了  夕小瑶科技说  https://mp.weixin.qq.com/s/ZOhvA66bB2r6eD2eQFEjqA
* 9.谷歌重磅：告别RAG，长上下文的大语言模型无需检索增强  夕小瑶科技说  https://mp.weixin.qq.com/s/lOORnoyrA8lqOEXWKxhAQg \
  论文标题：Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More? \
  论文链接：https://arxiv.org/pdf/2406.13121
* 10.扩散模型与表示学习：综述  专知  https://mp.weixin.qq.com/s/jWMrOTlMLNY7yRtzBLoi0Q \
  Diffusion Models and Representation Learning: A Survey

# 7.3 Wed
* 11.大语言模型增强知识表示学习综述  专知  https://mp.weixin.qq.com/s/AmttGtvKGIxdUKCKoNAnBw \
  大语言模型（LLMs）与知识表示学习（KRL）的整合，标志着人工智能领域的重要进展，增强了捕捉和利用复杂知识结构的能力。这种协同作用利用了LLMs的高级语言和语境理解能力，以提升KRL的准确性、适应性和效能，从而扩展其应用和潜力
* 12.参数少80%，效果仍超LoRA！上交大&上海AI Lab推出高效微调框架FLoRA  量子位  https://mp.weixin.qq.com/s/x2NID0EsUiNLC5RJ01s-wg
* 13.复旦大学：一个小技巧探测大模型的知识边界，有效消除幻觉  夕小瑶科技说  https://mp.weixin.qq.com/s/nrRZwvsxXW2TT_So2jlB-w \
  复旦大学提出了COKE方法( Confidence-derived Knowledge boundary Expression)，通过利用模型内部的置信信号，教导LLMs识别和表达其知识边界，从而减少幻觉现象。实验结果表明，COKE显著提升了模型在域内和域外的表现，使模型能够在回答已知问题的同时，坦诚面对其未知领域的问题。\
  简而言之，就是增加大模型“知道自己不知道”的知识，减少大模型“不知道自己不知道”的知识。就比如图中的大模型在面对自己未知的问题“《迷失太空2018》中的机器人是谁”时，它如果不知道，就应该给出“不知道”，而不是编造一个“Max Robinson”的答案。\
  论文标题：Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals \
  论文链接https://arxiv.org/abs/2406.10881
* 14.万字长文：意识的大一统理论要来了吗？| 追问顶刊   追问nextquestion  https://mp.weixin.qq.com/s/OM-_-yjfHuUug2OXcZEv4w
* 15.通研院研究发现大语言模型在心智推理和行为规划上显著落后于人类  北京通用人工智能研究院  https://mp.weixin.qq.com/s/paYcSJbiZMbU3Vdv2-5x8Q \
  Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities

# 7.4 Thur
* 16.多模态视觉-语言大模型的架构演进 Ⅱ  关于NLP那些你不知道的事  https://mp.weixin.qq.com/s/BALAiCIANhLBx8lSIrwahQ
* 17.13瓦功耗处理10亿参数，接近大脑效率，消除LLM中的矩阵乘法来颠覆AI现状  ScienceAI  https://mp.weixin.qq.com/s/xZuLePQ9g_1Zru2rrHNN-A
* 18.强强联合！当RAG遇到长上下文，滑铁卢大学发布LongRAG，效果领先GPT-4 Turbo 50%  夕小瑶科技说  https://mp.weixin.qq.com/s/e7yvnj9UK3KLcUkwnqjGaw \
  LongRAG \
  https://arxiv.org/pdf/2406.15319.pdf
* 19.Kimi论文自曝推理架构，80%流量都靠它承担  量子位  https://mp.weixin.qq.com/s/u9RG7zuDUO62CbrmGyZtOA \
  Kimi背后的推理架构名叫Mooncake（月饼），主要特点是采取了分离式的设计方案
* 20.8人半年肝出开源版GPT-4o，0延迟演示全网沸腾！背后技术揭秘，人人免费用  机器学习研究组订阅  https://mp.weixin.qq.com/s/CSa5euhZReHXOGS_bUvoYw \
  传送门：https://moshi.chat/?queue_id=talktomoshi

# 7.5 Fri
* 21.「吗喽」在想啥？AI读心术精准重建猕猴大脑图像，网友：我们成三体人了  新智元  https://mp.weixin.qq.com/s/Sl-qTrcjYc17yQUDCGdQqA

# 7.6 Sat
* 22.Adam有了mini版：内存占用少一半，吞吐量提升50%  机器之心  https://mp.weixin.qq.com/s/LOhtmS9MCZSsRBDAq2mEuQ \
  Adam-mini: Use Fewer Learning Rates To Gain More
* 23.参数更新量仅为LoRA的5%，性能不减反升！南加大提出高效精调法LaMDA  夕小瑶科技说  https://mp.weixin.qq.com/s/H8mQxtpRj0P8sRnkCg0bIg \
  LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation
* 24.上交&阿里：掀开多模态大模型的头盖骨，解密黑盒模型推理过程  夕小瑶科技说  https://mp.weixin.qq.com/s/dF4QltattjndvXMX92gutA \
  From Redundancy to Relevance: Enhancing Explainability in Multimodal Large Language Models \
  本文运用Grad-CAM方法对MLLMs的复杂推理过程进行了逐层可视化研究，深入探究了图像与用户提示之间的交互和信息流动，得出了一些有趣的结论：如LLaVA1.5和Qwen-vl等模型中，浅层图像Token的信息流存在明显冗余。MLLMs可能在浅层捕获输入的含义而在深层进行推理获得答案等。相信该研究有助于人们更好地理解多模态复杂推理的内在机制。
* 25.清华北航博士生「强迫」Gemma-2说中文！弱智吧、角色扮演、数学问题表现惊喜  机器学习研究组订阅  https://mp.weixin.qq.com/s/-QgtXysWHhzD-KQV5OLkOg \
  清华大学的一名人工智能博士生王慎执就在X上隆重推出了一款微调模型Gemma-2-9B-Chinese-Chat

# 7.7 Sun

# 7.8 Mon
* 26.核心代码仅三行！即插即用的视觉语言连接器，一键提升多模态大模型  PaperWeekly  https://mp.weixin.qq.com/s/lEMHTxKrK1er1EmzGC4uSw \
  Dense Connector for MLLMs
* 27.(**有趣**)哈佛DeepMind开辟「虚拟神经科学」新领域！在世界模拟器驯养「赛博老鼠」  新智元  https://mp.weixin.qq.com/s/Tk9U4pS7WNqRX8gZklkREQ \
* 28.像生物网络一样「生长」，具备「结构可塑性」的自组织神经网络来了  机器之心  https://mp.weixin.qq.com/s/fdQcKpuu75C3lgFkxfpwdQ \
  Evolving Self-Assembling Neural Networks: From Spontaneous Activity to Experience-Dependent Learning \
  论文链接：https://arxiv.org/pdf/2406.09787 \
  项目链接：https://github.com/erwanplantec/LNDP
* 29.充分利用视觉信息多问多答合成数据，提升多模态大模型数学推理能力   PaperWeekly  https://mp.weixin.qq.com/s/UcRF5iQWUYo1kCkp_QuXcw \
  Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models \
  https://github.com/HZQ950419/Math-LLaVA
* 30.微软 | 提出MInference算法，单卡A100实现百万token推理，速度快10倍！  AINLPer  https://mp.weixin.qq.com/s/42dyAmsFVleqgexoDez3JQ \
  MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention

# 7.9 Tue
* 31.(**非常重要**)彻底改变语言模型：全新架构TTT超越Transformer，ML模型代替RNN隐藏状态  机器之心  https://mp.weixin.qq.com/s/QSw9PKB_HhSxeO7agnzBgQ \
  大模型最强架构TTT问世！斯坦福UCSD等5年磨一剑， 一夜推翻Transformer  新智元  https://mp.weixin.qq.com/s/Z8BVt7g6rnuAFzoca1fjfg \
  新架构RNN反超Transformer：每个隐藏状态都是一个模型，一作：从根本上改变语言模型  量子位  https://mp.weixin.qq.com/s/iqy29NHgZumPXHVU_0C72A \
  Learning to (Learn at Test Time): RNNs with Expressive Hidden States \
  https://arxiv.org/abs/2407.04620
* 32.RAG微调Llama 3竟超越GPT-4！英伟达GaTech华人学者提出RankRAG框架  新智元  https://mp.weixin.qq.com/s/87qeqDSwtitYsruH_2Jdww \
  RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs \
  https://arxiv.org/abs/2407.02485
* 33.Meta新研究挑战CV领域基操：ViT根本不用patch，用像素做token效果更佳  新智元  https://mp.weixin.qq.com/s/o_Wb3Bt9Maipgczokeinrg \
  An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels \
  https://arxiv.org/abs/2406.09415
* 34.鄂维南院士领衔新作：大模型不止有RAG、参数存储，还有第3种记忆  机器之心  https://mp.weixin.qq.com/s/_7mpswMvpg5sRrIKsF-Vvw \
  Memory3 : Language Modeling with Explicit Memory
* 35.第一次，语言的神经激活被定位到细胞级  机器之心  https://mp.weixin.qq.com/s/Zk82EBEYaenRrBNF4Yy5jQ \
* 36.没想到！AlphaZero式树搜索也能用来增强大语言模型推理与训练  机器之心  https://mp.weixin.qq.com/s/k3HuSPGJFJ223thy316eCg \
  论文名称：AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training \
  论文链接：https://arxiv.org/abs/2309.17179 \
  代码链接：https://github.com/waterhorse1/LLM_Tree_Search
* 37.微软&清华提出全新预训练范式，指令预训练让8B模型实力暴涨！实力碾压70B模型  夕小瑶科技说  https://mp.weixin.qq.com/s/y21ii5AxErt-x3mFsAXW2w \
  Instruction Pre-Training: Language Models are Supervised Multitask Learners

# 7.10 Wed
* 38.DeepMind新方法：训练时间减少13倍，算力降低90%  量子位  https://mp.weixin.qq.com/s/8rkE6Rp2yw31gw0XhFcZXg \
  DeepMind团队提出了一种新的数据筛选方法**JEST**——将AI训练时间减少13倍，并将算力需求降低90%。\
  Data curation via joint example selection further accelerates multimodal learning
* 39.语义熵识破LLM幻觉！牛津大学新研究登Nature  新智元  https://mp.weixin.qq.com/s/fdLZ9DDqG9C_uxAAlKgQbw \
  Detecting hallucinations in large language models using semantic entropy \
  https://www.nature.com/articles/s41586-024-07421-0
* 40.(**值得看看**)LeCun新作：神经网络在实践中的灵活性到底有多大？  新智元  https://mp.weixin.qq.com/s/PjlXwwG3t5Fqp5MfrBVvBQ \
  Just How Flexible are Neural Networks in Practice? \
  https://arxiv.org/pdf/2406.11463 \
  这个灵活性指的是，神经网络拟合训练数据（样本数量）的能力，在实际应用中受到哪些因素的影响。\
  模型拟合数据的能力（或者说学习信息的能力），由有效模型复杂性（EMC）来表示。\
  这个EMC是怎么算的呢？ \
  一开始，在少量样本上训练模型。如果它在训练后达到100%的训练准确率，则将模型重新初始化并增大训练样本数量。\
  迭代执行此过程，每次逐步增加样本量，直到模型不再完全拟合所有训练样本，将模型能实现完美拟合的最大样本量作为网络的EMC。\
  ——一直喂饭，直到吃撑，则得到饭量大小。
* 41.什么可控学习？人大最新《可控学习》综述，信息检索中的方法和应用  专知  https://mp.weixin.qq.com/s/Vgyk6GMpp5RhnF2XU1P46w \
  可控学习（Controllable Learning，CL）作为可信机器学习的关键组成部分，确保学习者能够满足预定义目标，并且能够根据这些目标的变化自适应调整而无需重新训练。
* 42.谷歌 | 提出新型层设计：PEER，可对百万专家进行稀疏检索，超越密集前馈、稀疏MoE  AINLPer  https://mp.weixin.qq.com/s/lhv1HHRw0EvGK71UwX52Qg

# 7.11 Thur
* 43.(**重要**)视觉语言导航：大模型时代的综述  专知  https://mp.weixin.qq.com/s/fYJrwGiI9EbKg4eqdDmxTw \
  Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models \
  https://arxiv.org/pdf/2407.07035
* 44.(**重要**)具身大模型研究综述  专知  https://mp.weixin.qq.com/s/LUv27n9c2LRpjVTRDM-cHQ \
* 45.综述！清华 && 剑桥 | 深入探讨大模型（LLMs）知识冲突的研究进展及挑战  AINLPer  https://mp.weixin.qq.com/s/oRsNUD8L5Fj3_1oYNBGUpw \
  Knowledge Conflicts for LLMs: A Survey
* 46.DeepMind CEO称AI智力不如猫！LeCun持相同看法！曝光谷歌新项目AI将突破聊天限制  夕小瑶科技说  https://mp.weixin.qq.com/s/1oPI5N_0hxq1DT1cbRjR-A \
  LeCun强调，即使是当今最先进的人工智能系统，也比不上一只家猫的常识。他指出，一只家猫的大脑约有8亿个神经元，而神经元之间的连接数量（即突触数量）约为神经元数量的2000倍，这与LLM中的参数数量相当。例如，为免费版ChatGPT提供支持的OpenAI GPT-3.5模型有1750亿个参数，而据说运行在8个语言模型上的更先进的GPT-4，每个模型有2200亿个参数。\
  "所以我们的规模也许与一只猫相当。但为什么这些系统不如猫聪明呢？"LeCun问道。"一只猫能记忆、理解物理世界、规划复杂的动作，并进行一定程度的推理，实际上比最大的LLM要好得多。这表明，我们在概念上还缺少一些重要的东西，以使机器像动物和人类一样智能。"

# 7.12 Fri
* 47.Mamba一作再祭神作，H100利用率飙至75%！FlashAttention三代性能翻倍，比标准注意力快16倍  新智元  https://mp.weixin.qq.com/s/_8kNN1s-Y3DOkv72I4U-Mg
* 48.(**值得看看**)斯坦福炒虾机器人原班人马新作！最强大脑Gemini加持，机器人炫技导航玩出新花样  新智元  https://mp.weixin.qq.com/s/HEiI21RjriHpYkEPyyAHsQ \
  Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs \
  https://arxiv.org/abs/2407.07775v1
* 49.生成式模型不只会「模仿」！哈佛、UCSB等最新成果：性能可超越训练集专家水平  新智元  https://mp.weixin.qq.com/s/S-C8KyozZ7bWhf5CVi_iwA \
  生成式模型原本被设计来模仿人类的各种复杂行为，但人们普遍认为它们最多只能达到与其训练数据中的专家相当的水平。不过，最新的研究突破了这一限制，表明在特定领域，如国际象棋，通过采用低温采样技术，这些模型能够超越它们所学习的那些专家，展现出更高的能力。\
  最近来自哈佛大学、加州大学圣巴巴拉分校（UCSB）、普林斯顿大学的研究结果表明，模型在某些特定的领域可以实现「超越（transcend）训练数据中的专家水平」的性能，青出于蓝而胜于蓝。\
  Transcendence: Generative Models Can Outperform The Experts That Train Them
* 50.AI Agent满级进化！骑马种田、办公修图，样样精通，昆仑万维等发布通用Agent新框架   新智元  https://mp.weixin.qq.com/s/KYci5XDYS7hGHWYj7PwGhQ \
  昆仑万维携手北京智源人工智能研究院、新加坡南洋理工大学、北京大学等顶尖名校机构，联合提出了迄今为止第一个既能玩多种商业游戏又能操作各种软件应用的AI框架——Cradle \
  CRADLE: Empowering Foundation Agents Towards General Computer Control
* 51.ICML 2024 | 南开大学提出反向传播全新改进策略，不降速、大幅提升显存效率  PaperWeekly  https://mp.weixin.qq.com/s/jEtw0T8C0aiJj9ybNHn32A \
  Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation
* 52.AI慢思考蒸馏进快思考，Llama2跃升至GPT-4水平，不写过程也能做对题  量子位  https://mp.weixin.qq.com/s/l-fGuCMvnRngznYbmqOWhA \
  ??? 好奇怎么做到的 \
  Distilling System 2 into System 1 \
  https://arxiv.org/abs/2407.06023

# 7.13 Sat
* 53.大模型剧本杀开源：6个Claude里藏一个凶手！刚上线服务器就被挤爆，免费免下载免注册  量子位  https://mp.weixin.qq.com/s/N474yzjdPdLVl3mdbgOi6w 
* 54.OpenAI新模型「草莓」曝光：强推理/长任务规划/超大规模训练！还给出AGI分级  量子位  https://mp.weixin.qq.com/s/t8qQCirCG5_JggvBOkGdQQ 
* 55.【新书】构建大型语言模型应用：使用大型语言模型创建智能应用和代理，153页pdf  专知  https://mp.weixin.qq.com/s/DwRSsyA9xliSf9ibSAORCQ \
  构建LLM应用 \
  Building LLM Apps 
* 56.OpenAI翁荔提出大模型「外在幻觉」：万字blog详解抵抗办法、产幻原因和检测方式  量子位  https://mp.weixin.qq.com/s/1RF_TEDHg1HoIr00ubreeA \
  OpenAI Lilian Weng万字长文解读LLM幻觉：从理解到克服  机器之心  https://mp.weixin.qq.com/s/UGcui0rLW2Vz7y2Mt4atqA \
  https://lilianweng.github.io/posts/2024-07-07-hallucination/ \
  幻觉有两种类型： \
  上下文内幻觉：模型输出应该与上下文中的源内容一致（出现上下文内幻觉时，输出与源内容不一致）。 \
  外在幻觉：模型输出应该基于预训练数据集。然而，考虑到预训练数据集的规模，检索并识别每次生成的冲突成本过高。如果将预训练数据集看作是世界知识的象征，那么本质上是试图确保模型输出是事实性的并可以通过外部世界知识进行验证。同样重要的是，当模型不了解某个事实时，它应该明确表示不知道。

# 7.14 Sun
* 57.AI大模型有望再扩1000倍！剑桥耶鲁康奈尔：PNN是变革关键  新智元  https://mp.weixin.qq.com/s/XjU-r2rKGWaatObzdh5TGw \
  Training of Physical Neural Networks \
  https://arxiv.org/abs/2406.03372
* 58.(**值得看看**)6700万参数比肩万亿巨兽GPT-4！微软MIT等联手破解Transformer推理密码  新智元  https://mp.weixin.qq.com/s/ySRE3MaEH539vqrWDi6KBQ \
  公理训练让LLM学会因果推理：6700万参数模型比肩万亿参数级GPT-4  机器之心  https://mp.weixin.qq.com/s/Yera281WG8RNAOrRUYjH6g \
  来自微软、MIT等机构的学者提出了一种创新的训练范式，攻破了大模型的推理缺陷。他们通过因果模型构建数据集，直接教模型学习公理，结果只有67M参数的微型Transformer竟能媲美GPT-4的推理能力 \
  「因果推理」绝对是当前GenAI热潮下的小众领域，但是它有一个大佬级的坚定支持者——Yann LeCun，他在推特上的日常操作之一，就是炮轰Sora等生成模型，并为自己坚信的因果推理领域摇旗呐喊。 \
  Teaching Transformers Causal Reasoning through Axiomatic Training \
  https://arxiv.org/abs/2407.07612v1
* 59.WizardLM新作！ArenaLearning: 通过模拟LLM竞技场来构建大规模数据飞轮  PaperWeekly  https://mp.weixin.qq.com/s/9k9k9hK9wGF8mcR9VuTsCA \
  Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena

# 7.15 Mon
* 60.MoE也有Scaling Law，「百万专家」利用率近100%！DeepMind华人挑战MoE极限  机器学习研究组订阅  https://mp.weixin.qq.com/s/RkItkFgubd_u3sGINNrXYQ \
  Mixture of A Million Experts \
  https://arxiv.org/abs/2407.04153
* 61.微软开源的GraphRAG爆火，Github Star量破万，生成式AI进入知识图谱时代？  机器学习研究组订阅  https://mp.weixin.qq.com/s/YSpo3jluiysMd1-pyEcBEw \
  LLM 很强大，但也存在一些明显缺点，比如幻觉问题、可解释性差、抓不住问题重点、隐私和安全问题等。检索增强式生成（RAG）可大幅提升 LLM 的生成质量和结果有用性。\
  本月初，微软发布最强 RAG 知识库开源方案 GraphRAG，项目上线即爆火，现在星标量已经达到 10.5 k。\
  项目地址：https://github.com/microsoft/graphrag \
  官方文档：https://microsoft.github.io/graphrag/

# 7.16 Tue
* 62.神经网络架构「殊途同归」？ICML 2024论文：模型不同，但学习内容相同  新智元  https://mp.weixin.qq.com/s/XIkKLq1MHFBXjzwbkkF6RA \
  深度神经网络有多种规模和架构，大家普遍认为这会影响到模型学习到的抽象表示。然而，UCL两位学者发表在ICML 2024上第一篇论文指出，如果模型的架构足够灵活，某些网络行为在不同架构间是广泛存在的 \
  Scaling Laws for Neural Language Models
* 63.谷歌机器人专家：机器人在现实中碰过的壁，AI也会碰  机器之心  https://mp.weixin.qq.com/s/rW0qzhCBTRypNG8hPShAhA 

# 7.17 Wed
* 64.对齐全量微调！这是我看过最精彩的LoRA改进  PaperWeekly  https://mp.weixin.qq.com/s/61uN9UxNo6SoVukrpKs9_Q \
  LoRA-GA: Low-Rank Adaptation with Gradient Approximatio
* 65.原作亲自下场！Mistral首款开源7B Mamba模型「埃及艳后」效果惊艳  新智元  https://mp.weixin.qq.com/s/2SJPicNAjyZN2jwo8QC7xw
* 66.「图Transformers」综述  专知  https://mp.weixin.qq.com/s/pyiY-qR4UgFP8uNEplIjMQ \
  Graph Transformers: A Survey
* 67.大脑如何处理语言？普林斯顿团队对Transformer模型进行分析  ScienceAI  https://mp.weixin.qq.com/s/VWfje_q5aY-1rmlz8ZSA4w

# 7.18 Thur
* 68.GitHub星标超16万，爆火AutoGPT进阶版来了：定制节点、多智能体协同  机器之心  https://mp.weixin.qq.com/s/dBL47yYoVNkyPoPG8pcLLA \
  GitHub 地址：https://github.com/Significant-Gravitas/AutoGPT
* 69.LoRA综述来了! 浙大《大语言模型的LoRA研究》综述  专知  https://mp.weixin.qq.com/s/sb0j6wTHI7YtELzoQ9wCpw \
  A Survey on LoRA of Large Language Models
* 70.Meta发布混合多模态模型—Chameleon  AIGC开放社区  https://mp.weixin.qq.com/s/DM9rfoy27CeNJZ70kp6-lQ \
  Chameleon: Mixed-Modal Early-Fusion Foundation Models \
  https://arxiv.org/abs/2405.09818
* 71.OpenAI发布PVG：用小模型验证大模型输出，解决“黑盒”难题  AIGC开放社区  https://mp.weixin.qq.com/s/Gov3eCwkRTndy-iyCARy3w \
  OpenAI超级对齐团队再发「绝唱」！首提「证明者-验证者」博弈，训练GPT说人话  新智元  https://mp.weixin.qq.com/s/u8FL5mcmkdQkNMavyUHlzA \
  Prover-Verifier-Games: 在这个机制中， Prover（证明者）的任务就是生成内容，而Verifier（验证者）的任务就是判断这些内容是否正确 \
  OpenAI论文地址：https://cdn.openai.com/prover-verifier-games-improve-legibility-of-llm-outputs/legibility.pdf

# 7.19 Fri
* 72.大模型“自学”后能力反下降，Llama/Mistral都没逃过  量子位  https://mp.weixin.qq.com/s/qDF9FfuSrYwsdJbWNBXF-g \
  Progress or Regress？Self-Improvement Reversal in Post-training 
* 73.小模型卷起来了：Mistral联合英伟达开源12B小模型，128k上下文  机器之心  https://mp.weixin.qq.com/s/7oSxdFyqJ7MUpbfuNB_n5Q \
  英伟达Mistral AI联袂出击！120亿小模型王者强势登场，碾压Llama 3单张4090可跑  机器学习研究组订阅  https://mp.weixin.qq.com/s/9937EG8uImyxjCOtONeR2w \
  支持128K上下文 !

# 7.20 Sat
* 74.ACL 2024 | Parrot（鹦鹉）：增强大语言模型在多轮对话中的指令跟随能力  PaperWeekly  https://mp.weixin.qq.com/s/bC8ydkbsY1T4X8tvqXmjhw \
  Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models
* 75.大一统！深度学习和传统机器学习终迎来统一的RPN理论框架表示   PaperWeekly  https://mp.weixin.qq.com/s/dhdod5qLK7MPEhcRhVop5A \
  RPN: Reconciled Polynomial Network Towards Unifying PGMs, Kernel SVMs, MLP and KAN

# 7.21 Sun
* 76.基于强化学习的扩散模型微调：教程与综述  专知  https://mp.weixin.qq.com/s/nAhLiRcCB_Po1lsJFigWSw \
  Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review

# 7.22 Mon
* 77.大模型时代结束？大佬齐预测：AI模型或需先缩小规模，才能再次扩大规模  新智元  https://mp.weixin.qq.com/s/AQdZkdISilwzx0ui_mHxmg \
  GPT-4o mini和Mistral NeMo \
  正如Karpathy所说，经过海量数据训练出来的超大模型（如GPT-4），大部分其实是用来记住大量的无关紧要细节的，也就是死记硬背资料。\
  这与模型预训练的目的有关，在预训练阶段，模型被要求尽可能准确的复述接下来的内容，这相当于背课文，背的越准得分越高。\
  虽然，模型能学会里面反复出现的知识，但是，数据资料有时也会出现错误和偏见，模型还要先全部记住再进行微调。 
* 78.从空间智能到具身智能，跨维践行Sim2Real AI最高效路径  机器之心  https://mp.weixin.qq.com/s/8duAecqCQHx91RuADRmNug 
* 79.盛名一时的BERT哪去了？这个问题的答案昭示了LLM范式的转变  机器之心  https://mp.weixin.qq.com/s/fKeorQYwRlmmepuG1_aJlQ 
* 80.“具身智能小镇”来了！机器人逛超市买菜满街跑，AI充当NPC，来自上海AI Lab  量子位  https://mp.weixin.qq.com/s/SIIcNrCFYcUYZmHGckEqeg \
  Grounded 3D-LLM \
  论文地址：https://arxiv.org/abs/2407.10943 \
  GitHub地址：https://github.com/openrobotlab/grutopia?tab=readme-ov-file
* 81.苹果开源7B大模型，训练过程数据集一口气全给了，网友：开放得不像苹果  量子位  https://mp.weixin.qq.com/s/JTCnjvkSqwAC2uRWEEK08g \
  apple DCLM-7B
* 82.(**可以看看**)可「自主进化」的Agent？首个端到端智能体符号化训练框架开源了  PaperWeekly  https://mp.weixin.qq.com/s/8juRBFEd-VmoGUUDKylSGg \
  Symbolic Learning Enables Self-Evolving Agents

# 7.23 Tue
* 83.首个超越GPT4o级开源模型！Llama 3.1泄密：4050亿参数，下载链接、模型卡都有了  机器之心  https://mp.weixin.qq.com/s/sg42mHf-5Jneu3GkdW5wBA 
* 84.(**非常值得看看**)神经网络也有空间意识！学会在Minecraft创建地图，登上Nature子刊  机器之心  https://mp.weixin.qq.com/s/NEbi8YxiH3_Y7sSGDO5w8g \
  神经网络能够学习 Minecraft 世界中的物体彼此之间是如何组织的，并且能够「预测」在空间中移动时会遇到的环境。\
  Automated construction of cognitive maps with predictive coding \
  论文地址：https://www.nature.com/articles/s42256-024-00863-1 \
  代码地址：https://github.com/jgornet/predictive-coding-recovers-maps
* 85.真相了！大模型解数学题和人类真不一样：死记硬背、知识欠缺明显，GPT-4o表现最佳  机器之心  https://mp.weixin.qq.com/s/uU1lZV0Ymj31cmZryhffyQ \
  WE-MATH: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?
* 86.(**非常值得看看**)无限生成视频，还能规划决策，扩散强制整合下一token预测与全序列扩散  机器之心  https://mp.weixin.qq.com/s/kz4RvqdK6nGtA11y5nq5xQ \
  Diffusion Forcing:Next-token Prediction Meets Full-Sequence Diffusion \
  项目网站：https://boyuan.space/diffusion-forcing \
  代码地址：https://github.com/buoyancy99/diffusion-forcing
* 88.(**可以看看**)ICML2024: 华中科大发现大模型具有自我认知  夕小瑶科技说  https://mp.weixin.qq.com/s/jfuL2uEflSie4XerSJRrIQ \
  Self-Cognition in Large Language Models: An Exploratory Study
* 89.长上下文能力只是吹牛？最强GPT-4o正确率仅55.8%，开源模型不如瞎蒙  新智元  https://mp.weixin.qq.com/s/ZZpODFWZw4hu8tQtLvZtcg \
  近日，有两篇独立研究分别表明：长上下文水分很大！LLM实际上并不能「理解」内容
* 90.模拟的宇宙？如何逃脱？  CreateAMind  https://mp.weixin.qq.com/s/fXQPowX3lDk-RwjBjN8Bcg


# 7.24 Wed
* 91.【ICML2024教程】理解大型语言模型在规划中的作用，138页pdf  专知  https://mp.weixin.qq.com/s/8-Y_wCm_j10-aCwdVd4nMw \
  On the role of Large Language Models in Planning

# 7.25 Thur
* 92.释放比特自由——Wolfram的“一种新科学”介绍  集智俱乐部  https://mp.weixin.qq.com/s/vxFr2HekMU-Pb_OaadGKlA \
  计算机科学、数学家和理论物理学家 Stephen Wolfram 的开创性著作《一种新科学》（A New Kind of Science）
* 93.开源大模型杀疯了！Mistral新模型三分之一参数卷爆Llama 3.1，“新趋势已显而易见”   量子位  https://mp.weixin.qq.com/s/LM_WMyWZSRTsbUTX_s7Otw \
  Mistral AI发布最新模型Mistral Large 2，参数123B，用不到三分之一的参数量性能比肩Llama 3.1 405B，也不逊于GPT-4o、Claude 3 Opus等闭源模型
* 94.(**值得了解**)终于有人把大模型的内部一致性和自反馈讲明白了  夕小瑶科技说  https://mp.weixin.qq.com/s/fSc0Szi-zO6YVwp2oV8Uhg \
  Internal Consistency and Self-Feedback in Large Language Models: A Survey \
  ???什么是LLM的内部一致性和自反馈性 \
  内部一致性（Internal Consistency）框架：提出了一个理论框架，用于解释大型语言模型（LLMs）在生成文本时出现的推理不足和幻觉内容等问题。评估LLMs的潜在层、解码层和响应层之间的一致性，通过采样方法进行分析。\
  自我反馈（Self-Feedback）框架：自我反馈框架包括自我评估和自我更新两个模块。自我评估模块用于捕捉内部一致性信号，检查模型的响应和内部状态。自我更新模块利用自我评估捕获的信号来增强模型的响应或模型本身，以提升推理能力和减少幻觉。
* 95.(**值得看看**)减轻幻觉新SOTA，7B模型自迭代训练效果超越GPT-4，上海AI lab发布  夕小瑶科技说  https://mp.weixin.qq.com/s/M3dAx9PSP8x7NA1HC85zzA \
  ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models
* 96.大模型智障检测+1：Strawberry有几个r纷纷数不清，最新最强Llama3.1也傻了  量子位  https://mp.weixin.qq.com/s/QX87ChDMxIecFlhywOR50A 
* 97.打乱/跳过Transformer层会怎样？最新研究揭开其信息流动机制，一口气解答8大问题  量子位  https://mp.weixin.qq.com/s/9v2KqHwbbaTLsr53xt-PfQ \
  不是所有层都是必要的，至少省略部分中间层不会对整体性能造成严重影响
* 98.ICML 2024 | DPO是否比PPO更适合LLM？清华吴翼团队最新揭秘  PaperWeekly  https://mp.weixin.qq.com/s/EOgoi4QYFHlRCGY-EcfDPA
* 99.AI训AI惨遭投毒9次大崩溃，牛津剑桥等惊天发现登Nature封面！  新智元  https://mp.weixin.qq.com/s/1kUNJDqW6R5lSDH_2dM-sA \
  AI models collapse when trained on recursively generated data \
  论文地址：https://www.nature.com/articles/s41586-024-07566-y \
  研究者发现，如果在训练中不加区别地使用AI产生的内容，模型就会出现不可逆转的缺陷——原始内容分布的尾部（低概率事件）会消失！这种效应，被称为「模型崩溃」。合成数据就像是近亲繁殖，会产生质量低劣的后代。
* 100.(**值得看看**)大型语言模型对齐技术综述：RLHF、RLAIF、PPO、DPO 等  专知  https://mp.weixin.qq.com/s/dYSOoMs38bX0WALcyNpy9Q \
  A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More 

# 7.26 Fri
* 101.(**可以看看**)仅微调0.02%参数，性能接近全量微调！上海交大推出高效微调统一新范式 
 PaperWeekly  https://mp.weixin.qq.com/s/hH03e9N8BYiFOaAcJEicnQ \
  See Further for Parameter Efficient Fine-tuning by Standing on the Shoulders of Decomposition
* 102.(**具身智能综述**)全球首篇！调研近400篇文献，鹏城实验室&中大深度解析具身智能  机器之心  https://mp.weixin.qq.com/s/nRpIRi6dIEwG2NLgVDDkiQ \
  多模态大模型时代的全球首篇《具身智能》综述  专知  https://mp.weixin.qq.com/s/INSiUddgJVYEe9aist8pEQ \
  Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI \
  具身智能 Paper List: https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List

# 7.27 Sat
* 103.贾扬清点赞：3K star量的SGLang上新，加速Llama 405B推理秒杀vLLM、TensorRT-LLM  机器之心  https://mp.weixin.qq.com/s/FYwguU3USf12Wb5HXaHH3A \
* 104.ECCV 2024｜是真看到了，还是以为自己看到了？多模态大模型对文本预训练知识的过度依赖该解决了  机器之心  https://mp.weixin.qq.com/s/_R8t5RtIzmTvZhZc4cAwqw
* 105.大规模语言模型中的知识机制：综述与展望  专知  https://mp.weixin.qq.com/s/EwwJ9_gLvuW8b9mxdBdQaA \
  Knowledge Mechanisms in Large Language Models:A Survey and Perspective \
  本文从一个新的分类法角度回顾了知识机制的分析，包括知识利用和进化。知识利用探讨了记忆、理解与应用及创造的机制。知识进化则关注个体和群体LLMs中知识的动态发展

# 7.28 Sun
* 106.Llama 4训练已开启！Meta科学家最新采访，揭秘Llama 3.1是如何炼成的  机器学习研究组订阅  https://mp.weixin.qq.com/s/6Kuv0Brpt2G777X_5FDftg 
* 107.(**厉害**)陶哲轩点评谷歌AlphaProof：AI在数学竞赛中展现「超凡智慧」  机器学习研究组订阅  https://mp.weixin.qq.com/s/q65SGyp9RvVb7Lq3MhsCyg \
  DeepMind 文章连接：https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/

# 7.29 Mon
* 108.英伟达最新技术分享：手把手教你用Llama 3.1合成数据改进模型！附代码  新智元  https://mp.weixin.qq.com/s/PVWKiilddgVznulzTJVuHg \
  Creating Synthetic Data Using Llama 3.1 405B \
  原文地址：https://developer.nvidia.com/blog/creating-synthetic-data-using-llama-3-1-405b/?linkId=100000275486093
* 109.秘密打造「AI陶哲轩」 震惊数学圈！谷歌IMO梦之队首曝光，菲尔兹奖得主深度点评  新智元  https://mp.weixin.qq.com/s/nUr0bi0pTZyWJvsm_aI8cw
* 110.曾被吴恩达点赞，清华&面壁团队重磅开源AI Agent最新研究：把全世界智能体连接起来！  PaperWeekly  https://mp.weixin.qq.com/s/wPywXV34447heJR4jY6kPg \
  国内最懂Agent的大模型公司，带给了我 OpenAI 给不了的惊喜！  夕小瑶科技说  https://mp.weixin.qq.com/s/WUZuyvEPWKH9e82CxS9gbw \
  「智能体互联网」IoA——把全世界智能体连接起来 \
  Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence

# 7.30 Tue

# 7.31 Wed
* 111.从炒菜到缝针！斯坦福炒虾团队打造自主「AI达芬奇」，苦练神指当外科医生 
  新智元  https://mp.weixin.qq.com/s/Gpj71vx37skw8JPTMH9ogQ \
  Surgical Robot Transformer (SRT): Imitation Learning for Surgical Tasks
* 112.GPT-4o版「Her」终于来了！讲笑话、学猫叫，AI女友能有多撩人？  新智元  https://mp.weixin.qq.com/s/UqFVQwnYMNoLWEU7IF7srQ 
* 113.(**厉害，非常值得看看**)4轮暴训，Llama 7B击败GPT-4！Meta等让LLM「分饰三角」自评自进化  新智元  https://mp.weixin.qq.com/s/yDE7QLuaCJwpwH7Ln9VuCg \
  META-REWARDING LANGUAGE MODELS:
 Self-Improving Alignment with LLM-as-a-Meta-Judge \
  Self-Rewarding Language Models
* 114.
* 115.
* 116.
* 117.
* 118.
* 119.
