# 6.1 Sat

# 6.2 Sun
* 1.多模态大模型不够灵活，谷歌DeepMind创新架构**Zipper**：分开训练再「压缩」  机器之心  https://mp.weixin.qq.com/s/F8wstkJyYiNJCbSqYq3Pbw \
  论文标题：Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities \
  论文链接：https://arxiv.org/pdf/2405.18669

# 6.3 Mon
* 2.next-token被淘汰！Meta实测「多token」训练方法，推理提速3倍，性能大涨10%+  新智元  https://mp.weixin.qq.com/s/rUBktCIL6BgTAbdod72MrQ \
  Better & Faster Large Language Models via Multi-token Prediction \
  论文链接：https://arxiv.org/pdf/2404.19737
* 3.英伟达新研究：上下文长度虚标严重，32K性能合格的都不多  量子位  https://mp.weixin.qq.com/s/pNUT8_T5YMJXrzLbzUi9ww \
  RULER: What's the Real Context Size of Your Long-Context Language Models? \
  论文链接：https://arxiv.org/abs/2404.06654
* 4.ICML2024高分！魔改注意力，让小模型能打两倍大的模型  量子位  https://mp.weixin.qq.com/s/MdXJaurs2Yn59In4FQyVpQ \
  Improving Transformers with Dynamically Composable Multi-Head Attention \
  Arxiv 论文链接：https://arxiv.org/abs/2405.08553 \
  代码链接：https://github.com/Caiyun-AI/DCFormer 

# 6.4 Tue
* 5.再战Transformer！原作者带队的Mamba 2来了，新架构训练效率大幅提升  机器之心  https://mp.weixin.qq.com/s/31t6pJqcXrZDjT6XiJZC_g \
  论文地址：https://arxiv.org/pdf/2405.21060 \
  GitHub 地址：https://github.com/state-spaces/mamba \
  论文标题：Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality
* 6.单个4090可推理，2000亿稀疏大模型「天工MoE」开源  机器之心  https://mp.weixin.qq.com/s/h5bxuWca65t3LsQwqGq-Og 
* 7.LeCun新作：分层世界模型，数据驱动的人型机器人控制  新智元  https://mp.weixin.qq.com/s/BxhvxpM66JGtbR2KgYYtBg \
  Hierarchical World Models as Visual Whole-Body Humanoid Controllers \
  论文地址：https://arxiv.org/pdf/2405.18418 \
  项目介绍：https://nicklashansen.com/rlpuppeteer
* 8.多模态模型学会打扑克：表现超越GPT-4v，全新强化学习框架是关键  量子位  https://mp.weixin.qq.com/s/bAf-5NzOD3fdTwYzdKsELw \
  Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning \
  论文地址：https://arxiv.org/abs/2405.10292 \
  GitHub：https://github.com/RL4VLM/RL4VLM
* 9.重温被Mamba带火的SSM：线性系统和HiPPO矩阵  PaperWeekly  https://mp.weixin.qq.com/s/tIYTNqdkiXhdgcWO3OI-8A

# 6.5 Wed
* 10.李飞飞高徒 Jim Fan：具身智能的难点不是硬件，而是「Foundation Agent」  图灵人工智能  https://mp.weixin.qq.com/s/hiJKt8_FifNwqUP5163vTw
* 11.GLM-4开源版本终于来了：超越Llama3，多模态比肩GPT4V，MaaS平台也大升级  机器之心  https://mp.weixin.qq.com/s/MqxiXeYs8dg_lynsUIR0Tg \
  杀疯了！全面超越Llama3的强悍开源模型，仅9B，1000k上下文；GPT-4级别模型1年降价1万倍  夕小瑶科技说  https://mp.weixin.qq.com/s/ITSDWHCvnAY7XHUCODzbxw 
* 12.腾讯混元&北大| 发现「**浪涌现象**」，解决学习率调参难题  AINLPer  https://mp.weixin.qq.com/s/ElpyTzDgnTQRcqbyn_PUTg
* 13.绝绝子！UT| 提出新型大模型微调架构：**LOFIT**，相比LoRA，学习参数减少200倍！！  AINLPer  https://mp.weixin.qq.com/s/_3Yi0o2mhrdPmeiauxwKZA
* 14.【ETHZ博士论文】有限数据中的元学习先验：从理论到实践  专知  https://mp.weixin.qq.com/s/KSXznLnMJ1zp0WhRZTj0pw

# 6.6 Thur
* 15.首次证实白盒Transformer可扩展性！马毅教授CRATE-α：鲸吞14亿数据，性能稳步提升  新智元  https://mp.weixin.qq.com/s/0Ps_9BVHDulpEprlb-3sgg \
  Scaling White-Box Transformers for Vision \
  论文链接：https://arxiv.org/pdf/2405.20299  \
  项目链接：https://rayjryang.github.io/CRATE-alpha/
* 16.作为人工智能下一个关口的意识研究：从加扎尼加的意识学说切入  神经现实  https://mp.weixin.qq.com/s/lF30iooZSCKTk0iyg_CAhQ \
  《意识本能》

# 6.7 Fri
* 17.ACL 2024 | 让纯LLM实现类人的符号逻辑推理能力，开源框架SymbCoT来了  机器之心  https://mp.weixin.qq.com/s/qYDBKHQmJg4TKXgwIoaapQ \
  论文：Faithful Logical Reasoning via Symbolic Chain-of-Thought \
  论文地址：https://arxiv.org/pdf/2405.18357.pdf \
  代码地址：https://github.com/Aiden0526/SymbCoT
* 18.(**重要，自监督选择性搜索**)大语言模型何时需要检索？UCLA提出全新自监督选择性检索策略  PaperWeekly  https://mp.weixin.qq.com/s/1fGt0egAYYa36EMWSzRD6g 
* 19.(**值得看看**)OpenAI新作署名Ilya，提取1600万个特征看透GPT-4大脑！  新智元  https://mp.weixin.qq.com/s/nx1LkHMkQbgeO6xU3fuc2w \
  Scaling and evaluating sparse autoencoders \
  论文地址：https://cdn.openai.com/papers/sparse-autoencoders.pdf
* 20.全球开源新王Qwen2-72B诞生，碾压Llama3-70B击败国产闭源模型！AI圈大佬转疯了  新智元  https://mp.weixin.qq.com/s/H6BbNfBNhyJTWs4ML6K1CQ 

# 6.8 Sat
* 21.轻松构建聊天机器人、准确性新SOTA，RAG有了更强大的AI检索器  机器之心  https://mp.weixin.qq.com/s/Ic12BIYK13mIPnFbkxk-Ww \
  denser-retriever \
  GitHub地址：https://github.com/denser-org/denser-retriever/tree/main \
  博客地址：https://denser.ai/blog/denser-retriever/
* 22.Llama3-8B秒杀700亿巨兽？北大博士生等全新「**BoT**」框架推理暴涨70倍，24点图形推理一步成神  新智元  https://mp.weixin.qq.com/s/wdtM2o1KzLahaW-rPUr7Mg \
  思维缓冲区（Buffer of Thoughts，BoT） \
  北大、UC伯克利、斯坦福的研究人员提出了一种元缓冲区（meta-buffer）。它可以存储一系列信息丰富的高级思维，也就是所谓的「思维模板」，它是从各种任务的问题解决过程中蒸馏出来的 \
  论文地址：https://arxiv.org/abs/2406.04271
* 23.To Believe or Not to Believe？DeepMind新研究一眼看穿LLM幻觉  新智元  https://mp.weixin.qq.com/s/MurapI7vQVRqMhX8W_neeA \
  如果无法强迫LLM坚持输出真实信息，知道它什么时候在胡说八道也很重要。 \
  To Believe or Not to Believe Your LLM \
  论文地址：https://arxiv.org/abs/2406.02543

# 6.9 Sun
* 24.从LLM中完全消除矩阵乘法，效果出奇得好，10亿参数跑在FPGA上接近大脑功耗  机器之心  https://mp.weixin.qq.com/s/HUvGGug48nGBx067nCbkag \
  论文地址：https://arxiv.org/pdf/2406.02528 \
  项目地址：https://github.com/ridgerchu/matmulfreellm \
  论文标题：Scalable MatMul-free Language Modeling
* 25.可信度超越GPT-4V，清华&面壁揭秘「小钢炮」模型背后的高效对齐技术  机器之心  https://mp.weixin.qq.com/s/7otafJLrrj4jlZIltQxcjQ \
  论文：RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness \
  论文地址: https://arxiv.org/abs/2405.17220 \
  项目地址：https://github.com/RLHF-V/RLAIF-V \
  DEMO：https://huggingface.co/spaces/openbmb/RLAIF-V-12B
