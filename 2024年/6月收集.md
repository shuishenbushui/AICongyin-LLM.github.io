# 6.1 Sat

# 6.2 Sun
* 1.多模态大模型不够灵活，谷歌DeepMind创新架构**Zipper**：分开训练再「压缩」  机器之心  https://mp.weixin.qq.com/s/F8wstkJyYiNJCbSqYq3Pbw \
  论文标题：Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities \
  论文链接：https://arxiv.org/pdf/2405.18669

# 6.3 Mon
* 2.next-token被淘汰！Meta实测「多token」训练方法，推理提速3倍，性能大涨10%+  新智元  https://mp.weixin.qq.com/s/rUBktCIL6BgTAbdod72MrQ \
  Better & Faster Large Language Models via Multi-token Prediction \
  论文链接：https://arxiv.org/pdf/2404.19737
* 3.英伟达新研究：上下文长度虚标严重，32K性能合格的都不多  量子位  https://mp.weixin.qq.com/s/pNUT8_T5YMJXrzLbzUi9ww \
  RULER: What's the Real Context Size of Your Long-Context Language Models? \
  论文链接：https://arxiv.org/abs/2404.06654
* 4.ICML2024高分！魔改注意力，让小模型能打两倍大的模型  量子位  https://mp.weixin.qq.com/s/MdXJaurs2Yn59In4FQyVpQ \
  Improving Transformers with Dynamically Composable Multi-Head Attention \
  Arxiv 论文链接：https://arxiv.org/abs/2405.08553 \
  代码链接：https://github.com/Caiyun-AI/DCFormer 

# 6.4 Tue
* 5.再战Transformer！原作者带队的Mamba 2来了，新架构训练效率大幅提升  机器之心  https://mp.weixin.qq.com/s/31t6pJqcXrZDjT6XiJZC_g \
  论文地址：https://arxiv.org/pdf/2405.21060 \
  GitHub 地址：https://github.com/state-spaces/mamba \
  论文标题：Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality
* 6.单个4090可推理，2000亿稀疏大模型「天工MoE」开源  机器之心  https://mp.weixin.qq.com/s/h5bxuWca65t3LsQwqGq-Og 
* 7.LeCun新作：分层世界模型，数据驱动的人型机器人控制  新智元  https://mp.weixin.qq.com/s/BxhvxpM66JGtbR2KgYYtBg \
  Hierarchical World Models as Visual Whole-Body Humanoid Controllers \
  论文地址：https://arxiv.org/pdf/2405.18418 \
  项目介绍：https://nicklashansen.com/rlpuppeteer
* 8.多模态模型学会打扑克：表现超越GPT-4v，全新强化学习框架是关键  量子位  https://mp.weixin.qq.com/s/bAf-5NzOD3fdTwYzdKsELw \
  Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning \
  论文地址：https://arxiv.org/abs/2405.10292 \
  GitHub：https://github.com/RL4VLM/RL4VLM
* 9.重温被Mamba带火的SSM：线性系统和HiPPO矩阵  PaperWeekly  https://mp.weixin.qq.com/s/tIYTNqdkiXhdgcWO3OI-8A

# 6.5 Wed
* 10.李飞飞高徒 Jim Fan：具身智能的难点不是硬件，而是「Foundation Agent」  图灵人工智能  https://mp.weixin.qq.com/s/hiJKt8_FifNwqUP5163vTw
* 11.GLM-4开源版本终于来了：超越Llama3，多模态比肩GPT4V，MaaS平台也大升级  机器之心  https://mp.weixin.qq.com/s/MqxiXeYs8dg_lynsUIR0Tg \
  杀疯了！全面超越Llama3的强悍开源模型，仅9B，1000k上下文；GPT-4级别模型1年降价1万倍  夕小瑶科技说  https://mp.weixin.qq.com/s/ITSDWHCvnAY7XHUCODzbxw 
* 12.腾讯混元&北大| 发现「**浪涌现象**」，解决学习率调参难题  AINLPer  https://mp.weixin.qq.com/s/ElpyTzDgnTQRcqbyn_PUTg
* 13.绝绝子！UT| 提出新型大模型微调架构：**LOFIT**，相比LoRA，学习参数减少200倍！！  AINLPer  https://mp.weixin.qq.com/s/_3Yi0o2mhrdPmeiauxwKZA
* 14.【ETHZ博士论文】有限数据中的元学习先验：从理论到实践  专知  https://mp.weixin.qq.com/s/KSXznLnMJ1zp0WhRZTj0pw

# 6.6 Thur
* 15.首次证实白盒Transformer可扩展性！马毅教授CRATE-α：鲸吞14亿数据，性能稳步提升  新智元  https://mp.weixin.qq.com/s/0Ps_9BVHDulpEprlb-3sgg \
  Scaling White-Box Transformers for Vision \
  论文链接：https://arxiv.org/pdf/2405.20299  \
  项目链接：https://rayjryang.github.io/CRATE-alpha/
* 16.作为人工智能下一个关口的意识研究：从加扎尼加的意识学说切入  神经现实  https://mp.weixin.qq.com/s/lF30iooZSCKTk0iyg_CAhQ \
  《意识本能》

# 6.7 Fri
* 17.ACL 2024 | 让纯LLM实现类人的符号逻辑推理能力，开源框架SymbCoT来了  机器之心  https://mp.weixin.qq.com/s/qYDBKHQmJg4TKXgwIoaapQ \
  论文：Faithful Logical Reasoning via Symbolic Chain-of-Thought \
  论文地址：https://arxiv.org/pdf/2405.18357.pdf \
  代码地址：https://github.com/Aiden0526/SymbCoT
* 18.(**重要，自监督选择性搜索**)大语言模型何时需要检索？UCLA提出全新自监督选择性检索策略  PaperWeekly  https://mp.weixin.qq.com/s/1fGt0egAYYa36EMWSzRD6g 
* 19.(**值得看看**)OpenAI新作署名Ilya，提取1600万个特征看透GPT-4大脑！  新智元  https://mp.weixin.qq.com/s/nx1LkHMkQbgeO6xU3fuc2w \
  Scaling and evaluating sparse autoencoders \
  论文地址：https://cdn.openai.com/papers/sparse-autoencoders.pdf
* 20.全球开源新王Qwen2-72B诞生，碾压Llama3-70B击败国产闭源模型！AI圈大佬转疯了  新智元  https://mp.weixin.qq.com/s/H6BbNfBNhyJTWs4ML6K1CQ 

# 6.8 Sat
* 21.轻松构建聊天机器人、准确性新SOTA，RAG有了更强大的AI检索器  机器之心  https://mp.weixin.qq.com/s/Ic12BIYK13mIPnFbkxk-Ww \
  denser-retriever \
  GitHub地址：https://github.com/denser-org/denser-retriever/tree/main \
  博客地址：https://denser.ai/blog/denser-retriever/
* 22.Llama3-8B秒杀700亿巨兽？北大博士生等全新「**BoT**」框架推理暴涨70倍，24点图形推理一步成神  新智元  https://mp.weixin.qq.com/s/wdtM2o1KzLahaW-rPUr7Mg \
  思维缓冲区（Buffer of Thoughts，BoT） \
  北大、UC伯克利、斯坦福的研究人员提出了一种元缓冲区（meta-buffer）。它可以存储一系列信息丰富的高级思维，也就是所谓的「思维模板」，它是从各种任务的问题解决过程中蒸馏出来的 \
  论文地址：https://arxiv.org/abs/2406.04271
* 23.To Believe or Not to Believe？DeepMind新研究一眼看穿LLM幻觉  新智元  https://mp.weixin.qq.com/s/MurapI7vQVRqMhX8W_neeA \
  如果无法强迫LLM坚持输出真实信息，知道它什么时候在胡说八道也很重要。 \
  To Believe or Not to Believe Your LLM \
  论文地址：https://arxiv.org/abs/2406.02543

# 6.9 Sun
* 24.从LLM中完全消除矩阵乘法，效果出奇得好，10亿参数跑在FPGA上接近大脑功耗  机器之心  https://mp.weixin.qq.com/s/HUvGGug48nGBx067nCbkag \
  论文地址：https://arxiv.org/pdf/2406.02528 \
  项目地址：https://github.com/ridgerchu/matmulfreellm \
  论文标题：Scalable MatMul-free Language Modeling
* 25.可信度超越GPT-4V，清华&面壁揭秘「小钢炮」模型背后的高效对齐技术  机器之心  https://mp.weixin.qq.com/s/7otafJLrrj4jlZIltQxcjQ \
  论文：RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness \
  论文地址: https://arxiv.org/abs/2405.17220 \
  项目地址：https://github.com/RLHF-V/RLAIF-V \
  DEMO：https://huggingface.co/spaces/openbmb/RLAIF-V-12B

# 6.10 Mon
* 26.Karpathy最新四小时视频教程：从零复现GPT-2，通宵运行即搞定  机器之心  https://mp.weixin.qq.com/s/BI8EdDyTEk8meL_FhX-ftw \
   GitHub 地址：https://github.com/karpathy/build-nanogpt
* 27.(**重要**)大模型在持续学习中的最新进展：综述  专知  https://mp.weixin.qq.com/s/yQcZnLG9hFeBgPV-trQOsA \
  https://arxiv.org/pdf/2405.18653
* 28.两句话，让LLM逻辑推理瞬间崩溃！最新「爱丽丝梦游仙境」曝出GPT、Claude等重大缺陷  新智元  https://mp.weixin.qq.com/s/iLGMOQOS-xHqsXLVfstguQ \
  对此，LeCun也在第一时间转评道：「再次强调，推理能力和常识不应与存储和大致检索大量事实的能力混为一谈。」\
  Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models \
  论文地址：https://arxiv.org/abs/2406.02061 \
  开源地址：https://github.com/LAION-AI/AIW 
  

# 6.11 Tue
* 29.赋予机器人思考能力！北大提出**自纠正多模态大模型**，赋能端到端机器人操作 
 PaperWeekly  https://mp.weixin.qq.com/s/tOBBV00wHaMPoqmg9y54ZQ \
  项目主页：https://sites.google.com/view/sc-mllm-web \
  Github链接：https://github.com/2644521362/SC-MLLM \
  论文链接：https://arxiv.org/pdf/2405.17418 \
  Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation
* 30.【CMU博士论文】构建自适应性强的通用机器人，248页pdf  专知  https://mp.weixin.qq.com/s/louXBjjF3-V4FI-zjwhizg 

# 6.12 Wed
* 31.ICML 2024 哈佛大学最新研究：越强的大模型越不懂人类  夕小瑶科技说  https://mp.weixin.qq.com/s/XfoD6INP-nC3Nth9tPKuHg \
  论文题目：Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function \
  论文链接：http://arxiv.org/abs/2406.01382
* 32.复旦字节强强联手，量身定制多模态思维链，让7B模型全面超越GPT-4V  夕小瑶科技说  https://mp.weixin.qq.com/s/Co5PV5fGS85XV7-NHWdzoA \
  论文标题：VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models  \
  论文链接：https://arxiv.org/abs/2405.16919 \
  VoCoT 具有两个关键特征：（1）以对象为中心的推理路径，围绕跨模态共享的对象级信息展开，以及（2）以多模态交叉和对齐的方式对对象概念进行视觉上的表征，有效地弥合了 LMM 在长文本过程中的模态差异。
* 33.手机流畅运行470亿大模型：上交大发布LLM手机推理框架PowerInfer-2，提速29倍  量子位  https://mp.weixin.qq.com/s/vylZp7MG7TA3pQKOBWbYRQ
* 34.((**重要**)港大&腾讯 | 提出SELF-TUNING学习框架，让LLM自学获取新知识，表现出色！  AINLPer  https://mp.weixin.qq.com/s/-FTbJYMbCyiQXyTZ6BHjaA \
  SELF-TUNING: Instructing LLMs to Effectively Acquire New Knowledge 
 through Self-Teaching \
  https://arxiv.org/pdf/2406.06326

# 6.13 Thur
* 35.GPT-4尚未出现自我意识！这项研究用「上帝之点」解读，迈向AGI局限无法克服  新智元  https://mp.weixin.qq.com/s/dhH6CsVqo6sNFLBklLW4Gg \
  Flight Model: New Explorations into the Fundamental Principles of Intelligence and Consciousness \
  论文地址：http://dx.doi.org/10.13140/RG.2.2.24518.28484

# 6.14 Fri

# 6.15 Sat

# 6.16 Sun
* 36.(**重要**)ACL 2024论文盖棺定论：大语言模型≠世界模拟器，Yann LeCun：太对了  机器之心  https://mp.weixin.qq.com/s/FBqYb_gcBr5D204mDtmCOA \
  GPT-4不是世界模型，LeCun双手赞同！ACL力证LLM永远无法模拟世界  新智元  https://mp.weixin.qq.com/s/-YjuaZ44SnVEsooYJea0Qw \
  Can Language Models Serve as Text-Based World Simulators? \
  论文地址：https://arxiv.org/pdf/2406.06485 \
  基准测试的代码和数据: \
  https://github.com/cognitiveailab/GPT-simulator
* 37.Transformer升级之路：RoPE的底数设计原则  PaperWeekly  https://mp.weixin.qq.com/s/YhpfIz0Pi1OMLwN3V3J1mQ \
  《Base of RoPE Bounds Context Length》
* 38.(**重要**)打通智能体「自我进化」全流程！复旦推出通用智能体平台AgentGym   PaperWeekly  https://mp.weixin.qq.com/s/WyxQMJoj03FsNT3QRVD05A \
  AgentGym: Evolving Large Language Model-based Agents across Diverse Environments \
  论文链接：https://arxiv.org/abs/2406.04151 \
  代码仓库：https://github.com/WooooDyy/AgentGym
* 39.(**重要**)大语言模型的终身学习综述  专知  https://mp.weixin.qq.com/s/S7eq26JeGMphQV3PUnZ40w \
  Towards Lifelong Learning of Large Language Models: A Survey

# 6.17 Mon
* 40.(**重要**)大模型+蒙特卡洛树搜索，一招让LLaMa-3 8B奥数水平直逼GPT-4  机器之心  https://mp.weixin.qq.com/s/g2w7Rn7Q0mtz9xTPX-Q0Mw \
  AI 参与数学竞赛的主要短板是逻辑推理能力弱，证明题很难拿到完整得分点。这也是 GPT-4、LLaMA 等当前大语言模型（LLM）在需要策略和逻辑推理的任务中面临的重大挑战。\
  其中的一大障碍是输出的准确性和可信度，尤其是在需要保证精度的数学上下文中，LLM 在推理时往往容易产生幻觉。输出结果表面上看似合理，但实际上不相关或事实不正确，最终导致不合理的推理过程。\
  虽然像 Self-Refine 这样的重写技术有助于缓解这种倾向，但依然可能导致现实世界复杂的数学问题产生误导性或错误的结果。\
  因此，为了应对这些挑战，来自复旦大学、上海 AI Lab 的研究者提出了 MCT Self-Refine（MCTSr），将 LLM 与蒙特卡洛树搜索（MCTS）算法相结合，并重点提高 LLM 在复杂数学推理任务（比如奥数竞赛题）中的表现。\
  作为一种决策工具，MCTS 广泛应用于人工智能中需要战略规划的场景，通常用于游戏和复杂的问题解决环境。本文通过将 MCTS 的系统探索能力与 LLM 的 Self-Refine 和 Self-Evaluation 能力相结合， 旨在创建一个更强大的框架来应对当前 LLM 难以解决的复杂推理任务。\
  Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B: A Technical Report \
  论文地址：https://arxiv.org/pdf/2406.07394 \
  项目地址：https://github.com/trotsky1997/MathBlackBox
* 41.3D 版 SORA 来了！DreamTech 推出全球首个原生 3D-DiT 大模型 Direct3D  机器之心  https://mp.weixin.qq.com/s/y2uVCgy0ywSlsF860Byt3g 
* 42.LLM最全「怪癖」首曝光！马里兰OpenAI等30+学者祭出75页提示报告  新智元  https://mp.weixin.qq.com/s/iKB9j1Nr4bDLA4rVv6jHWA \
  论文地址：https://arxiv.org/abs/2406.06608
* 43.树莓派上部署RAG！微软Phi-3技术报告揭示「小而美」模型如何诞生  新智元  https://mp.weixin.qq.com/s/pF9SvSzcakkdssfZYFQP9Q 
* 44.大模型「幻觉」全无？图神经网络成破解核心，精准预测因果消除「幻觉」  新智元  https://mp.weixin.qq.com/s/mmWi2RPXBcU2KtVUoVOH6Q 
* 45.(**重要**)拯救Transformer推理能力！DeepMind新研究TransNAR：给模型嵌入「算法推理大脑」  新智元  https://mp.weixin.qq.com/s/YPICpkYHAC7zTLC_0M_XkQ \
  将Transformers的语言理解能力与基于图神经网络（GNN）的神经算法推理器（NAR）的稳健性结合起来，提升其算法推理能力
* 46.机器遗忘综述：技术与新出现的隐私风险  专知  https://mp.weixin.qq.com/s/g2VD6KKt4PW74PXjVI0nCw

# 6.18 Tue
* 47.全球首个开源类Sora猛升级，16秒720p画质电影感拉满！代码权重全开源  新智元  https://mp.weixin.qq.com/s/3upT7Kl9vr-b4k4d-SE35w
* 48.黄仁勋提到的机器人世界，还需要AI数据来“调教” | CVPR 2024  量子位  https://mp.weixin.qq.com/s/l4GQLBurZt0dkbqzVu8YGA

# 6.19 Wed
* 49.低层视觉中的扩散模型：综述  机器学习研究组订阅  https://mp.weixin.qq.com/s/4aW0wNUQ4wVfKPLUR-fC2g

# 6.20 Thur
* 50.通用人工智能：是什么？如何测试？如何实现？  集智俱乐部  https://mp.weixin.qq.com/s/EncRoqcTacbxPTa9jC6RjA
* 51.吵翻了：意识是如何产生的？科学家能否达成一致？  集智俱乐部  https://mp.weixin.qq.com/s/t-yfhhlff3Rx-nSPAmWw6A
* 52.北大推出全新机器人多模态大模型！面向通用和机器人场景的高效推理和操作  机器之心  https://mp.weixin.qq.com/s/ED2bnE6NDT83zlF9QXHg6Q \
  北大 | 推出全新机器人多模态大模型！（视觉编码+Mamba）  AINLPer  https://mp.weixin.qq.com/s/TxV93_zApwUxo4hDiBnoBw \
  论文：RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation \
  论文链接：https://arxiv.org/abs/2406.04339 \
  项目主页：https://sites.google.com/view/robomamba-web \
  Github：https://github.com/lmzpai/roboMamba

# 6.21 Fri

# 6.22 Sat
* 53.GPT-5一年半后拥有「博士级智能」，Claude 3.5首超人类博士！全知全能ASI将成人类「新神」？ 
 新智元  https://mp.weixin.qq.com/s/GdosKK2J0h0zESrWsTuXrQ \
  ASI as the New God: Technocratic Theocracy \
  论文地址：https://arxiv.org/pdf/2406.08492

# 6.23 Sun

# 6.24 Mon
* 54.(**值得看看**)语言≠思维，大模型学不了推理：一篇Nature让AI社区炸锅了  机器之心  https://mp.weixin.qq.com/s/BgMNITn5e1RGUOHQLKv7yg \
  Language is primarily a tool for communication rather than thought \
  论文链接：https://www.nature.com/articles/s41586-024-07522-w \
  尽管失去了语言能力，一些患有严重失语症的人仍然能够进行所有测试形式的思考和推理，他们在各种认知任务中的完整表现就是明证。他们根本无法将这些想法映射到语言表达上，无论是在语言生成中（他们无法通过语言向他人传达自己的想法），还是在理解中（他们无法从他人的单词和句子中提取意义）\
  参与多种形式的思考和推理并不需要语言网络。因此，语言不太可能成为任何形式思维的关键基础
* 55.(**值得看看**)微软Florence-2官宣开源，一统视觉基础模型！华人团队联手打造  新智元  https://mp.weixin.qq.com/s/Ng25ggmOmoPWEt2faxl-8Q \
  Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks \
  论文地址：https://arxiv.org/pdf/2311.06242
* 56.最新！何恺明团队发布：打破自回归图像生成瓶颈，告别矢量量化  夕小瑶科技说  https://mp.weixin.qq.com/s/X8QyqZbKw-DL0PfsLmDC1Q \
  ???什么是矢量量化 \
  论文题目:Autoregressive Image Generation without Vector Quantization \
  论文链接:https://arxiv.org/abs/2406.11838
* 57.大模型测试题爆火，GPT-4和Claude3都跪了，LeCun转发：新Benchmark  量子位  https://mp.weixin.qq.com/s/IZlRQwUQFzRl-_Fxq7dJ6g \
* 58.Skywork AI | 提出新框架：Q*，旨在解决大模型多步推理（Multi-step）错误问题  AINLPer  https://mp.weixin.qq.com/s/dDaYBhewnc9-GslzS7vv1A \
  Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning \
  https://arxiv.org/pdf/2406.14283 \
  大模型（LLMs）在推理任务上展现出了卓越能力，例如如数学推理、代码生成和行动规划等。然而，LLMs在进行多步骤推理时，由于其自回归生成的特性，随着推理步骤数量的增加，很容易引入错误、幻觉和不一致的陈述。文章指出，LLMs的自回归生成过程可以被视为“**System 1**”思维模式，这种思维快速、本能但准确性较低。\
  最近大多数研究都侧重于通过以下方式提高LLM的“**System 1**”能力：1）构建更广泛专业知识复杂提示，来引导LLM的潜在能力，而无需修改参数。2）使用大量特定任务的语料来对LLM进行微调，但是该方法的代价会增加计算资源并且存在灾难遗忘的风险；3）通过训练奖励模型来对候选答案进行排序。\
  另一方面，解决复杂的推理问题需要更深入、更审慎和更合乎逻辑的思维步骤，即“**System 2**”模式。以解决数学应用题为例，任何错误的中间推理步骤（例如计算错误、误解）都可能导致错误的最终答案。当前增强“**System 2**” 推理能力的主要方法，包括使用基本树搜索算法（例如 BFS 或 DFS）、蒙特卡洛树搜索 (MCTS) 和 A* 算法等。\
  尽管如此，这些方法中使用的效用函数通常需要针对每个特定任务进行费力的专业知识设计，很难扩展到新的场景。此外，在解决具有许多推理步骤的问题时，使用MCTS进行推理时，在解决多步骤推理问题时需要大量的模拟，这显著减慢了整体的解码过程。\
  基于以上背景，本文作者提出了Q_star框架，旨在通过深思熟虑的规划来提高大模型（LLMs）在多步骤推理方面的能力。与现有的推理方法不同，Q_star框架不依赖于领域知识来设计启发式函数。而是通过使用即插即用的Q值模型作为启发式函数，它能够指导LLMs选择最有可能的下一步，从而有效解决多步推理任务。
  
# 6.25 Tue
* 59.(**可以看看**)经典综述：自由能原理——统一的大脑理论  集智俱乐部  https://mp.weixin.qq.com/s/dXMwY5GxY8mc2gGe-7jQmw \
  论文题目：The free-energy principle: a unified brain theory?  \
  论文地址：https://www.nature.com/articles/nrn2787

# 6.26 Wed
* 60.人类有爱、悲伤和死亡意识，AI无法对齐丨记智源大会意识与通用人工智能论坛  智源社区  https://mp.weixin.qq.com/s/NQRYN6HjBPrEPzARdCMlUQ
* 61.模拟5亿年自然进化史，全新蛋白质大模型ESM3诞生！前Meta老将力作LeCun转赞  机器学习研究组订阅  https://mp.weixin.qq.com/s/GDrCAFGE_DS_owyAL-d5qQ
* 62.豪赌！哈佛辍学华人竟然发布了只支持Transformer一种算法的AI芯片，一张顶20张H100 ，比GB200快  夕小瑶科技说  https://mp.weixin.qq.com/s/v8QsmfDCinzKa698SxWodQ \
  LeCun谢赛宁首发全新视觉多模态模型，等效1000张A100干翻GPT-4V  新智元  https://mp.weixin.qq.com/s/dSypqARyADPUN3S-e06R3Q

# 6.27 Thur
* 63.基于大型语言模型的AI聊天机器人的完整综述  专知  https://mp.weixin.qq.com/s/ROLWjjv6JTtyZwnj1I5Bzw \
  A Complete Survey on LLM-based AI Chatbots \
  https://arxiv.org/abs/2406.16937

# 6.28 Fri
* 64.(**重要**)在环境交互中持续进化！神经符号视角下的LLM自训练框架  PaperWeekly  https://mp.weixin.qq.com/s/X76DENKYXVcOySWi-Wl6JA \
  论文题目：Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models \
  论文地址：https://arxiv.org/abs/2406.11736
* 65.(**重要**)GPT-4批评GPT-4实现「自我提升」！OpenAI前超级对齐团队又一力作被公开  机器学习研究组订阅  https://mp.weixin.qq.com/s/-sYs8HarCThi9mBaZP5oMA \
  LLM Critics Help Catch LLM Bugs \
  论文地址：https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf
* 66.单张A100全精度推理！谷歌明星开源模型Gemma 2上新9B/27B，挑战3140亿Grok-1  机器学习研究组订阅  https://mp.weixin.qq.com/s/Dj0FnPzabsv2papvWOvoDQ \
  项目地址：https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315

# 6.29 Sat
* 67.【ETHZ博士论文】从图像学习深度，136页pdf  专知  https://mp.weixin.qq.com/s/Rp9rLivijn6DtpJlskVX5Q \
  learning depth from images
* 68.通用多模态人工智能：架构、挑战和机遇**综述**  专知  https://mp.weixin.qq.com/s/fEN6Rv5hYzuLruyeSjX6Ow \
  Generalist Multimodal AI: A Review of Architectures, Challenges and Opportunities \
  https://arxiv.org/pdf/2406.05496
* 69.(**非常重要**)ICML 2024｜Transformer究竟如何推理？基于样例还是基于规则  机器之心  https://mp.weixin.qq.com/s/aVRiGW3xU_LpvxZzjDpwzQ \
  论文地址：https://arxiv.org/abs/2402.17709 \
  项目主页：https://github.com/GraphPKU/Case_or_Rule \
  论文标题：Case-Based or Rule-Based: How Do Transformers Do the Math? \
  ...这些黑色区域的形状和位置表明，只有当测试案例的每一步在训练集中都出现过时，模型才能够成功；否则就会失败。更重要的是，这一现象表明，即使有 step-by-step 的推理过程的帮助，transformers 也难以学会 rule-based reasoning —— **模型仍然在机械地记忆见过的单个步骤，而没有学会背后的规则!!!*** \
  本文探究了 transformers 在做数学推理问题时究竟是采用 case-based reasoning 还是 rule-based reasoning，并提出了 Rule-Following Fine-Tuning 的规则遵循微调方法来显式地教会 transformers 进行 rule-based reasoning。RFFT 展现了强大的长度泛化能力，并有潜力全面提升 LLMs 的推理能力

# 6.30 Sun
* 70.LLaVA-HD全新升级，更快更强！已开源  PaperWeekly  https://mp.weixin.qq.com/s/2P978rTKgCziBCzTuE91jQ \
  **高分辨率**
  论文标题：Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models \
  论文链接：https://arxiv.org/abs/2406.08487 \
  代码链接：https://github.com/yfzhang114/SliME
* 71.【CMU博士论文】使用结构化推理增强语言模型，320页pdf  专知  https://mp.weixin.qq.com/s/HRZkjQdpgEBvFyQCBeM6dQ \
  Enhancing Language Models with Structured Reasoning
* 72.菲尔兹奖得主亲测GPT-4o，经典过河难题破解失败！最强Claude 3.5回答离谱，LeCun嘲讽LLM  机器学习研究组订阅  https://mp.weixin.qq.com/s/x2lKS2d7Xtrpy3mAUA8A8A 

