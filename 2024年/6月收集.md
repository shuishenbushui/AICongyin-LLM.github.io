# 6.1 Sat

# 6.2 Sun
* 1.多模态大模型不够灵活，谷歌DeepMind创新架构**Zipper**：分开训练再「压缩」  机器之心  https://mp.weixin.qq.com/s/F8wstkJyYiNJCbSqYq3Pbw \
  论文标题：Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities \
  论文链接：https://arxiv.org/pdf/2405.18669

# 6.3 Mon
* 2.next-token被淘汰！Meta实测「多token」训练方法，推理提速3倍，性能大涨10%+  新智元  https://mp.weixin.qq.com/s/rUBktCIL6BgTAbdod72MrQ \
  Better & Faster Large Language Models via Multi-token Prediction \
  论文链接：https://arxiv.org/pdf/2404.19737
* 3.英伟达新研究：上下文长度虚标严重，32K性能合格的都不多  量子位  https://mp.weixin.qq.com/s/pNUT8_T5YMJXrzLbzUi9ww \
  RULER: What's the Real Context Size of Your Long-Context Language Models? \
  论文链接：https://arxiv.org/abs/2404.06654
* 4.ICML2024高分！魔改注意力，让小模型能打两倍大的模型  量子位  https://mp.weixin.qq.com/s/MdXJaurs2Yn59In4FQyVpQ \
  Improving Transformers with Dynamically Composable Multi-Head Attention \
  Arxiv 论文链接：https://arxiv.org/abs/2405.08553 \
  代码链接：https://github.com/Caiyun-AI/DCFormer 

# 6.4 Tue
* 5.再战Transformer！原作者带队的Mamba 2来了，新架构训练效率大幅提升  机器之心  https://mp.weixin.qq.com/s/31t6pJqcXrZDjT6XiJZC_g \
  论文地址：https://arxiv.org/pdf/2405.21060 \
  GitHub 地址：https://github.com/state-spaces/mamba \
  论文标题：Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality
* 6.单个4090可推理，2000亿稀疏大模型「天工MoE」开源  机器之心  https://mp.weixin.qq.com/s/h5bxuWca65t3LsQwqGq-Og 
* 7.LeCun新作：分层世界模型，数据驱动的人型机器人控制  新智元  https://mp.weixin.qq.com/s/BxhvxpM66JGtbR2KgYYtBg \
  Hierarchical World Models as Visual Whole-Body Humanoid Controllers \
  论文地址：https://arxiv.org/pdf/2405.18418 \
  项目介绍：https://nicklashansen.com/rlpuppeteer
* 8.多模态模型学会打扑克：表现超越GPT-4v，全新强化学习框架是关键  量子位  https://mp.weixin.qq.com/s/bAf-5NzOD3fdTwYzdKsELw \
  Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning \
  论文地址：https://arxiv.org/abs/2405.10292 \
  GitHub：https://github.com/RL4VLM/RL4VLM
* 9.重温被Mamba带火的SSM：线性系统和HiPPO矩阵  PaperWeekly  https://mp.weixin.qq.com/s/tIYTNqdkiXhdgcWO3OI-8A

# 6.5 Wed
* 10.李飞飞高徒 Jim Fan：具身智能的难点不是硬件，而是「Foundation Agent」  图灵人工智能  https://mp.weixin.qq.com/s/hiJKt8_FifNwqUP5163vTw
* 11.GLM-4开源版本终于来了：超越Llama3，多模态比肩GPT4V，MaaS平台也大升级  机器之心  https://mp.weixin.qq.com/s/MqxiXeYs8dg_lynsUIR0Tg \
  杀疯了！全面超越Llama3的强悍开源模型，仅9B，1000k上下文；GPT-4级别模型1年降价1万倍  夕小瑶科技说  https://mp.weixin.qq.com/s/ITSDWHCvnAY7XHUCODzbxw 
* 12.腾讯混元&北大| 发现「**浪涌现象**」，解决学习率调参难题  AINLPer  https://mp.weixin.qq.com/s/ElpyTzDgnTQRcqbyn_PUTg
* 13.绝绝子！UT| 提出新型大模型微调架构：**LOFIT**，相比LoRA，学习参数减少200倍！！  AINLPer  https://mp.weixin.qq.com/s/_3Yi0o2mhrdPmeiauxwKZA
* 14.【ETHZ博士论文】有限数据中的元学习先验：从理论到实践  专知  https://mp.weixin.qq.com/s/KSXznLnMJ1zp0WhRZTj0pw

# 6.6 Thur
* 15.首次证实白盒Transformer可扩展性！马毅教授CRATE-α：鲸吞14亿数据，性能稳步提升  新智元  https://mp.weixin.qq.com/s/0Ps_9BVHDulpEprlb-3sgg \
  Scaling White-Box Transformers for Vision \
  论文链接：https://arxiv.org/pdf/2405.20299  \
  项目链接：https://rayjryang.github.io/CRATE-alpha/
* 16.作为人工智能下一个关口的意识研究：从加扎尼加的意识学说切入  神经现实  https://mp.weixin.qq.com/s/lF30iooZSCKTk0iyg_CAhQ \
  《意识本能》

# 6.7 Fri
* 17.ACL 2024 | 让纯LLM实现类人的符号逻辑推理能力，开源框架SymbCoT来了  机器之心  https://mp.weixin.qq.com/s/qYDBKHQmJg4TKXgwIoaapQ \
  论文：Faithful Logical Reasoning via Symbolic Chain-of-Thought \
  论文地址：https://arxiv.org/pdf/2405.18357.pdf \
  代码地址：https://github.com/Aiden0526/SymbCoT
* 18.(**重要，自监督选择性搜索**)大语言模型何时需要检索？UCLA提出全新自监督选择性检索策略  PaperWeekly  https://mp.weixin.qq.com/s/1fGt0egAYYa36EMWSzRD6g 
* 19.(**值得看看**)OpenAI新作署名Ilya，提取1600万个特征看透GPT-4大脑！  新智元  https://mp.weixin.qq.com/s/nx1LkHMkQbgeO6xU3fuc2w \
  Scaling and evaluating sparse autoencoders \
  论文地址：https://cdn.openai.com/papers/sparse-autoencoders.pdf
* 20.全球开源新王Qwen2-72B诞生，碾压Llama3-70B击败国产闭源模型！AI圈大佬转疯了  新智元  https://mp.weixin.qq.com/s/H6BbNfBNhyJTWs4ML6K1CQ 

# 6.8 Sat
* 21.轻松构建聊天机器人、准确性新SOTA，RAG有了更强大的AI检索器  机器之心  https://mp.weixin.qq.com/s/Ic12BIYK13mIPnFbkxk-Ww \
  denser-retriever \
  GitHub地址：https://github.com/denser-org/denser-retriever/tree/main \
  博客地址：https://denser.ai/blog/denser-retriever/
* 22.Llama3-8B秒杀700亿巨兽？北大博士生等全新「**BoT**」框架推理暴涨70倍，24点图形推理一步成神  新智元  https://mp.weixin.qq.com/s/wdtM2o1KzLahaW-rPUr7Mg \
  思维缓冲区（Buffer of Thoughts，BoT） \
  北大、UC伯克利、斯坦福的研究人员提出了一种元缓冲区（meta-buffer）。它可以存储一系列信息丰富的高级思维，也就是所谓的「思维模板」，它是从各种任务的问题解决过程中蒸馏出来的 \
  论文地址：https://arxiv.org/abs/2406.04271
* 23.To Believe or Not to Believe？DeepMind新研究一眼看穿LLM幻觉  新智元  https://mp.weixin.qq.com/s/MurapI7vQVRqMhX8W_neeA \
  如果无法强迫LLM坚持输出真实信息，知道它什么时候在胡说八道也很重要。 \
  To Believe or Not to Believe Your LLM \
  论文地址：https://arxiv.org/abs/2406.02543

# 6.9 Sun
* 24.从LLM中完全消除矩阵乘法，效果出奇得好，10亿参数跑在FPGA上接近大脑功耗  机器之心  https://mp.weixin.qq.com/s/HUvGGug48nGBx067nCbkag \
  论文地址：https://arxiv.org/pdf/2406.02528 \
  项目地址：https://github.com/ridgerchu/matmulfreellm \
  论文标题：Scalable MatMul-free Language Modeling
* 25.可信度超越GPT-4V，清华&面壁揭秘「小钢炮」模型背后的高效对齐技术  机器之心  https://mp.weixin.qq.com/s/7otafJLrrj4jlZIltQxcjQ \
  论文：RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness \
  论文地址: https://arxiv.org/abs/2405.17220 \
  项目地址：https://github.com/RLHF-V/RLAIF-V \
  DEMO：https://huggingface.co/spaces/openbmb/RLAIF-V-12B

# 6.10 Mon
* 26.Karpathy最新四小时视频教程：从零复现GPT-2，通宵运行即搞定  机器之心  https://mp.weixin.qq.com/s/BI8EdDyTEk8meL_FhX-ftw \
   GitHub 地址：https://github.com/karpathy/build-nanogpt
* 27.(**重要**)大模型在持续学习中的最新进展：综述  专知  https://mp.weixin.qq.com/s/yQcZnLG9hFeBgPV-trQOsA \
  https://arxiv.org/pdf/2405.18653
* 28.两句话，让LLM逻辑推理瞬间崩溃！最新「爱丽丝梦游仙境」曝出GPT、Claude等重大缺陷  新智元  https://mp.weixin.qq.com/s/iLGMOQOS-xHqsXLVfstguQ \
  对此，LeCun也在第一时间转评道：「再次强调，推理能力和常识不应与存储和大致检索大量事实的能力混为一谈。」\
  Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models \
  论文地址：https://arxiv.org/abs/2406.02061 \
  开源地址：https://github.com/LAION-AI/AIW 
  

# 6.11 Tue
* 29.赋予机器人思考能力！北大提出**自纠正多模态大模型**，赋能端到端机器人操作 
 PaperWeekly  https://mp.weixin.qq.com/s/tOBBV00wHaMPoqmg9y54ZQ \
  项目主页：https://sites.google.com/view/sc-mllm-web \
  Github链接：https://github.com/2644521362/SC-MLLM \
  论文链接：https://arxiv.org/pdf/2405.17418 \
  Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation
* 30.【CMU博士论文】构建自适应性强的通用机器人，248页pdf  专知  https://mp.weixin.qq.com/s/louXBjjF3-V4FI-zjwhizg 

# 6.12 Wed
* 31.ICML 2024 哈佛大学最新研究：越强的大模型越不懂人类  夕小瑶科技说  https://mp.weixin.qq.com/s/XfoD6INP-nC3Nth9tPKuHg \
  论文题目：Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function \
  论文链接：http://arxiv.org/abs/2406.01382
* 32.复旦字节强强联手，量身定制多模态思维链，让7B模型全面超越GPT-4V  夕小瑶科技说  https://mp.weixin.qq.com/s/Co5PV5fGS85XV7-NHWdzoA \
  论文标题：VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models  \
  论文链接：https://arxiv.org/abs/2405.16919 \
  VoCoT 具有两个关键特征：（1）以对象为中心的推理路径，围绕跨模态共享的对象级信息展开，以及（2）以多模态交叉和对齐的方式对对象概念进行视觉上的表征，有效地弥合了 LMM 在长文本过程中的模态差异。
* 33.手机流畅运行470亿大模型：上交大发布LLM手机推理框架PowerInfer-2，提速29倍  量子位  https://mp.weixin.qq.com/s/vylZp7MG7TA3pQKOBWbYRQ
* 34.((**重要**)港大&腾讯 | 提出SELF-TUNING学习框架，让LLM自学获取新知识，表现出色！  AINLPer  https://mp.weixin.qq.com/s/-FTbJYMbCyiQXyTZ6BHjaA \
  SELF-TUNING: Instructing LLMs to Effectively Acquire New Knowledge 
 through Self-Teaching \
  https://arxiv.org/pdf/2406.06326

# 6.13 Thur
* 35.GPT-4尚未出现自我意识！这项研究用「上帝之点」解读，迈向AGI局限无法克服  新智元  https://mp.weixin.qq.com/s/dhH6CsVqo6sNFLBklLW4Gg \
  Flight Model: New Explorations into the Fundamental Principles of Intelligence and Consciousness \
  论文地址：http://dx.doi.org/10.13140/RG.2.2.24518.28484

# 6.14 Fri

# 6.15 Sat

# 6.16 Sun
* 36.(**重要**)ACL 2024论文盖棺定论：大语言模型≠世界模拟器，Yann LeCun：太对了  机器之心  https://mp.weixin.qq.com/s/FBqYb_gcBr5D204mDtmCOA \
  GPT-4不是世界模型，LeCun双手赞同！ACL力证LLM永远无法模拟世界  新智元  https://mp.weixin.qq.com/s/-YjuaZ44SnVEsooYJea0Qw \
  Can Language Models Serve as Text-Based World Simulators? \
  论文地址：https://arxiv.org/pdf/2406.06485 \
  基准测试的代码和数据: \
  https://github.com/cognitiveailab/GPT-simulator
* 37.Transformer升级之路：RoPE的底数设计原则  PaperWeekly  https://mp.weixin.qq.com/s/YhpfIz0Pi1OMLwN3V3J1mQ \
  《Base of RoPE Bounds Context Length》
* 38.(**重要**)打通智能体「自我进化」全流程！复旦推出通用智能体平台AgentGym   PaperWeekly  https://mp.weixin.qq.com/s/WyxQMJoj03FsNT3QRVD05A \
  AgentGym: Evolving Large Language Model-based Agents across Diverse Environments \
  论文链接：https://arxiv.org/abs/2406.04151 \
  代码仓库：https://github.com/WooooDyy/AgentGym
* 39.(**重要**)大语言模型的终身学习综述  专知  https://mp.weixin.qq.com/s/S7eq26JeGMphQV3PUnZ40w \
  Towards Lifelong Learning of Large Language Models: A Survey
* 40
