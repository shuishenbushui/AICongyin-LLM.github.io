# 5.1 Wed
* 1.(**非常厉害 OctopusV3**)参数量不到10亿的OctopusV3，如何媲美GPT-4V和GPT-4？  机器之心  https://mp.weixin.qq.com/s/mUpX-nvo221WVii-gnjUmQ \
  论文标题：Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent \
  论文链接：https://arxiv.org/pdf/2404.11459.pdf \
  模型权重和推理代码：https://www.nexa4ai.com/apply
* 2.(**自我奖励模型**)「用 AI 训 AI」这事靠谱吗？  机器之心  https://mp.weixin.qq.com/s/bLLoYDTpq8q7ExfwyDekOQ \
  Meta 等提出的**自我奖励模型**具备双重角色：一方面，它遵循模型的指令来生成给定提示的响应；另一方面，它也能够根据示例生成和评估新的指令，进而将其添加到训练集中。该模型建立在假设之上，即利用基础的预训练语言模型和少量的人工注释数据，可以创建一个同时具备指令遵循和自指令创建能力的模型 \
* 3.Llama 3细节公布！AI产品总监站台讲解：Llama系列超庞大生态系统  新智元  https://mp.weixin.qq.com/s/iDAlop_LNv9evZtfPMPyUg \
  **目前发布的其实是Llama 3的非常早期版本**，团队原本打算将这些模型称为预发布或预览版本，因为模型并不具有计划中包含的全部功能
* 4.(**LLM幻觉综述**)《多模态大型语言模型的幻觉现象》综述  专知  https://mp.weixin.qq.com/s/O89fDn8UtgPF-QKYeeF3-g \
  Hallucination of Multimodal Large Language Models: A Survey

# 5.2 Thur
* 5.(**KAN**)MLP一夜被干掉！MIT加州理工等革命性KAN破记录，发现数学定理碾压DeepMind  新智元  https://mp.weixin.qq.com/s/vqhTFPbcUQaCsQnARZrn0g \
  全新神经网络架构KAN一夜爆火！200参数顶30万，MIT华人一作，轻松复现Nature封面AI数学研究  量子位  https://mp.weixin.qq.com/s/5WFJMPJvtaofeGDxFQ9aDw \
  无需怀念MLP，新网络KAN基于柯尔莫哥洛夫-阿诺德定理，带着更少的参数、更强的性能、更好的可解释性来了，深度学习架构革新进入新时代！ \
  KAN: Kolmogorov-Arnold Networks \
  论文地址：https://arxiv.org/pdf/2404.19756 \
  项目链接：https://kindxiaoming.github.io/pykan/
* 6.Meta 联合纽约大学和华盛顿大学提出MetaCLIP，带你揭开CLIP的高质量数据之谜  机器之心  https://mp.weixin.qq.com/s/bEhDOBWcGeUZGMGA6lHoCA \
  原文链接：https://arxiv.org/abs/2309.16671 \
  项目链接：https://github.com/facebookresearch/MetaCLIP \
  论文标题：Demystifying CLIP Data
* 7.GitHub 8.9K Star，伯克利大学开源LLM记忆管理框架MemGPT  AI科技大本营  https://mp.weixin.qq.com/s/holcsXlfNQ9ZYBX5xEECNw \
  开源链接：https://github.com/cpacker/MemGPT

# 5.3 Fri
* 8.终于有人调查了小模型过拟合：三分之二都有数据污染，微软Phi-3、Mixtral 8x22B被点名  机器之心  https://mp.weixin.qq.com/s/YRYaCSsaegjBtwevpwlLHQ \
  论文标题：A Careful Examination of Large Language Model Performance on Grade School Arithmetic \
  论文链接：https://arxiv.org/pdf/2405.00332
* 9.小模型性能饱和、表现不佳，根源是因为Softmax?  机器之心  https://mp.weixin.qq.com/s/bvv-frM8bKhkZiqOa9nqDA \
  Why do small language models underperform? Studying LM Saturation via the Softmax Bottleneck \
  论文链接：https://arxiv.org/pdf/2404.07647.pdf
* 10.CVPR 2024 Highlight | 基于单曝光压缩成像，不依赖生成模型也能从单张图像中重建三维场景  机器之心  https://mp.weixin.qq.com/s/8F6Wij7kOkEEFzAHo00j8g \
  原文链接：https://arxiv.org/abs/2403.20018 \
  项目链接：https://github.com/WU-CVGL/SCINeRF \
  论文标题：SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image
* 11.(**meta一次预测多token**)一次预测多个token，Meta新模型推理加速3倍，编程任务提高17%  量子位  https://mp.weixin.qq.com/s/GuIqBdj4MteR9eBlTesdBA \
  对于背后原理，团队认为多token预测缓解了训练时Teacher Forcing和推理时自回归生成之间的分布差异。\
  也就是说，在训练的时候，模型看到的都是标准答案，生成的时候却得靠自己。好比人类在家做练习册时有答案，考试时却啥也没有，就会不适应。\
  而多token预测相当于训练时就逼着模型多想几步，这样到了考场上，才能应对自如。\
  从信息论的角度，团队还给出了一个更精确的论证。\
  传统的下一个Token预测，目标是最小化当前位置的信息熵。而2-Token预测实际上最小化的是当前和下一位置的信息熵之和。\
  数学推导表明，后者其实隐含了更大的互信息权重，也就是更看重当前Token和未来Token的相关性。这就是为什么多Token预测更”有远见”。\
  论文地址：https://arxiv.org/abs/2404.19737 \
  Better & Faster Large Language Models via Multi-token Prediction
* 12.(**多任务学习MTL综述**)释放多任务学习的力量：涵盖传统、深度和预训练基础模型时代的综述  专知  https://mp.weixin.qq.com/s/LjkpH4daIzpSF9FAC6V8-A \
  Unleashing the Power of Multi-Task Learning- A Comprehensive Survey Spanning Traditional, Deep, and Pretrained Foundation Model Eras \
  https://github.com/junfish/AwesomeMultitask-Learning

# 5.4 Sat
# 5.5 Sun

# 5.6 Mon
# 5.7 Tue
# 5.8 Wed
# 5.9 Thur
# 5.10 Fri
# 5.11 Sat
# 5.12 Sun

# 5.13 Mon
# 5.14 Tue
# 5.15 Wed
# 5.16 Thur
# 5.17 Fri
# 5.18 Sat
# 5.19 Sun

# 5.20 Mon
# 5.21 Tue
# 5.22 Wed
# 5.23 Thur
# 5.24 Fri
# 5.25 Sat
# 5.26 Sun

# 5.27 Mon
# 5.28 Tue
# 5.29 Wed
# 5.30 Thur
# 5.31 Fri
