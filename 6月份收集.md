# 6.1 周4
* 1、LLM推理提速2.8倍，CMU提出投机式推理引擎，小模型撬动大模型高效推理 https://mp.weixin.qq.com/s/kCJUJI1QMIcUQLAleQGosA \
近日，来自卡耐基梅隆大学（CMU）的 Catalyst Group 团队发布了一款「投机式推理」引擎 SpecInfer，可以借助轻量化的小模型来帮助大模型，在完全不影响生成内容准确度的情况下，实现两到三倍的推理加速。\
论文链接：https://arxiv.org/abs/2305.09781 \
项目链接：https://github.com/flexflow/FlexFlow/tree/inference \
![image](https://github.com/shuishenbushui/AICongyin-LLM.github.io/assets/45891944/37932cd8-70a3-4e83-8141-542bd14c2461)

# 6.3 周6
* 1、SelFee，一个迭代自修改的大语言模型。 \
详细介绍：kaistai.github.io/SelFee/
韩国顶尖公立大学KAIST（韩国科学技术院）根据LLaMA模型，推出了具备自我反馈、迭代能力的类ChatGPT开源模型SelFee（70亿、130亿两种参数）。SelFee的自我反馈、迭代能力相当新奇，无需借助外力可自我评估并生成。无论是在开源界、还是各大科技公司推出的产品中都很罕见。\
开源地址：https://github.com/kaistAI/SelFee \
在线体验：https://kaistai.github.io/SelFee/demo

# 6.7 周3
* 1、Awesome-Graph-LLM：图相关大型语言模型(LLM)相关资源列表: github.com/XiaoxinHe/Awesome-Graph-LLM \
* 2、参数高效微调（PEFT）技术 \
在面对特定的下游任务时，如果进行Full FineTuning（即对预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。\
PEFT技术旨在通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。因此，PEFT技术可以在提高模型效果的同时，大大缩短模型训练时间和计算成本，让更多人能够参与到深度学习研究中来。\
Prefix Tuning：与full fine-tuning更新所有参数的方式不同，该方法是在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而Transformer中的其他部分参数固定。该方法其实和构造Prompt类似，只是Prompt是人为构造的“显式”的提示,并且无法更新参数，而Prefix则是可以学习的“隐式”的提示。 同时，为了防止直接更新Prefix的参数导致训练不稳定的情况，他们在Prefix层前面加了MLP结构(相当于将Prefix分解为更小维度的Input与MLP的组合后输出的结果)，训练完成后，只保留Prefix的参数。\
Prompt Tuning：该方法可以看作是Prefix Tuning的简化版本，只在输入层加入prompt tokens，并不需要加入MLP进行调整来解决难训练的问题。随着预训练模型参数量的增加，Prompt Tuning的方法会逼近fine-tuning的结果。\
P-Tuning：该方法的提出主要是为了解决这样一个问题：大模型的Prompt构造方式严重影响下游任务的效果。P-Tuning将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式来对prompt embedding进行一层处理。\
P-Tuning v2：让Prompt Tuning能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌Fine-tuning的结果。相比Prompt Tuning和P-tuning的方法，P-Tuning v2方法在多层加入了Prompts tokens作为输入，带来两个方面的好处：
带来更多可学习的参数（从P-tuning和Prompt Tuning的0.1%增加到0.1%-3%），同时也足够参数高效。加入到更深层结构中的Prompt能给模型预测带来更直接的影响。\
Adapter Tuning：该方法设计了Adapter结构（首先是一个down-project层将高维度特征映射到低维特征，然后过一个非线形层之后，再用一个up-project结构将低维特征映射回原来的高维特征；同时也设计了skip-connection结构，确保了在最差的情况下能够退化为identity），并将其嵌入Transformer的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的Adapter结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数）。\
LoRA：在涉及到矩阵相乘的模块，引入A、B这样两个低秩矩阵模块去模拟full fine-tuning的过程，相当于只对语言模型中起关键作用的低秩本质维度进行更新。\
典型应用：\
ChatGLM-Tuning ：一种平价的chatgpt实现方案，基于清华的 ChatGLM-6B + LoRA 进行finetune。\
Alpaca-Lora：使用低秩自适应（LoRA）复现斯坦福羊驼的结果。Stanford Alpaca 是在 LLaMA 整个模型上微调，而 Alpaca-Lora 则是利用 Lora 技术，在冻结原模型 LLaMA 参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅微调的成本显著下降，还能获得和全模型微调类似的效果。\
BLOOM-LORA：由于LLaMA的限制，我们尝试使用Alpaca-Lora重新实现BLOOM-LoRA。\
PEFT实现：\
PEFT：Huggingface推出的PEFT库。\
unify-parameter-efficient-tuning：一个参数高效迁移学习的统一框架。\
经验与教训\
经验：\
对于同一模型，选择不同的训练框架，对于资源的消耗情况可能存在显著差异（比如使用Huggingface Transformers和DeepSpeed训练OPT-30相对于使用Alpa对于资源的消耗会低不少）。\
进行大模型模型训练时，先使用小规模模型（如：OPT-125m/2.7b）进行尝试，然后再进行大规模模型（如：OPT-13b/30b...）的尝试，便于出现问题时进行排查。
